- index: 1
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with one time dimension (timeseries), up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n8. **Keep iterating until errors are fixed:** At each stage, if there are parse or reconcile errors, keep updating the relevant file(s) to fix the error.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring files in object stores like S3 or GCS; to preview file contents, load one file into a table using a model and query it with `query_sql`\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Models as SQL files:** Always create new models as `.yaml` files, not `.sql` files (which are harder to extend later).\n- **Not creating connector files:** When Rill has native support for a connector (like S3 or BigQuery), always create a dedicated connector resource file for it.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. This also includes models that load external data using a native SQL function, like `read_parquet(...)` or `s3(...)`. Non-materialized models become views, which re-execute on every query.\n- **Referencing non-existant environment variables:** Only reference environment variables that are present in `.env` (returned in `env` from `project_status`). If you need the user to add another environment variable, stop and ask them to do so.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n- **Not adding a time dimension (timeseries) in metrics views**: Metrics views are much more useful when they have a time dimension. Make sure to set one of them as the primary time dimension using the `timeseries:` property.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n- **Don't stop if there are errors:** When a file has an error after you made changes, keep looping until you have done your best to fix the error. You should not give up easily, the user does expect you to try and fix errors.\n"
    - role: user
      content_type: text
      content: |-
        This may not relate to the user's task, but for context, the project's default OLAP connector is named duckdb (driver: duckdb).
         The default OLAP is in readwrite mode, so you can use it in models if you want.


        Call "navigate" tool for the main file created/edited in the conversation. Use kind "file" and pass the written file path.
        Prefer dashboard or metrics view files over other files.

        Task: I have some data in S3. Can you tell me what buckets are available? And also show me a little preview of what files are available at s3://integration-test.rilldata.com/glob_test/?
  response:
    - role: assistant
      content_type: tool_call
      id: call_1jnOKCXk90QWL6SHU10miqYS
      tool_name: list_buckets
      content: '{"connector":"s3"}'
    - role: assistant
      content_type: tool_call
      id: call_rotf2e04fOW8ho90Ta9XIaIn
      tool_name: list_bucket_objects
      content: '{"bucket":"integration-test.rilldata.com","connector":"s3","page_size":10,"path":"glob_test/"}'
- index: 2
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with one time dimension (timeseries), up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n8. **Keep iterating until errors are fixed:** At each stage, if there are parse or reconcile errors, keep updating the relevant file(s) to fix the error.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring files in object stores like S3 or GCS; to preview file contents, load one file into a table using a model and query it with `query_sql`\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Models as SQL files:** Always create new models as `.yaml` files, not `.sql` files (which are harder to extend later).\n- **Not creating connector files:** When Rill has native support for a connector (like S3 or BigQuery), always create a dedicated connector resource file for it.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. This also includes models that load external data using a native SQL function, like `read_parquet(...)` or `s3(...)`. Non-materialized models become views, which re-execute on every query.\n- **Referencing non-existant environment variables:** Only reference environment variables that are present in `.env` (returned in `env` from `project_status`). If you need the user to add another environment variable, stop and ask them to do so.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n- **Not adding a time dimension (timeseries) in metrics views**: Metrics views are much more useful when they have a time dimension. Make sure to set one of them as the primary time dimension using the `timeseries:` property.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n- **Don't stop if there are errors:** When a file has an error after you made changes, keep looping until you have done your best to fix the error. You should not give up easily, the user does expect you to try and fix errors.\n"
    - role: user
      content_type: text
      content: |-
        This may not relate to the user's task, but for context, the project's default OLAP connector is named duckdb (driver: duckdb).
         The default OLAP is in readwrite mode, so you can use it in models if you want.


        Call "navigate" tool for the main file created/edited in the conversation. Use kind "file" and pass the written file path.
        Prefer dashboard or metrics view files over other files.

        Task: I have some data in S3. Can you tell me what buckets are available? And also show me a little preview of what files are available at s3://integration-test.rilldata.com/glob_test/?
    - role: assistant
      content_type: tool_call
      id: bbbf02fe4fd54f55
      tool_name: list_buckets
      content: '{"connector":"s3"}'
    - role: tool
      content_type: tool_response
      id: bbbf02fe4fd54f55
      content: '{"buckets":["integration-test.rilldata.com"]}'
    - role: assistant
      content_type: tool_call
      id: 45e6cd2ef77d4db9
      tool_name: list_bucket_objects
      content: '{"bucket":"integration-test.rilldata.com","connector":"s3","page_size":10,"path":"glob_test/"}'
    - role: tool
      content_type: tool_response
      id: 45e6cd2ef77d4db9
      content: '{"objects":[{"path":"glob_test/y=2010/","is_dir":true},{"path":"glob_test/y=2023/","is_dir":true},{"path":"glob_test/y=2024/","is_dir":true}]}'
  response:
    - role: assistant
      content_type: text
      content: |-
        Buckets available via the `s3` connector:
        - `integration-test.rilldata.com`

        Preview of objects under `s3://integration-test.rilldata.com/glob_test/` (top-level):
        - `glob_test/y=2010/` (dir)
        - `glob_test/y=2023/` (dir)
        - `glob_test/y=2024/` (dir)
