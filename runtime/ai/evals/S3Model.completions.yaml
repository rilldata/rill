- index: 1
  input:
    - role: system
      content_type: text
      content: |-
        You are a routing agent that determines which specialized agent should handle a user's request.
        You operate in the context of a business intelligence tool that supports data modeling and data exploration, and more.
        Your input includes the user's previous messages and responses, as well as the user's latest message, which you are responsible for routing.
        Routing guidelines:
        - If the user's question relates to developing or permanently changing the data model, metrics, or dashboards, you should route to the developer.
        - If the user's question relates to exploring dashboards or retrieving specific business metrics, you should route to the analyst.
        - If the user asks a general question, you should route to the analyst.
        - If the user specifically requests an agent by name, you must route to that agent if it's available.
        You must answer with a single agent choice and no further explanation. Pick only from this list of available agents (description in parentheses):
        - analyst_agent (Agent that assists with data analysis tasks.)
        - developer_agent (Agent that assists with development tasks.)
    - role: user
      content_type: text
      content: I have some CSV files in S3. Can you create a connector for S3 and a DuckDB model that loads the data at s3://integration-test.rilldata.com/glob_test/y=*/*.csv? I've already added environment variables for S3 access. Please proceed without asking clarifying questions.
  response:
    - role: assistant
      content_type: text
      content: '{"agent":"developer_agent"}'
- index: 2
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Models as SQL files:** Always create new models as `.yaml` files, not `.sql` files (which are harder to extend later).\n- **Not creating connector files:** When Rill has native support for a connector (like S3 or BigQuery), always create a dedicated connector resource file for it.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. This also includes models that load external data using a native SQL function, like `read_parquet(...)` or `s3(...)`. Non-materialized models become views, which re-execute on every query.\n- **Referencing non-existant environment variables:** Only reference environment variables that are present in `.env` (returned in `env` from `project_status`). If you need the user to add another environment variable, stop and ask them to do so.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: user
      content_type: text
      content: |-
        This may not relate to the user's task, but for context, the project's default OLAP connector is named duckdb (driver: duckdb).
         The default OLAP is in readwrite mode, so you can use it in models if you want.


        Task: I have some CSV files in S3. Can you create a connector for S3 and a DuckDB model that loads the data at s3://integration-test.rilldata.com/glob_test/y=*/*.csv? I've already added environment variables for S3 access. Please proceed without asking clarifying questions.
  response:
    - role: assistant
      content_type: tool_call
      id: call_9HAZC3d0uL4dBWFvkNuYJLvV
      tool_name: list_files
      content: '{}'
    - role: assistant
      content_type: tool_call
      id: call_ELClhMoefnq2fAfdsFkP3oSd
      tool_name: project_status
      content: '{}'
    - role: assistant
      content_type: tool_call
      id: call_Ck36c9BVPajJ5fPZQr4pFe6i
      tool_name: search_files
      content: '{"glob_pattern":"**/*.yaml","pattern":"type:\\s*connector|driver:\\s*s3|s3"}'
- index: 3
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Models as SQL files:** Always create new models as `.yaml` files, not `.sql` files (which are harder to extend later).\n- **Not creating connector files:** When Rill has native support for a connector (like S3 or BigQuery), always create a dedicated connector resource file for it.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. This also includes models that load external data using a native SQL function, like `read_parquet(...)` or `s3(...)`. Non-materialized models become views, which re-execute on every query.\n- **Referencing non-existant environment variables:** Only reference environment variables that are present in `.env` (returned in `env` from `project_status`). If you need the user to add another environment variable, stop and ask them to do so.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: user
      content_type: text
      content: |-
        This may not relate to the user's task, but for context, the project's default OLAP connector is named duckdb (driver: duckdb).
         The default OLAP is in readwrite mode, so you can use it in models if you want.


        Task: I have some CSV files in S3. Can you create a connector for S3 and a DuckDB model that loads the data at s3://integration-test.rilldata.com/glob_test/y=*/*.csv? I've already added environment variables for S3 access. Please proceed without asking clarifying questions.
    - role: assistant
      content_type: tool_call
      id: 96eedb8a328f4b8c
      tool_name: list_files
      content: '{}'
    - role: tool
      content_type: tool_response
      id: 96eedb8a328f4b8c
      content: '{"files":[{"path":"/connectors/duckdb.yaml","resources":[{"kind":"rill.runtime.v1.Connector","name":"duckdb","reconcile_error":"","reconcile_status":"RECONCILE_STATUS_IDLE"}]},{"path":"/rill.yaml"}]}'
    - role: assistant
      content_type: tool_call
      id: 12392148d788479a
      tool_name: project_status
      content: '{}'
    - role: tool
      content_type: tool_response
      id: 12392148d788479a
      content: '{"default_olap_connector":"duckdb","env":["connector.openai.api_key","connector.s3.aws_access_key_id","connector.s3.aws_secret_access_key"],"resources":[{"kind":"rill.runtime.v1.ProjectParser","name":"parser","path":"","reconcile_error":"","reconcile_status":"RECONCILE_STATUS_IDLE","refs":[]},{"kind":"rill.runtime.v1.Connector","name":"duckdb","path":"/connectors/duckdb.yaml","reconcile_error":"","reconcile_status":"RECONCILE_STATUS_IDLE","refs":[]}],"parse_errors":[]}'
    - role: assistant
      content_type: tool_call
      id: e4a7881f2a234928
      tool_name: search_files
      content: '{"glob_pattern":"**/*.yaml","pattern":"type:\\s*connector|driver:\\s*s3|s3"}'
    - role: tool
      content_type: tool_response
      id: e4a7881f2a234928
      content: '{"matches":[{"path":"/connectors/duckdb.yaml","lines":[2],"snippets":["  1: \n\u003e 2: type: connector\n  3: driver: duckdb\n  4: managed: true"]}]}'
  response:
    - role: assistant
      content_type: tool_call
      id: call_iCOtUUsQ2Nj3bwF7wPrSjDkh
      tool_name: develop_file
      content: '{"path":"connectors/s3.yaml","prompt":"Create a new S3 connector resource.\n\nRequirements:\n- File path: connectors/s3.yaml\n- Resource name should be `s3` (derived from filename).\n- Use driver: s3.\n- Use env vars already present in the project status: `connector.s3.aws_access_key_id` and `connector.s3.aws_secret_access_key`.\n- Configure region conservatively (us-east-1) but allow override via env var if present (do not introduce new required env vars). If you include optional env var references, guard them with defaults if supported.\n- Keep minimal properties needed for Rill to access public/private buckets.\n\nReturn valid YAML for a Rill connector.","type":"connector"}'
- index: 4
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Models as SQL files:** Always create new models as `.yaml` files, not `.sql` files (which are harder to extend later).\n- **Not creating connector files:** When Rill has native support for a connector (like S3 or BigQuery), always create a dedicated connector resource file for it.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. This also includes models that load external data using a native SQL function, like `read_parquet(...)` or `s3(...)`. Non-materialized models become views, which re-execute on every query.\n- **Referencing non-existant environment variables:** Only reference environment variables that are present in `.env` (returned in `env` from `project_status`). If you need the user to add another environment variable, stop and ask them to do so.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: system
      content_type: text
      content: |-
        # Instructions for developing a connector in Rill

        ## Introduction

        Connectors are resources that contain credentials and settings for connecting to external systems. They are typically found at the root of the project's DAG, providing access to data sources and services that power downstream resources like models and metrics views.

        Connectors are usually lightweight resources. When reconciled, they validate the connection to the external system but do not move or process data. The main exception is managed OLAP connectors (with `managed: true`), which trigger database provisioning.

        ### Driver capabilities

        Each connector uses a **driver** that implements one or more capabilities:

        - **OLAP database**: Can power metrics views and dashboards (e.g., `duckdb`, `clickhouse`)
        - **SQL database**: Can run SQL queries as model inputs (e.g., `postgres`, `bigquery`, `snowflake`)
        - **Information schema**: Can list tables and their schemas (e.g., `duckdb`, `bigquery`)
        - **Object store**: Can list, read, and write flat files (e.g., `s3`, `gcs`)
        - **Notifier**: Can send notifications and alerts (e.g., `slack`)
        - **AI**: Can generate embeddings or responses (e.g., `openai`)

        ## Core Concepts

        ### Naming conventions

        Connectors are typically named after their driver (e.g., a file `connectors/duckdb.yaml` creates a connector named `duckdb`). Use descriptive names when you have multiple connectors of the same type:
        - `connectors/s3_data.yaml` and `connectors/s3_staging.yaml`
        - `connectors/clickhouse_prod.yaml` and `connectors/clickhouse_dev.yaml`

        ### Secrets management

        Always store sensitive credentials in `.env` and reference them using template syntax:

        ```yaml
        type: connector
        driver: s3
        aws_access_key_id: "{{ .env.aws_access_key_id }}"
        aws_secret_access_key: "{{ .env.aws_secret_access_key }}"
        ```

        NOTE: Some legacy projects use the deprecated `.vars` instead of `.env`.

        ### Managed connectors

        OLAP connectors can be provisioned automatically by Rill using `managed: true`. This is supported for `duckdb` and `clickhouse` drivers:

        ```yaml
        type: connector
        driver: duckdb
        managed: true
        ```

        When a managed connector is reconciled, Rill provisions the database infrastructure. The user is billed for the CPU, memory, and disk usage of the provisioned database.

        ### Access modes

        Control read/write access using the `mode` property:

        - `mode: read`: Prevents Rill models from writing to this connector
        - `mode: readwrite`: Allows both reading and writing (default for managed connectors)

        Use `mode: read` when connecting to external databases with pre-existing tables to prevent unintended modifications.

        ### Dev/Prod configuration

        Use `dev:` and `prod:` blocks for environment-specific settings:

        ```yaml
        type: connector
        driver: clickhouse
        mode: readwrite

        # Use a local database in development to prevent overwriting data in the production cluster when iterating on changes.
        dev:
          managed: true

        prod:
          host: "{{ .env.clickhouse_host }}"
          port: 9440
          username: "{{ .env.clickhouse_user }}"
          password: "{{ .env.clickhouse_password }}"
          ssl: true
        ```

        ## Driver-Specific Notes

        ### DuckDB

        DuckDB is Rill's default embedded OLAP database. Key properties:

        - `managed: true`: Rill provisions and manages the database
        - `init_sql`: SQL to run at startup (install extensions, create secrets, attach databases)

        For MotherDuck (cloud DuckDB), use the `path` property with `md:` prefix:

        ```yaml
        type: connector
        driver: duckdb
        path: "md:my_database"
        token: "{{ .env.motherduck_token }}"
        ```

        ### ClickHouse

        ClickHouse can be user-managed or Rill-managed. Key properties:

        - `managed: true`: Rill provisions and manages an empty Clickhouse cluster. If set, don't set any other connector properties.
        - `host`, `port`, `username`, `password`: Connection credentials
        - `database`: Target database name
        - `ssl: true`: Required for ClickHouse Cloud
        - `cluster`: Cluster name for multi-node Clickhouse clusters
        - `dsn`: Alternative connection string format (format: `clickhouse://host:port?username=<username>&...`)

        Common ports:
        - `8443`: HTTPS native protocol (ClickHouse Cloud)
        - `9440`: Secure native protocol
        - `9000`: Native protocol (non-SSL)

        ### S3

        AWS S3 and S3-compatible storage. Key properties:

        - `aws_access_key_id`, `aws_secret_access_key`: AWS credentials
        - `region`: AWS region
        - `endpoint`: Custom endpoint for S3-compatible services (R2, MinIO, GCS interop)
        - `path_prefixes`: A list of bucket paths that the connector can access, e.g. `[s3://my-bucket]`; useful for improving bucket introspection

        ### GCS

        Google Cloud Storage. Key properties:

        - `google_application_credentials`: Service account JSON (must be a literal JSON string value)
        - `key_id`: HMAC key ID to use instead of a service account JSON; required for direct use with DuckDB and Clickhouse through S3 compatibility
        - `secret`: HMAC secret to use instead of a service account JSON; required for direct use with DuckDB and Clickhouse through S3 compatibility

        ### BigQuery

        Google BigQuery. Key properties:

        - `project_id`: GCP project ID
        - `google_application_credentials`: Service account JSON

        ### Snowflake

        Snowflake data warehouse. Key properties:

        - `account`, `user`, `privateKey`, `database`, `schema`, `warehouse`, `role`: Connection parameters
        - `dsn`: Connection string to use instead of separate connection parameters

        ### Postgres

        PostgreSQL database. Key properties:

        - `host`, `port`, `user`, `password`, `dbname`: Connection parameters
        - `sslmode`: SSL mode (`disable`, `require`, `verify-full`, etc.)

        ### Druid

        Apache Druid. Can be configured via host/port or DSN:

        - `host`, `port`, `username`, `password`, `ssl`: Direct connection
        - `dsn`: Full connection string

        ### Redshift

        Amazon Redshift. Key properties:

        - `aws_access_key_id`, `aws_secret_access_key`: AWS credentials
        - `workgroup`: Redshift Serverless workgroup name
        - `region`: AWS region
        - `database`: Database name

        ### Athena

        Amazon Athena. Key properties:

        - `aws_access_key_id`, `aws_secret_access_key`: AWS credentials
        - `workgroup`: Redshift Serverless workgroup name
        - `region`: AWS region
        - `output_location`: S3 path in format `s3://bucket/path` to store temporary query results in (Athena only)

        ### Other drivers

        - **Slack**: Use `bot_token` for alert notifications
        - **OpenAI**: Use `api_key` for AI-powered features
        - **HTTPS**: Simple connector for public HTTP sources
        - **Pinot**: Use `broker_host`, `controller_host`, `username`, `password`

        ## JSON Schema

        Here is a full JSON schema for the connector syntax:

        ```
        allOf:
            - properties:
                type:
                    const: connector
                    description: Refers to the resource type and must be `connector`
                    type: string
              required:
                - type
              title: Properties
              type: object
            - $ref: '#/definitions/common_properties'
            - oneOf:
                - examples:
                    - allow_host_access: true
                      aws_access_key_id: myawsaccesskey
                      aws_access_token: mytemporarytoken
                      aws_region: us-east-1
                      aws_secret_access_key: myawssecretkey
                      driver: athena
                      external_id: MyExternalID
                      output_location: s3://my-bucket/athena-output/
                      role_arn: arn:aws:iam::123456789012:role/MyRole
                      role_session_name: MySession
                      type: connector
                      workgroup: primary
                  properties:
                    allow_host_access:
                        description: Allow the Athena client to access host environment configurations such as environment variables or local AWS credential files. Defaults to true, enabling use of credentials and settings from the host environment unless explicitly disabled.
                        type: boolean
                    aws_access_key_id:
                        description: AWS Access Key ID used for authentication. Required when using static credentials directly or as base credentials for assuming a role.
                        type: string
                    aws_access_token:
                        description: AWS session token used with temporary credentials. Required only if the Access Key and Secret Key are part of a temporary session credentials.
                        type: string
                    aws_region:
                        description: AWS region where Athena and the result S3 bucket are located (e.g., us-east-1). Defaults to 'us-east-1' if not specified.
                        type: string
                    aws_secret_access_key:
                        description: AWS Secret Access Key paired with the Access Key ID. Required when using static credentials directly or as base credentials for assuming a role.
                        type: string
                    driver:
                        const: athena
                        description: Refers to the driver type and must be driver `athena`
                        type: string
                    external_id:
                        description: External ID required by some roles when assuming them, typically for cross-account access. Used only if 'role_arn' is specified and the role's trust policy requires it.
                        type: string
                    output_location:
                        description: S3 URI where Athena query results should be stored (e.g., s3://your-bucket/athena/results/). Optional if the selected workgroup has a default result configuration.
                        type: string
                    role_arn:
                        description: ARN of the IAM role to assume. When specified, the SDK uses the base credentials to call STS AssumeRole and obtain temporary credentials scoped to this role.
                        type: string
                    role_session_name:
                        description: Session name to associate with the STS AssumeRole session. Used only if 'role_arn' is specified. Useful for identifying and auditing the session.
                        type: string
                    workgroup:
                        description: Athena workgroup to use for query execution. Defaults to 'primary' if not specified.
                        type: string
                  required:
                    - driver
                  title: Athena
                  type: object
                - examples:
                    - azure_storage_account: mystorageaccount
                      azure_storage_key: credentialstring
                      driver: azure
                      type: connector
                  properties:
                    allow_host_access:
                        description: Allow access to host environment configuration
                        type: boolean
                    azure_storage_account:
                        description: Azure storage account name
                        type: string
                    azure_storage_connection_string:
                        description: Optional azure connection string for storage account
                        type: string
                    azure_storage_key:
                        description: Azure storage access key
                        type: string
                    azure_storage_sas_token:
                        description: Optional azure SAS token for authentication
                        type: string
                    driver:
                        const: azure
                        description: Refers to the driver type and must be driver `azure`
                        type: string
                    path_prefixes:
                        description: |
                            A list of container or virtual directory prefixes that this connector is allowed to access.
                            Useful when different containers or paths use different credentials, allowing the system
                            to route access through the appropriate connector based on the blob path.
                            Example: `azure://my-bucket/`, ` azure://my-bucket/path/` ,`azure://my-bucket/path/prefix`
                        type:
                            - string
                            - array
                  required:
                    - driver
                    - azure_storage_account
                    - azure_storage_key
                  title: Azure
                  type: object
                - examples:
                    - allow_host_access: true
                      driver: bigquery
                      google_application_credentials: credentialjsonstring
                      project_id: my-project-id
                      type: connector
                  properties:
                    allow_host_access:
                        description: Enable the BigQuery client to use credentials from the host environment when no service account JSON is provided. This includes Application Default Credentials from environment variables, local credential files, or the Google Compute Engine metadata server. Defaults to true, allowing seamless authentication in GCP environments.
                        type: boolean
                    dataset_id:
                        description: BigQuery dataset ID
                        type: string
                    driver:
                        const: bigquery
                        description: Refers to the driver type and must be driver `bigquery`
                        type: string
                    google_application_credentials:
                        description: Raw contents of the Google Cloud service account key (in JSON format) used for authentication.
                        type: string
                    location:
                        description: BigQuery dataset location
                        type: string
                    project_id:
                        description: Google Cloud project ID
                        type: string
                  required:
                    - driver
                  title: BigQuery
                  type: object
                - examples:
                    - cluster: mycluster
                      database: mydatabase
                      driver: clickhouse
                      host: localhost
                      managed: false
                      mode: readwrite
                      password: mypassword
                      port: 9000
                      ssl: true
                      type: connector
                      username: myusername
                  properties:
                    can_scale_to_zero:
                        description: Indicates if the database can scale to zero
                        type: boolean
                    cluster:
                        description: Cluster name, required for running distributed queries
                        type: string
                    conn_max_lifetime:
                        description: Maximum time a connection may be reused
                        type: string
                    database:
                        description: Name of the ClickHouse database within the cluster
                        type: string
                    dial_timeout:
                        description: Timeout for dialing the ClickHouse server
                        type: string
                    driver:
                        const: clickhouse
                        description: Refers to the driver type and must be driver `clickhouse`
                        type: string
                    dsn:
                        description: DSN(Data Source Name) for the ClickHouse connection
                        type: string
                    embed_port:
                        description: Port to run ClickHouse locally (0 for random port)
                        type: integer
                    host:
                        description: Host where the ClickHouse instance is running
                        type: string
                    log_queries:
                        description: Controls whether to log raw SQL queries
                        type: boolean
                    managed:
                        description: '`true` means Rill will provision the connector using the default provisioner. `false` disables automatic provisioning.'
                        type: boolean
                    max_idle_conns:
                        description: Maximum number of idle connections in the pool
                        type: integer
                    max_open_conns:
                        description: Maximum number of open connections to the database
                        type: integer
                    mode:
                        description: '`read` - Controls the operation mode for the ClickHouse connection. Defaults to ''read'' for safe operation with external databases. Set to ''readwrite'' to enable model creation and table mutations. Note: When ''managed: true'', this is automatically set to ''readwrite''.'
                        type: string
                    password:
                        description: Password for authentication
                        type: string
                    port:
                        description: Port where the ClickHouse instance is accessible
                        type: integer
                    query_settings:
                        description: query settings to be set on dashboard queries. `query_settings_override` takes precedence over these settings and if set these are ignored. Each setting must be separated by a comma. Example `max_threads = 8, max_memory_usage = 10000000000`
                        type: string
                    query_settings_override:
                        description: override the default settings used in queries. Changing the default settings can lead to incorrect query results and is generally not recommended. If you need to add settings, use `query_settings`
                        type: string
                    read_timeout:
                        description: Maximum time for a connection to read data
                        type: string
                    ssl:
                        description: Indicates whether a secured SSL connection is required
                        type: boolean
                    username:
                        description: Username for authentication
                        type: string
                  required:
                    - driver
                  title: ClickHouse
                  type: object
                - examples:
                    - driver: druid
                      host: localhost
                      password: mypassword
                      port: 8082
                      ssl: true
                      type: connector
                      username: myusername
                  properties:
                    driver:
                        const: druid
                        description: Refers to the driver type and must be driver `druid`
                        type: string
                    dsn:
                        description: Data Source Name (DSN) for connecting to Druid
                        type: string
                    host:
                        description: Hostname of the Druid coordinator or broker
                        type: string
                    log_queries:
                        description: Log raw SQL queries sent to Druid
                        type: boolean
                    max_open_conns:
                        description: Maximum number of open database connections (0 = default, -1 = unlimited)
                        type: integer
                    password:
                        description: Password for authenticating with Druid
                        type: string
                    port:
                        description: Port number of the Druid service
                        type: integer
                    skip_version_check:
                        description: Skip checking Druid version compatibility
                        type: boolean
                    ssl:
                        description: Enable SSL for secure connection
                        type: boolean
                    username:
                        description: Username for authenticating with Druid
                        type: string
                  required:
                    - driver
                    - dsn
                  title: Druid
                  type: object
                - examples:
                    - allow_host_access: true
                      cpu: 4
                      driver: duckdb
                      init_sql: INSTALL httpfs; LOAD httpfs;
                      log_queries: true
                      memory_limit_gb: 16
                      mode: readwrite
                      pool_size: 5
                      read_write_ratio: 0.7
                      type: connector
                  properties:
                    allow_host_access:
                        description: Whether access to local environment and file system is allowed
                        type: boolean
                    attach:
                        description: Full ATTACH statement to attach a DuckDB database
                        type: string
                    boot_queries:
                        description: Deprecated - Use init_sql instead
                        type: string
                    conn_init_sql:
                        description: SQL executed when a new connection is initialized
                        type: string
                    cpu:
                        description: Number of CPU cores available to the database
                        minimum: 1
                        type: integer
                    create_secrets_from_connectors:
                        description: List of connector names for which temporary secrets should be created before executing the SQL.
                        type:
                            - string
                            - array
                    database_name:
                        description: Name of the attached DuckDB database (auto-detected if not set)
                        type: string
                    driver:
                        const: duckdb
                        description: Must be "duckdb"
                        type: string
                    init_sql:
                        description: SQL executed during database initialization
                        type: string
                    log_queries:
                        description: Whether to log raw SQL queries executed through OLAP
                        type: boolean
                    memory_limit_gb:
                        description: Amount of memory in GB available to the database
                        minimum: 1
                        type: integer
                    mode:
                        default: read
                        description: Set the mode for the DuckDB connection.
                        enum:
                            - read
                            - readwrite
                        type: string
                    path:
                        description: Path to external DuckDB database
                        type: string
                    pool_size:
                        description: Number of concurrent connections and queries allowed
                        minimum: 1
                        type: integer
                    read_write_ratio:
                        default: 0.5
                        description: Ratio of resources allocated to read vs write operations
                        maximum: 1
                        minimum: 0
                        type: number
                    schema_name:
                        description: Default schema used by the DuckDB database
                        type: string
                  required:
                    - driver
                  title: DuckDB
                  type: object
                - examples:
                    - driver: duckdb
                      mode: read
                      path: /path/to/my-duckdb-database.db
                      type: connector
                  properties:
                    driver:
                        const: duckdb
                        description: Refers to the driver type and must be driver `duckdb`
                        type: string
                    mode:
                        default: read
                        description: Set the mode for the DuckDB connection.
                        enum:
                            - read
                            - readwrite
                        type: string
                    path:
                        description: Path to the DuckDB database
                        type: string
                  required:
                    - driver
                    - db
                  title: External DuckDB
                  type: object
                - examples:
                    - driver: gcs
                      google_application_credentials: credentialjsonstring
                      type: connector
                  properties:
                    allow_host_access:
                        description: Allow access to host environment configuration
                        type: boolean
                    driver:
                        const: gcs
                        description: Refers to the driver type and must be driver `gcs`
                        type: string
                    google_application_credentials:
                        description: Google Cloud credentials JSON string
                        type: string
                    key_id:
                        description: Optional S3-compatible Key ID when used in compatibility mode
                        type: string
                    path_prefixes:
                        description: "A list of bucket path prefixes that this connector is allowed to access. \nUseful when different buckets or bucket prefixes use different credentials, \nallowing the system to select the appropriate connector based on the bucket path.\nExample: `gs://my-bucket/`, ` gs://my-bucket/path/` ,`gs://my-bucket/path/prefix`\n"
                        type:
                            - string
                            - array
                    secret:
                        description: Optional S3-compatible Secret when used in compatibility mode
                        type: string
                  required:
                    - driver
                    - bucket
                  title: GCS
                  type: object
                - examples:
                    - driver: https
                      headers:
                        Authorization: Bearer my-token
                      type: connector
                  properties:
                    driver:
                        const: https
                        description: Refers to the driver type and must be driver `https`
                        type: string
                    headers:
                        additionalProperties:
                            type: string
                        description: HTTP headers to include in the request
                        type: object
                    path_prefixes:
                        description: |
                            A list of HTTP/HTTPS URL prefixes that this connector is allowed to access.
                            Useful when different URL namespaces use different credentials, enabling the
                            system to choose the appropriate connector based on the URL path.
                            Example: `https://example.com/`, ` https://example.com/path/` ,`https://example.com/path/prefix`
                        type:
                            - string
                            - array
                  required:
                    - driver
                    - path
                  title: HTTPS
                  type: object
                - examples:
                    - driver: duckdb
                      path: md:my_database
                      schema_name: my_schema
                      token: '{{ .env.connector.motherduck.token }}'
                      type: connector
                  properties:
                    create_secrets_from_connectors:
                        description: List of connector names for which temporary secrets should be created before executing the SQL.
                        type:
                            - string
                            - array
                    driver:
                        const: duckdb
                        description: Refers to the driver type and must be driver `duckdb`.
                        type: string
                    init_sql:
                        description: SQL executed during database initialization.
                        type: string
                    mode:
                        default: read
                        description: Set the mode for the MotherDuck connection. By default, it is set to 'read' which allows only read operations. Set to 'readwrite' to enable model creation and table mutations.
                        enum:
                            - read
                            - readwrite
                        type: string
                    path:
                        description: Path to your MD database
                        type: string
                    schema_name:
                        description: Define your schema if not main, uses main by default
                        type: string
                    token:
                        description: MotherDuck token
                        type: string
                  required:
                    - driver
                    - path
                    - token
                  title: MotherDuck
                  type: object
                - examples:
                    - database: mydb
                      driver: mysql
                      host: localhost
                      password: p@ss
                      port: 3306
                      ssl-mode: preferred
                      type: connector
                      user: user
                    - driver: mysql
                      dsn: mysql://user:p%40ss@localhost:3306/mydb?ssl-mode=preferred
                      type: connector
                  properties:
                    database:
                        description: Name of the MySQL database
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `mysql`
                        type: string
                    dsn:
                        description: |
                            **Data Source Name (DSN)** for the MySQL connection, provided in [MySQL URI format](https://dev.mysql.com/doc/refman/8.4/en/connecting-using-uri-or-key-value-pairs.html#connecting-using-uri).
                            The DSN must follow the standard MySQL URI scheme:
                            ```text
                            mysql://user:password@host:3306/my-db
                            ```
                            Rules for special characters in password:
                            - The following characters are allowed [unescaped in the URI](https://datatracker.ietf.org/doc/html/rfc3986#section-2.3): `~` `.` `_` `-`
                            - All other special characters must be percent-encoded (`%XX` format).
                            ```text
                            mysql://user:pa%40ss@localhost:3306/my-db   # password contains '@'
                            mysql://user:pa%3Ass@localhost:3306/my-db   # password contains ':'
                            ```
                        type: string
                    host:
                        description: Hostname of the MySQL server
                        type: string
                    password:
                        description: Password for authentication
                        type: string
                    port:
                        description: Port number for the MySQL server
                        type: integer
                    ssl-mode:
                        description: 'ssl mode options: `disabled`, `preferred`, or `required`.'
                        type: string
                    user:
                        description: Username for authentication
                        type: string
                  required:
                    - driver
                  title: MySQL
                  type: object
                - examples:
                    - api_key: my-api-key
                      api_type: openai
                      api_version: "2023-05-15"
                      base_url: https://api.openai.com/v1
                      driver: openai
                      model: gpt-4o
                      type: connector
                  properties:
                    api_key:
                        description: API key for connecting to OpenAI
                        type: string
                    api_type:
                        description: The type of OpenAI API to use
                        type: string
                    api_version:
                        description: The version of the OpenAI API to use (e.g., '2023-05-15'). Required when API Type is AZURE or AZURE_AD
                        type: string
                    base_url:
                        description: The base URL for the OpenAI API (e.g., 'https://api.openai.com/v1')
                        type: string
                    driver:
                        description: The driver type, must be set to "openai"
                        type: string
                    model:
                        description: The OpenAI model to use (e.g., 'gpt-4o')
                        type: string
                  required:
                    - api_key
                  title: OpenAI
                  type: object
                - examples:
                    - broker_host: localhost
                      broker_port: 9000
                      controller_host: localhost
                      controller_port: 9000
                      driver: pinot
                      log_queries: true
                      max_open_conns: 100
                      password: mypassword
                      ssl: true
                      type: connector
                      username: myusername
                  properties:
                    broker_host:
                        description: Hostname of the Pinot broker
                        type: string
                    broker_port:
                        description: Port number for the Pinot broker
                        type: integer
                    controller_host:
                        description: Hostname of the Pinot controller
                        type: string
                    controller_port:
                        description: Port number for the Pinot controller
                        type: integer
                    driver:
                        description: Refers to the driver type and must be driver `pinot`
                        type: string
                    dsn:
                        description: DSN(Data Source Name) for the Pinot connection
                        type: string
                    log_queries:
                        description: Log raw SQL queries executed through Pinot
                        type: boolean
                    max_open_conns:
                        description: Maximum number of open connections to the Pinot database
                        type: integer
                    password:
                        description: Password for authenticating with Pinot
                        type: string
                    ssl:
                        description: Enable SSL connection to Pinot
                        type: boolean
                    username:
                        description: Username for authenticating with Pinot
                        type: string
                  required:
                    - driver
                    - dsn
                    - broker_host
                    - controller_host
                  title: Pinot
                  type: object
                - examples:
                    - dbname: mydatabase
                      driver: postgres
                      host: localhost
                      password: mypassword
                      port: 5432
                      sslmode: prefer
                      type: connector
                      user: myusername
                    - driver: postgres
                      dsn: user=myusername password='my pass\'word' host=localhost port=5432 dbname=mydatabase sslmode=prefer
                      type: connector
                    - driver: postgres
                      dsn: postgres://myusername:p%40ss@localhost:5432/mydatabase?sslmode=prefer
                      type: connector
                  properties:
                    dbname:
                        description: Name of the Postgres database
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `postgres`
                        type: string
                    dsn:
                        description: |
                            **Data Source Name (DSN)** for the PostgreSQL connection, provided in
                            [PostgreSQL connection string format](https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING).
                            PostgreSQL supports both **key=value format** and **URI format**.

                            key=value format example:
                            ```text
                            user=user password=password host=host port=5432 dbname=mydb
                            ```
                            Rules for key=value format for special characters:
                            - To write an empty value, or a value containing spaces, `=`, single quotes, or backslashes, surround it with single quotes.
                            - Single quotes and backslashes inside a value must be escaped with a backslash (`\'` and `\\`).

                            URI format example:
                            ```text
                            postgres://user:password@host:5432/mydb
                            ```

                            Rules for URI format:
                            - The following characters are allowed [unescaped in the URI](https://datatracker.ietf.org/doc/html/rfc3986#section-2.3): `~` `.` `_` `-`
                            - All other special characters must be percent-encoded (`%XX` format).

                            Examples (URI format with encoded characters):
                            ```text
                            postgres://user:pa%40ss@localhost:5432/my-db   # '@' is encoded as %40
                            postgres://user:pa%3Ass@localhost:5432/my-db   # ':' is encoded as %3A
                            ```
                        type: string
                    host:
                        description: Hostname of the Postgres server
                        type: string
                    password:
                        description: Password for authentication
                        type: string
                    port:
                        description: Port number for the Postgres server
                        type: string
                    sslmode:
                        description: 'ssl mode options: `disable`, `allow`, `prefer` or `require`.'
                        type: string
                    user:
                        description: Username for authentication
                        type: string
                  required:
                    - driver
                  title: Postgres
                  type: object
                - examples:
                    - aws_access_key_id: my-access-key-id
                      aws_access_token: my-access-token
                      aws_secret_access_key: my-secret-access-key
                      cluster_identifier: my-cluster-identifier
                      database: mydatabase
                      driver: redshift
                      region: us-east-1
                      type: connector
                      workgroup: my-workgroup
                  properties:
                    aws_access_key_id:
                        description: AWS Access Key ID used for authenticating with Redshift.
                        type: string
                    aws_access_token:
                        description: AWS Session Token for temporary credentials (optional).
                        type: string
                    aws_secret_access_key:
                        description: AWS Secret Access Key used for authenticating with Redshift.
                        type: string
                    cluster_identifier:
                        description: Cluster identifier for provisioned Redshift clusters, in case of Redshift Serverless use 'workgroup' .
                        type: string
                    database:
                        description: Name of the Redshift database to query.
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `redshift`
                        type: string
                    region:
                        description: AWS region where the Redshift cluster or workgroup is hosted (e.g., 'us-east-1').
                        type: string
                    workgroup:
                        description: Workgroup name for Redshift Serverless, in case of provisioned Redshift clusters use 'cluster_identifier'.
                        type: string
                  required:
                    - driver
                    - aws_access_key_id
                    - aws_secret_access_key
                    - database
                  title: Redshift
                  type: object
                - examples:
                    - aws_access_key_id: my-access-key-id
                      aws_access_token: my-access-token
                      aws_secret_access_key: my-secret-access-key
                      driver: s3
                      endpoint: https://my-s3-endpoint.com
                      region: us-east-1
                      type: connector
                  properties:
                    allow_host_access:
                        description: Allow access to host environment configuration
                        type: boolean
                    aws_access_key_id:
                        description: AWS Access Key ID used for authentication
                        type: string
                    aws_access_token:
                        description: Optional AWS session token for temporary credentials
                        type: string
                    aws_secret_access_key:
                        description: AWS Secret Access Key used for authentication
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `s3`
                        type: string
                    endpoint:
                        description: Optional custom endpoint URL for S3-compatible storage
                        type: string
                    path_prefixes:
                        description: |
                            A list of bucket path prefixes that this connector is allowed to access.
                            Useful when different buckets or bucket prefixes use different credentials,
                            allowing the system to select the appropriate connector based on the bucket path.
                            Example: `s3://my-bucket/`, ` s3://my-bucket/path/` ,`s3://my-bucket/path/prefix`
                        type:
                            - string
                            - array
                    region:
                        description: AWS region of the S3 bucket
                        type: string
                  required:
                    - driver
                    - bucket
                  title: S3
                  type: object
                - examples:
                    - client_id: my-client-id
                      driver: salesforce
                      endpoint: https://login.salesforce.com
                      password: mypassword
                      type: connector
                      username: myusername
                  properties:
                    client_id:
                        description: Client ID used for Salesforce OAuth authentication
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `salesforce`
                        type: string
                    endpoint:
                        description: Salesforce API endpoint URL
                        type: string
                    key:
                        description: Authentication key for Salesforce (secret)
                        type: string
                    password:
                        description: Salesforce account password (secret)
                        type: string
                    username:
                        description: Salesforce account username
                        type: string
                  required:
                    - driver
                    - username
                    - endpoint
                    - client_id
                  title: Salesforce
                  type: object
                - examples:
                    - bot_token: xoxb-my-bot-token
                      driver: slack
                      type: connector
                  properties:
                    bot_token:
                        description: Bot token used for authenticating Slack API requests
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `slack`
                        type: string
                  required:
                    - driver
                    - bot_token
                  title: Slack
                  type: object
                - examples:
                    - account: my_account_identifier
                      database: my_db
                      driver: snowflake
                      parallel_fetch_limit: 2
                      privateKey: '{{ .env.SNOWFLAKE_PRIVATE_KEY }}'
                      role: my_role
                      schema: my_schema
                      type: connector
                      user: my_user
                      warehouse: my_wh
                    - driver: snowflake
                      dsn: '{{ .env.SNOWFLAKE_DSN }}'
                      parallel_fetch_limit: 2
                      type: connector
                  properties:
                    account:
                        description: Snowflake account identifier. To find your Snowflake account identifier, look at your Snowflake account URL. The account identifier is everything before .snowflakecomputing.com
                        type: string
                    authenticator:
                        description: Optional authenticator type (e.g., SNOWFLAKE_JWT).
                        type: string
                    database:
                        description: Name of the Snowflake database.
                        type: string
                    driver:
                        const: snowflake
                        description: Refers to the driver type and must be driver `snowflake`
                        type: string
                    dsn:
                        description: "DSN (Data Source Name) for the Snowflake connection.\n\nThis is intended for **advanced configuration** where you want to specify\nproperties that are not explicitly defined above.  \nIt can only be used when the other connection fields (account, user, password,\ndatabase, schema, warehouse, role, authenticator, privateKey) are **not used**.\n\nFor details on private key generation and encoding, see the `privateKey` property.\n"
                        type: string
                    parallel_fetch_limit:
                        description: Maximum number of concurrent fetches during query execution.
                        type: integer
                    password:
                        description: Password for the Snowflake connection. _(deprecated, use privateKey instead)_
                        type: string
                    privateKey:
                        description: |
                            Private key for JWT authentication.
                            :::tip
                            Private key must be generated as a **PKCS#8 (nocrypt) key**, since the Snowflake Go driver
                            only supports unencrypted private keys. After generating, it must be **base64 URL encoded**.

                            Example commands to generate and encode:

                            ```bash
                            # Generate a 2048-bit unencrypted PKCS#8 private key
                            openssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out rsa_key.p8 -nocrypt

                            # Convert URL safe format for Snowflake
                            cat rsa_key.p8 | grep -v "\----" | tr -d '\n' | tr '+/' '-_'
                            ```
                            See: https://docs.snowflake.com/en/guide/key-pair-auth
                            :::
                        type: string
                    role:
                        description: Snowflake role to use.
                        type: string
                    schema:
                        description: Schema within the database to use.
                        type: string
                    user:
                        description: Username for the Snowflake connection.
                        type: string
                    warehouse:
                        description: Compute warehouse to use for queries.
                        type: string
                  required:
                    - type
                    - driver
                  title: Snowflake
                  type: object
                - examples:
                    - driver: sqlite
                      dsn: file:mydatabase.db
                      type: connector
                  properties:
                    driver:
                        description: Refers to the driver type and must be driver `sqlite`
                        type: string
                    dsn:
                        description: DSN(Data Source Name) for the sqlite connection
                        type: string
                  required:
                    - driver
                    - dsn
                  title: SQLite
                  type: object
        description: |
            Connector YAML files define how Rill connects to external data sources and OLAP engines. Each connector specifies a driver type and its required connection parameters.

            ## Available Connector Types

            ### _OLAP Engines_
            - [**ClickHouse**](#clickhouse) - ClickHouse analytical database
            - [**Druid**](#druid) - Apache Druid
            - [**DuckDB**](#duckdb) - Embedded DuckDB engine (default)
            - [**MotherDuck**](#motherduck) - MotherDuck cloud database
            - [**Pinot**](#pinot) - Apache Pinot

            ### _Data Warehouses_
            - [**Athena**](#athena) - Amazon Athena
            - [**BigQuery**](#bigquery) - Google BigQuery
            - [**Redshift**](#redshift) - Amazon Redshift
            - [**Snowflake**](#snowflake) - Snowflake data warehouse

            ### _Databases_
            - [**MySQL**](#mysql) - MySQL databases
            - [**PostgreSQL**](#postgres) - PostgreSQL databases
            - [**SQLite**](#sqlite) - SQLite databases

            ### _Cloud Storage_
            - [**Azure**](#azure) - Azure Blob Storage
            - [**GCS**](#gcs) - Google Cloud Storage
            - [**S3**](#s3) - Amazon S3 storage

            ### _Other_
            - [**Extenral DuckDB**](#external-duckdb) - External DuckDB database
            - [**HTTPS**](#https) - Public files via HTTP/HTTPS
            - [**OpenAI**](#openai) - OpenAI data
            - [**Salesforce**](#salesforce) - Salesforce data
            - [**Slack**](#slack) - Slack data

            :::warning Security Recommendation
            For all credential parameters (passwords, tokens, keys), use environment variables with the syntax `{{.env.connector.<connector_driver>.<parameter_name>}}`. This keeps sensitive data out of your YAML files and version control. See our [credentials documentation](/developer/build/connectors/credentials/) for complete setup instructions.
            :::
        id: connectors
        title: Connector YAML
        type: object
        ```

        ## Examples

        ### DuckDB: Managed

        Explicit:

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb
        managed: true
        ```

        or relying on defaults:

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb
        ```

        ### DuckDB: With init_sql for S3 secrets

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb

        init_sql: |
          CREATE SECRET IF NOT EXISTS s3 (
            TYPE S3,
            KEY_ID '{{ .env.aws_access_key_id }}',
            SECRET '{{ .env.aws_secret_access_key }}',
            REGION 'us-east-1'
          )
        ```

        This is now deprecated in favor of creating a dedicated `s3.yaml` connector file, which Rill will automatically load and create as a secret in DuckDB.

        ### DuckDB: With extensions

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb

        init_sql: |
          INSTALL spatial;
          LOAD spatial;
        ```

        ### DuckDB: MotherDuck database with existing tables

        ```yaml
        # connectors/motherduck.yaml
        type: connector
        driver: duckdb
        path: "md:my_database"
        token: "{{ .env.motherduck_token }}"
        schema_name: main
        mode: read
        ```

        ### ClickHouse: Cloud with SSL

        ```yaml
        # connectors/clickhouse.yaml
        type: connector
        driver: clickhouse
        host: "abc123.us-east-1.aws.clickhouse.cloud"
        port: 8443
        username: "default"
        password: "{{ .env.clickhouse_password }}"
        database: "default"
        ssl: true
        ```

        ### ClickHouse: Readwrite with cluster

        ```yaml
        # connectors/clickhouse.yaml
        type: connector
        driver: clickhouse
        mode: readwrite

        host: "{{ .env.clickhouse_host }}"
        port: 9440
        username: "{{ .env.clickhouse_user }}"
        password: "{{ .env.clickhouse_password }}"
        database: "default"
        cluster: "my_cluster"
        ssl: true
        ```

        ### ClickHouse: Dev/prod configuration

        ```yaml
        # connectors/clickhouse.yaml
        type: connector
        driver: clickhouse
        mode: readwrite

        dev:
          managed: true

        prod:
          host: "{{ .env.clickhouse_host }}"
          port: 9440
          username: "{{ .env.clickhouse_user }}"
          password: "{{ .env.clickhouse_password }}"
          database: "default"
          cluster: "{{ .env.clickhouse_cluster }}"
          ssl: true
        ```

        ### S3: Basic with credentials and region

        ```yaml
        # connectors/s3.yaml
        type: connector
        driver: s3
        aws_access_key_id: "{{ .env.aws_access_key_id }}"
        aws_secret_access_key: "{{ .env.aws_secret_access_key }}"
        region: us-west-2
        ```

        ### S3: Cloudflare R2 (S3-compatible)

        ```yaml
        # connectors/r2.yaml
        type: connector
        driver: s3
        aws_access_key_id: "{{ .env.r2_access_key_id }}"
        aws_secret_access_key: "{{ .env.r2_secret_access_key }}"
        endpoint: "https://{{ .env.r2_account_id }}.r2.cloudflarestorage.com"
        region: auto
        ```

        ### GCS: Minimal (uses default credentials)

        ```yaml
        # connectors/gcs.yaml
        type: connector
        driver: gcs
        ```

        ### GCS: With explicit credentials

        ```yaml
        # connectors/gcs.yaml
        type: connector
        driver: gcs
        google_application_credentials: "{{ .env.gcs_service_account_json }}"
        ```

        ### BigQuery

        ```yaml
        # connectors/bigquery.yaml
        type: connector
        driver: bigquery
        project_id: "my-gcp-project"
        google_application_credentials: "{{ .env.bigquery_service_account_json }}"
        ```

        ### Snowflake: Basic with DSN

        ```yaml
        # connectors/snowflake.yaml
        type: connector
        driver: snowflake
        dsn: "{{ .env.snowflake_dsn }}"
        ```

        ### Postgres

        ```yaml
        # connectors/postgres.yaml
        type: connector
        driver: postgres
        host: "{{ .env.postgres_host }}"
        port: 5432
        user: "{{ .env.postgres_user }}"
        password: "{{ .env.postgres_password }}"
        dbname: "analytics"
        sslmode: require
        ```

        ### Druid: Host-based

        ```yaml
        # connectors/druid.yaml
        type: connector
        driver: druid
        host: "{{ .env.druid_host }}"
        port: 8888
        username: "{{ .env.druid_user }}"
        password: "{{ .env.druid_password }}"
        ssl: true
        ```

        ### Redshift: Serverless

        ```yaml
        # connectors/redshift.yaml
        type: connector
        driver: redshift
        aws_access_key_id: "{{ .env.aws_access_key_id }}"
        aws_secret_access_key: "{{ .env.aws_secret_access_key }}"
        workgroup: "my-workgroup"
        region: us-east-1
        database: "analytics"
        ```

        ### OpenAI

        ```yaml
        # connectors/openai.yaml
        type: connector
        driver: openai
        api_key: "{{ .env.openai_api_key }}"
        ```

        ### Slack

        ```yaml
        # connectors/slack.yaml
        type: connector
        driver: slack
        bot_token: "{{ .env.slack_bot_token }}"
        ```

        ### Pinot

        ```yaml
        # connectors/pinot.yaml
        type: connector
        driver: pinot
        broker_host: "{{ .env.pinot_broker_host }}"
        controller_host: "{{ .env.pinot_controller_host }}"
        username: "{{ .env.pinot_user }}"
        password: "{{ .env.pinot_password }}"
        ssl: true
        ```
    - role: user
      content_type: text
      content: |-
        You should develop a Rill project file based on the following task description:
        - Develop file at path: /connectors/s3.yaml
        - The file should be of type: connector
        - Task description: Create a new S3 connector resource.

        Requirements:
        - File path: connectors/s3.yaml
        - Resource name should be `s3` (derived from filename).
        - Use driver: s3.
        - Use env vars already present in the project status: `connector.s3.aws_access_key_id` and `connector.s3.aws_secret_access_key`.
        - Configure region conservatively (us-east-1) but allow override via env var if present (do not introduce new required env vars). If you include optional env var references, guard them with defaults if supported.
        - Keep minimal properties needed for Rill to access public/private buckets.

        Return valid YAML for a Rill connector.

        Here is some important context:
        - You are running as a sub-agent of a larger developer agent. Stay aligned on your specific task and avoid extra discovery.
        - When you call 'write_file', if it returns a parse or reconcile error, do your best to fix the issue and try again. If you think the error is unrelated to the current path, let the parent agent know to handle it.

        Here is some additional context that may or may not be relevant to your task:
        - The project's default OLAP connector is named duckdb (driver: duckdb).
  response:
    - role: assistant
      content_type: tool_call
      id: call_qoSLdJ791v8TjfOWzgElBylj
      tool_name: write_file
      content: '{"contents":"type: connector\ndriver: s3\n\n# Credentials are sourced from existing env vars (see project status)\naws_access_key_id: \"{{ .env.connector.s3.aws_access_key_id }}\"\naws_secret_access_key: \"{{ .env.connector.s3.aws_secret_access_key }}\"\n\n# Default conservatively to us-east-1; can be overridden by setting env var\nregion: \"{{ .env.connector.s3.region | default \\\"us-east-1\\\" }}\"\n","path":"connectors/s3.yaml"}'
- index: 5
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Models as SQL files:** Always create new models as `.yaml` files, not `.sql` files (which are harder to extend later).\n- **Not creating connector files:** When Rill has native support for a connector (like S3 or BigQuery), always create a dedicated connector resource file for it.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. This also includes models that load external data using a native SQL function, like `read_parquet(...)` or `s3(...)`. Non-materialized models become views, which re-execute on every query.\n- **Referencing non-existant environment variables:** Only reference environment variables that are present in `.env` (returned in `env` from `project_status`). If you need the user to add another environment variable, stop and ask them to do so.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: system
      content_type: text
      content: |-
        # Instructions for developing a connector in Rill

        ## Introduction

        Connectors are resources that contain credentials and settings for connecting to external systems. They are typically found at the root of the project's DAG, providing access to data sources and services that power downstream resources like models and metrics views.

        Connectors are usually lightweight resources. When reconciled, they validate the connection to the external system but do not move or process data. The main exception is managed OLAP connectors (with `managed: true`), which trigger database provisioning.

        ### Driver capabilities

        Each connector uses a **driver** that implements one or more capabilities:

        - **OLAP database**: Can power metrics views and dashboards (e.g., `duckdb`, `clickhouse`)
        - **SQL database**: Can run SQL queries as model inputs (e.g., `postgres`, `bigquery`, `snowflake`)
        - **Information schema**: Can list tables and their schemas (e.g., `duckdb`, `bigquery`)
        - **Object store**: Can list, read, and write flat files (e.g., `s3`, `gcs`)
        - **Notifier**: Can send notifications and alerts (e.g., `slack`)
        - **AI**: Can generate embeddings or responses (e.g., `openai`)

        ## Core Concepts

        ### Naming conventions

        Connectors are typically named after their driver (e.g., a file `connectors/duckdb.yaml` creates a connector named `duckdb`). Use descriptive names when you have multiple connectors of the same type:
        - `connectors/s3_data.yaml` and `connectors/s3_staging.yaml`
        - `connectors/clickhouse_prod.yaml` and `connectors/clickhouse_dev.yaml`

        ### Secrets management

        Always store sensitive credentials in `.env` and reference them using template syntax:

        ```yaml
        type: connector
        driver: s3
        aws_access_key_id: "{{ .env.aws_access_key_id }}"
        aws_secret_access_key: "{{ .env.aws_secret_access_key }}"
        ```

        NOTE: Some legacy projects use the deprecated `.vars` instead of `.env`.

        ### Managed connectors

        OLAP connectors can be provisioned automatically by Rill using `managed: true`. This is supported for `duckdb` and `clickhouse` drivers:

        ```yaml
        type: connector
        driver: duckdb
        managed: true
        ```

        When a managed connector is reconciled, Rill provisions the database infrastructure. The user is billed for the CPU, memory, and disk usage of the provisioned database.

        ### Access modes

        Control read/write access using the `mode` property:

        - `mode: read`: Prevents Rill models from writing to this connector
        - `mode: readwrite`: Allows both reading and writing (default for managed connectors)

        Use `mode: read` when connecting to external databases with pre-existing tables to prevent unintended modifications.

        ### Dev/Prod configuration

        Use `dev:` and `prod:` blocks for environment-specific settings:

        ```yaml
        type: connector
        driver: clickhouse
        mode: readwrite

        # Use a local database in development to prevent overwriting data in the production cluster when iterating on changes.
        dev:
          managed: true

        prod:
          host: "{{ .env.clickhouse_host }}"
          port: 9440
          username: "{{ .env.clickhouse_user }}"
          password: "{{ .env.clickhouse_password }}"
          ssl: true
        ```

        ## Driver-Specific Notes

        ### DuckDB

        DuckDB is Rill's default embedded OLAP database. Key properties:

        - `managed: true`: Rill provisions and manages the database
        - `init_sql`: SQL to run at startup (install extensions, create secrets, attach databases)

        For MotherDuck (cloud DuckDB), use the `path` property with `md:` prefix:

        ```yaml
        type: connector
        driver: duckdb
        path: "md:my_database"
        token: "{{ .env.motherduck_token }}"
        ```

        ### ClickHouse

        ClickHouse can be user-managed or Rill-managed. Key properties:

        - `managed: true`: Rill provisions and manages an empty Clickhouse cluster. If set, don't set any other connector properties.
        - `host`, `port`, `username`, `password`: Connection credentials
        - `database`: Target database name
        - `ssl: true`: Required for ClickHouse Cloud
        - `cluster`: Cluster name for multi-node Clickhouse clusters
        - `dsn`: Alternative connection string format (format: `clickhouse://host:port?username=<username>&...`)

        Common ports:
        - `8443`: HTTPS native protocol (ClickHouse Cloud)
        - `9440`: Secure native protocol
        - `9000`: Native protocol (non-SSL)

        ### S3

        AWS S3 and S3-compatible storage. Key properties:

        - `aws_access_key_id`, `aws_secret_access_key`: AWS credentials
        - `region`: AWS region
        - `endpoint`: Custom endpoint for S3-compatible services (R2, MinIO, GCS interop)
        - `path_prefixes`: A list of bucket paths that the connector can access, e.g. `[s3://my-bucket]`; useful for improving bucket introspection

        ### GCS

        Google Cloud Storage. Key properties:

        - `google_application_credentials`: Service account JSON (must be a literal JSON string value)
        - `key_id`: HMAC key ID to use instead of a service account JSON; required for direct use with DuckDB and Clickhouse through S3 compatibility
        - `secret`: HMAC secret to use instead of a service account JSON; required for direct use with DuckDB and Clickhouse through S3 compatibility

        ### BigQuery

        Google BigQuery. Key properties:

        - `project_id`: GCP project ID
        - `google_application_credentials`: Service account JSON

        ### Snowflake

        Snowflake data warehouse. Key properties:

        - `account`, `user`, `privateKey`, `database`, `schema`, `warehouse`, `role`: Connection parameters
        - `dsn`: Connection string to use instead of separate connection parameters

        ### Postgres

        PostgreSQL database. Key properties:

        - `host`, `port`, `user`, `password`, `dbname`: Connection parameters
        - `sslmode`: SSL mode (`disable`, `require`, `verify-full`, etc.)

        ### Druid

        Apache Druid. Can be configured via host/port or DSN:

        - `host`, `port`, `username`, `password`, `ssl`: Direct connection
        - `dsn`: Full connection string

        ### Redshift

        Amazon Redshift. Key properties:

        - `aws_access_key_id`, `aws_secret_access_key`: AWS credentials
        - `workgroup`: Redshift Serverless workgroup name
        - `region`: AWS region
        - `database`: Database name

        ### Athena

        Amazon Athena. Key properties:

        - `aws_access_key_id`, `aws_secret_access_key`: AWS credentials
        - `workgroup`: Redshift Serverless workgroup name
        - `region`: AWS region
        - `output_location`: S3 path in format `s3://bucket/path` to store temporary query results in (Athena only)

        ### Other drivers

        - **Slack**: Use `bot_token` for alert notifications
        - **OpenAI**: Use `api_key` for AI-powered features
        - **HTTPS**: Simple connector for public HTTP sources
        - **Pinot**: Use `broker_host`, `controller_host`, `username`, `password`

        ## JSON Schema

        Here is a full JSON schema for the connector syntax:

        ```
        allOf:
            - properties:
                type:
                    const: connector
                    description: Refers to the resource type and must be `connector`
                    type: string
              required:
                - type
              title: Properties
              type: object
            - $ref: '#/definitions/common_properties'
            - oneOf:
                - examples:
                    - allow_host_access: true
                      aws_access_key_id: myawsaccesskey
                      aws_access_token: mytemporarytoken
                      aws_region: us-east-1
                      aws_secret_access_key: myawssecretkey
                      driver: athena
                      external_id: MyExternalID
                      output_location: s3://my-bucket/athena-output/
                      role_arn: arn:aws:iam::123456789012:role/MyRole
                      role_session_name: MySession
                      type: connector
                      workgroup: primary
                  properties:
                    allow_host_access:
                        description: Allow the Athena client to access host environment configurations such as environment variables or local AWS credential files. Defaults to true, enabling use of credentials and settings from the host environment unless explicitly disabled.
                        type: boolean
                    aws_access_key_id:
                        description: AWS Access Key ID used for authentication. Required when using static credentials directly or as base credentials for assuming a role.
                        type: string
                    aws_access_token:
                        description: AWS session token used with temporary credentials. Required only if the Access Key and Secret Key are part of a temporary session credentials.
                        type: string
                    aws_region:
                        description: AWS region where Athena and the result S3 bucket are located (e.g., us-east-1). Defaults to 'us-east-1' if not specified.
                        type: string
                    aws_secret_access_key:
                        description: AWS Secret Access Key paired with the Access Key ID. Required when using static credentials directly or as base credentials for assuming a role.
                        type: string
                    driver:
                        const: athena
                        description: Refers to the driver type and must be driver `athena`
                        type: string
                    external_id:
                        description: External ID required by some roles when assuming them, typically for cross-account access. Used only if 'role_arn' is specified and the role's trust policy requires it.
                        type: string
                    output_location:
                        description: S3 URI where Athena query results should be stored (e.g., s3://your-bucket/athena/results/). Optional if the selected workgroup has a default result configuration.
                        type: string
                    role_arn:
                        description: ARN of the IAM role to assume. When specified, the SDK uses the base credentials to call STS AssumeRole and obtain temporary credentials scoped to this role.
                        type: string
                    role_session_name:
                        description: Session name to associate with the STS AssumeRole session. Used only if 'role_arn' is specified. Useful for identifying and auditing the session.
                        type: string
                    workgroup:
                        description: Athena workgroup to use for query execution. Defaults to 'primary' if not specified.
                        type: string
                  required:
                    - driver
                  title: Athena
                  type: object
                - examples:
                    - azure_storage_account: mystorageaccount
                      azure_storage_key: credentialstring
                      driver: azure
                      type: connector
                  properties:
                    allow_host_access:
                        description: Allow access to host environment configuration
                        type: boolean
                    azure_storage_account:
                        description: Azure storage account name
                        type: string
                    azure_storage_connection_string:
                        description: Optional azure connection string for storage account
                        type: string
                    azure_storage_key:
                        description: Azure storage access key
                        type: string
                    azure_storage_sas_token:
                        description: Optional azure SAS token for authentication
                        type: string
                    driver:
                        const: azure
                        description: Refers to the driver type and must be driver `azure`
                        type: string
                    path_prefixes:
                        description: |
                            A list of container or virtual directory prefixes that this connector is allowed to access.
                            Useful when different containers or paths use different credentials, allowing the system
                            to route access through the appropriate connector based on the blob path.
                            Example: `azure://my-bucket/`, ` azure://my-bucket/path/` ,`azure://my-bucket/path/prefix`
                        type:
                            - string
                            - array
                  required:
                    - driver
                    - azure_storage_account
                    - azure_storage_key
                  title: Azure
                  type: object
                - examples:
                    - allow_host_access: true
                      driver: bigquery
                      google_application_credentials: credentialjsonstring
                      project_id: my-project-id
                      type: connector
                  properties:
                    allow_host_access:
                        description: Enable the BigQuery client to use credentials from the host environment when no service account JSON is provided. This includes Application Default Credentials from environment variables, local credential files, or the Google Compute Engine metadata server. Defaults to true, allowing seamless authentication in GCP environments.
                        type: boolean
                    dataset_id:
                        description: BigQuery dataset ID
                        type: string
                    driver:
                        const: bigquery
                        description: Refers to the driver type and must be driver `bigquery`
                        type: string
                    google_application_credentials:
                        description: Raw contents of the Google Cloud service account key (in JSON format) used for authentication.
                        type: string
                    location:
                        description: BigQuery dataset location
                        type: string
                    project_id:
                        description: Google Cloud project ID
                        type: string
                  required:
                    - driver
                  title: BigQuery
                  type: object
                - examples:
                    - cluster: mycluster
                      database: mydatabase
                      driver: clickhouse
                      host: localhost
                      managed: false
                      mode: readwrite
                      password: mypassword
                      port: 9000
                      ssl: true
                      type: connector
                      username: myusername
                  properties:
                    can_scale_to_zero:
                        description: Indicates if the database can scale to zero
                        type: boolean
                    cluster:
                        description: Cluster name, required for running distributed queries
                        type: string
                    conn_max_lifetime:
                        description: Maximum time a connection may be reused
                        type: string
                    database:
                        description: Name of the ClickHouse database within the cluster
                        type: string
                    dial_timeout:
                        description: Timeout for dialing the ClickHouse server
                        type: string
                    driver:
                        const: clickhouse
                        description: Refers to the driver type and must be driver `clickhouse`
                        type: string
                    dsn:
                        description: DSN(Data Source Name) for the ClickHouse connection
                        type: string
                    embed_port:
                        description: Port to run ClickHouse locally (0 for random port)
                        type: integer
                    host:
                        description: Host where the ClickHouse instance is running
                        type: string
                    log_queries:
                        description: Controls whether to log raw SQL queries
                        type: boolean
                    managed:
                        description: '`true` means Rill will provision the connector using the default provisioner. `false` disables automatic provisioning.'
                        type: boolean
                    max_idle_conns:
                        description: Maximum number of idle connections in the pool
                        type: integer
                    max_open_conns:
                        description: Maximum number of open connections to the database
                        type: integer
                    mode:
                        description: '`read` - Controls the operation mode for the ClickHouse connection. Defaults to ''read'' for safe operation with external databases. Set to ''readwrite'' to enable model creation and table mutations. Note: When ''managed: true'', this is automatically set to ''readwrite''.'
                        type: string
                    password:
                        description: Password for authentication
                        type: string
                    port:
                        description: Port where the ClickHouse instance is accessible
                        type: integer
                    query_settings:
                        description: query settings to be set on dashboard queries. `query_settings_override` takes precedence over these settings and if set these are ignored. Each setting must be separated by a comma. Example `max_threads = 8, max_memory_usage = 10000000000`
                        type: string
                    query_settings_override:
                        description: override the default settings used in queries. Changing the default settings can lead to incorrect query results and is generally not recommended. If you need to add settings, use `query_settings`
                        type: string
                    read_timeout:
                        description: Maximum time for a connection to read data
                        type: string
                    ssl:
                        description: Indicates whether a secured SSL connection is required
                        type: boolean
                    username:
                        description: Username for authentication
                        type: string
                  required:
                    - driver
                  title: ClickHouse
                  type: object
                - examples:
                    - driver: druid
                      host: localhost
                      password: mypassword
                      port: 8082
                      ssl: true
                      type: connector
                      username: myusername
                  properties:
                    driver:
                        const: druid
                        description: Refers to the driver type and must be driver `druid`
                        type: string
                    dsn:
                        description: Data Source Name (DSN) for connecting to Druid
                        type: string
                    host:
                        description: Hostname of the Druid coordinator or broker
                        type: string
                    log_queries:
                        description: Log raw SQL queries sent to Druid
                        type: boolean
                    max_open_conns:
                        description: Maximum number of open database connections (0 = default, -1 = unlimited)
                        type: integer
                    password:
                        description: Password for authenticating with Druid
                        type: string
                    port:
                        description: Port number of the Druid service
                        type: integer
                    skip_version_check:
                        description: Skip checking Druid version compatibility
                        type: boolean
                    ssl:
                        description: Enable SSL for secure connection
                        type: boolean
                    username:
                        description: Username for authenticating with Druid
                        type: string
                  required:
                    - driver
                    - dsn
                  title: Druid
                  type: object
                - examples:
                    - allow_host_access: true
                      cpu: 4
                      driver: duckdb
                      init_sql: INSTALL httpfs; LOAD httpfs;
                      log_queries: true
                      memory_limit_gb: 16
                      mode: readwrite
                      pool_size: 5
                      read_write_ratio: 0.7
                      type: connector
                  properties:
                    allow_host_access:
                        description: Whether access to local environment and file system is allowed
                        type: boolean
                    attach:
                        description: Full ATTACH statement to attach a DuckDB database
                        type: string
                    boot_queries:
                        description: Deprecated - Use init_sql instead
                        type: string
                    conn_init_sql:
                        description: SQL executed when a new connection is initialized
                        type: string
                    cpu:
                        description: Number of CPU cores available to the database
                        minimum: 1
                        type: integer
                    create_secrets_from_connectors:
                        description: List of connector names for which temporary secrets should be created before executing the SQL.
                        type:
                            - string
                            - array
                    database_name:
                        description: Name of the attached DuckDB database (auto-detected if not set)
                        type: string
                    driver:
                        const: duckdb
                        description: Must be "duckdb"
                        type: string
                    init_sql:
                        description: SQL executed during database initialization
                        type: string
                    log_queries:
                        description: Whether to log raw SQL queries executed through OLAP
                        type: boolean
                    memory_limit_gb:
                        description: Amount of memory in GB available to the database
                        minimum: 1
                        type: integer
                    mode:
                        default: read
                        description: Set the mode for the DuckDB connection.
                        enum:
                            - read
                            - readwrite
                        type: string
                    path:
                        description: Path to external DuckDB database
                        type: string
                    pool_size:
                        description: Number of concurrent connections and queries allowed
                        minimum: 1
                        type: integer
                    read_write_ratio:
                        default: 0.5
                        description: Ratio of resources allocated to read vs write operations
                        maximum: 1
                        minimum: 0
                        type: number
                    schema_name:
                        description: Default schema used by the DuckDB database
                        type: string
                  required:
                    - driver
                  title: DuckDB
                  type: object
                - examples:
                    - driver: duckdb
                      mode: read
                      path: /path/to/my-duckdb-database.db
                      type: connector
                  properties:
                    driver:
                        const: duckdb
                        description: Refers to the driver type and must be driver `duckdb`
                        type: string
                    mode:
                        default: read
                        description: Set the mode for the DuckDB connection.
                        enum:
                            - read
                            - readwrite
                        type: string
                    path:
                        description: Path to the DuckDB database
                        type: string
                  required:
                    - driver
                    - db
                  title: External DuckDB
                  type: object
                - examples:
                    - driver: gcs
                      google_application_credentials: credentialjsonstring
                      type: connector
                  properties:
                    allow_host_access:
                        description: Allow access to host environment configuration
                        type: boolean
                    driver:
                        const: gcs
                        description: Refers to the driver type and must be driver `gcs`
                        type: string
                    google_application_credentials:
                        description: Google Cloud credentials JSON string
                        type: string
                    key_id:
                        description: Optional S3-compatible Key ID when used in compatibility mode
                        type: string
                    path_prefixes:
                        description: "A list of bucket path prefixes that this connector is allowed to access. \nUseful when different buckets or bucket prefixes use different credentials, \nallowing the system to select the appropriate connector based on the bucket path.\nExample: `gs://my-bucket/`, ` gs://my-bucket/path/` ,`gs://my-bucket/path/prefix`\n"
                        type:
                            - string
                            - array
                    secret:
                        description: Optional S3-compatible Secret when used in compatibility mode
                        type: string
                  required:
                    - driver
                    - bucket
                  title: GCS
                  type: object
                - examples:
                    - driver: https
                      headers:
                        Authorization: Bearer my-token
                      type: connector
                  properties:
                    driver:
                        const: https
                        description: Refers to the driver type and must be driver `https`
                        type: string
                    headers:
                        additionalProperties:
                            type: string
                        description: HTTP headers to include in the request
                        type: object
                    path_prefixes:
                        description: |
                            A list of HTTP/HTTPS URL prefixes that this connector is allowed to access.
                            Useful when different URL namespaces use different credentials, enabling the
                            system to choose the appropriate connector based on the URL path.
                            Example: `https://example.com/`, ` https://example.com/path/` ,`https://example.com/path/prefix`
                        type:
                            - string
                            - array
                  required:
                    - driver
                    - path
                  title: HTTPS
                  type: object
                - examples:
                    - driver: duckdb
                      path: md:my_database
                      schema_name: my_schema
                      token: '{{ .env.connector.motherduck.token }}'
                      type: connector
                  properties:
                    create_secrets_from_connectors:
                        description: List of connector names for which temporary secrets should be created before executing the SQL.
                        type:
                            - string
                            - array
                    driver:
                        const: duckdb
                        description: Refers to the driver type and must be driver `duckdb`.
                        type: string
                    init_sql:
                        description: SQL executed during database initialization.
                        type: string
                    mode:
                        default: read
                        description: Set the mode for the MotherDuck connection. By default, it is set to 'read' which allows only read operations. Set to 'readwrite' to enable model creation and table mutations.
                        enum:
                            - read
                            - readwrite
                        type: string
                    path:
                        description: Path to your MD database
                        type: string
                    schema_name:
                        description: Define your schema if not main, uses main by default
                        type: string
                    token:
                        description: MotherDuck token
                        type: string
                  required:
                    - driver
                    - path
                    - token
                  title: MotherDuck
                  type: object
                - examples:
                    - database: mydb
                      driver: mysql
                      host: localhost
                      password: p@ss
                      port: 3306
                      ssl-mode: preferred
                      type: connector
                      user: user
                    - driver: mysql
                      dsn: mysql://user:p%40ss@localhost:3306/mydb?ssl-mode=preferred
                      type: connector
                  properties:
                    database:
                        description: Name of the MySQL database
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `mysql`
                        type: string
                    dsn:
                        description: |
                            **Data Source Name (DSN)** for the MySQL connection, provided in [MySQL URI format](https://dev.mysql.com/doc/refman/8.4/en/connecting-using-uri-or-key-value-pairs.html#connecting-using-uri).
                            The DSN must follow the standard MySQL URI scheme:
                            ```text
                            mysql://user:password@host:3306/my-db
                            ```
                            Rules for special characters in password:
                            - The following characters are allowed [unescaped in the URI](https://datatracker.ietf.org/doc/html/rfc3986#section-2.3): `~` `.` `_` `-`
                            - All other special characters must be percent-encoded (`%XX` format).
                            ```text
                            mysql://user:pa%40ss@localhost:3306/my-db   # password contains '@'
                            mysql://user:pa%3Ass@localhost:3306/my-db   # password contains ':'
                            ```
                        type: string
                    host:
                        description: Hostname of the MySQL server
                        type: string
                    password:
                        description: Password for authentication
                        type: string
                    port:
                        description: Port number for the MySQL server
                        type: integer
                    ssl-mode:
                        description: 'ssl mode options: `disabled`, `preferred`, or `required`.'
                        type: string
                    user:
                        description: Username for authentication
                        type: string
                  required:
                    - driver
                  title: MySQL
                  type: object
                - examples:
                    - api_key: my-api-key
                      api_type: openai
                      api_version: "2023-05-15"
                      base_url: https://api.openai.com/v1
                      driver: openai
                      model: gpt-4o
                      type: connector
                  properties:
                    api_key:
                        description: API key for connecting to OpenAI
                        type: string
                    api_type:
                        description: The type of OpenAI API to use
                        type: string
                    api_version:
                        description: The version of the OpenAI API to use (e.g., '2023-05-15'). Required when API Type is AZURE or AZURE_AD
                        type: string
                    base_url:
                        description: The base URL for the OpenAI API (e.g., 'https://api.openai.com/v1')
                        type: string
                    driver:
                        description: The driver type, must be set to "openai"
                        type: string
                    model:
                        description: The OpenAI model to use (e.g., 'gpt-4o')
                        type: string
                  required:
                    - api_key
                  title: OpenAI
                  type: object
                - examples:
                    - broker_host: localhost
                      broker_port: 9000
                      controller_host: localhost
                      controller_port: 9000
                      driver: pinot
                      log_queries: true
                      max_open_conns: 100
                      password: mypassword
                      ssl: true
                      type: connector
                      username: myusername
                  properties:
                    broker_host:
                        description: Hostname of the Pinot broker
                        type: string
                    broker_port:
                        description: Port number for the Pinot broker
                        type: integer
                    controller_host:
                        description: Hostname of the Pinot controller
                        type: string
                    controller_port:
                        description: Port number for the Pinot controller
                        type: integer
                    driver:
                        description: Refers to the driver type and must be driver `pinot`
                        type: string
                    dsn:
                        description: DSN(Data Source Name) for the Pinot connection
                        type: string
                    log_queries:
                        description: Log raw SQL queries executed through Pinot
                        type: boolean
                    max_open_conns:
                        description: Maximum number of open connections to the Pinot database
                        type: integer
                    password:
                        description: Password for authenticating with Pinot
                        type: string
                    ssl:
                        description: Enable SSL connection to Pinot
                        type: boolean
                    username:
                        description: Username for authenticating with Pinot
                        type: string
                  required:
                    - driver
                    - dsn
                    - broker_host
                    - controller_host
                  title: Pinot
                  type: object
                - examples:
                    - dbname: mydatabase
                      driver: postgres
                      host: localhost
                      password: mypassword
                      port: 5432
                      sslmode: prefer
                      type: connector
                      user: myusername
                    - driver: postgres
                      dsn: user=myusername password='my pass\'word' host=localhost port=5432 dbname=mydatabase sslmode=prefer
                      type: connector
                    - driver: postgres
                      dsn: postgres://myusername:p%40ss@localhost:5432/mydatabase?sslmode=prefer
                      type: connector
                  properties:
                    dbname:
                        description: Name of the Postgres database
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `postgres`
                        type: string
                    dsn:
                        description: |
                            **Data Source Name (DSN)** for the PostgreSQL connection, provided in
                            [PostgreSQL connection string format](https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING).
                            PostgreSQL supports both **key=value format** and **URI format**.

                            key=value format example:
                            ```text
                            user=user password=password host=host port=5432 dbname=mydb
                            ```
                            Rules for key=value format for special characters:
                            - To write an empty value, or a value containing spaces, `=`, single quotes, or backslashes, surround it with single quotes.
                            - Single quotes and backslashes inside a value must be escaped with a backslash (`\'` and `\\`).

                            URI format example:
                            ```text
                            postgres://user:password@host:5432/mydb
                            ```

                            Rules for URI format:
                            - The following characters are allowed [unescaped in the URI](https://datatracker.ietf.org/doc/html/rfc3986#section-2.3): `~` `.` `_` `-`
                            - All other special characters must be percent-encoded (`%XX` format).

                            Examples (URI format with encoded characters):
                            ```text
                            postgres://user:pa%40ss@localhost:5432/my-db   # '@' is encoded as %40
                            postgres://user:pa%3Ass@localhost:5432/my-db   # ':' is encoded as %3A
                            ```
                        type: string
                    host:
                        description: Hostname of the Postgres server
                        type: string
                    password:
                        description: Password for authentication
                        type: string
                    port:
                        description: Port number for the Postgres server
                        type: string
                    sslmode:
                        description: 'ssl mode options: `disable`, `allow`, `prefer` or `require`.'
                        type: string
                    user:
                        description: Username for authentication
                        type: string
                  required:
                    - driver
                  title: Postgres
                  type: object
                - examples:
                    - aws_access_key_id: my-access-key-id
                      aws_access_token: my-access-token
                      aws_secret_access_key: my-secret-access-key
                      cluster_identifier: my-cluster-identifier
                      database: mydatabase
                      driver: redshift
                      region: us-east-1
                      type: connector
                      workgroup: my-workgroup
                  properties:
                    aws_access_key_id:
                        description: AWS Access Key ID used for authenticating with Redshift.
                        type: string
                    aws_access_token:
                        description: AWS Session Token for temporary credentials (optional).
                        type: string
                    aws_secret_access_key:
                        description: AWS Secret Access Key used for authenticating with Redshift.
                        type: string
                    cluster_identifier:
                        description: Cluster identifier for provisioned Redshift clusters, in case of Redshift Serverless use 'workgroup' .
                        type: string
                    database:
                        description: Name of the Redshift database to query.
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `redshift`
                        type: string
                    region:
                        description: AWS region where the Redshift cluster or workgroup is hosted (e.g., 'us-east-1').
                        type: string
                    workgroup:
                        description: Workgroup name for Redshift Serverless, in case of provisioned Redshift clusters use 'cluster_identifier'.
                        type: string
                  required:
                    - driver
                    - aws_access_key_id
                    - aws_secret_access_key
                    - database
                  title: Redshift
                  type: object
                - examples:
                    - aws_access_key_id: my-access-key-id
                      aws_access_token: my-access-token
                      aws_secret_access_key: my-secret-access-key
                      driver: s3
                      endpoint: https://my-s3-endpoint.com
                      region: us-east-1
                      type: connector
                  properties:
                    allow_host_access:
                        description: Allow access to host environment configuration
                        type: boolean
                    aws_access_key_id:
                        description: AWS Access Key ID used for authentication
                        type: string
                    aws_access_token:
                        description: Optional AWS session token for temporary credentials
                        type: string
                    aws_secret_access_key:
                        description: AWS Secret Access Key used for authentication
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `s3`
                        type: string
                    endpoint:
                        description: Optional custom endpoint URL for S3-compatible storage
                        type: string
                    path_prefixes:
                        description: |
                            A list of bucket path prefixes that this connector is allowed to access.
                            Useful when different buckets or bucket prefixes use different credentials,
                            allowing the system to select the appropriate connector based on the bucket path.
                            Example: `s3://my-bucket/`, ` s3://my-bucket/path/` ,`s3://my-bucket/path/prefix`
                        type:
                            - string
                            - array
                    region:
                        description: AWS region of the S3 bucket
                        type: string
                  required:
                    - driver
                    - bucket
                  title: S3
                  type: object
                - examples:
                    - client_id: my-client-id
                      driver: salesforce
                      endpoint: https://login.salesforce.com
                      password: mypassword
                      type: connector
                      username: myusername
                  properties:
                    client_id:
                        description: Client ID used for Salesforce OAuth authentication
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `salesforce`
                        type: string
                    endpoint:
                        description: Salesforce API endpoint URL
                        type: string
                    key:
                        description: Authentication key for Salesforce (secret)
                        type: string
                    password:
                        description: Salesforce account password (secret)
                        type: string
                    username:
                        description: Salesforce account username
                        type: string
                  required:
                    - driver
                    - username
                    - endpoint
                    - client_id
                  title: Salesforce
                  type: object
                - examples:
                    - bot_token: xoxb-my-bot-token
                      driver: slack
                      type: connector
                  properties:
                    bot_token:
                        description: Bot token used for authenticating Slack API requests
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `slack`
                        type: string
                  required:
                    - driver
                    - bot_token
                  title: Slack
                  type: object
                - examples:
                    - account: my_account_identifier
                      database: my_db
                      driver: snowflake
                      parallel_fetch_limit: 2
                      privateKey: '{{ .env.SNOWFLAKE_PRIVATE_KEY }}'
                      role: my_role
                      schema: my_schema
                      type: connector
                      user: my_user
                      warehouse: my_wh
                    - driver: snowflake
                      dsn: '{{ .env.SNOWFLAKE_DSN }}'
                      parallel_fetch_limit: 2
                      type: connector
                  properties:
                    account:
                        description: Snowflake account identifier. To find your Snowflake account identifier, look at your Snowflake account URL. The account identifier is everything before .snowflakecomputing.com
                        type: string
                    authenticator:
                        description: Optional authenticator type (e.g., SNOWFLAKE_JWT).
                        type: string
                    database:
                        description: Name of the Snowflake database.
                        type: string
                    driver:
                        const: snowflake
                        description: Refers to the driver type and must be driver `snowflake`
                        type: string
                    dsn:
                        description: "DSN (Data Source Name) for the Snowflake connection.\n\nThis is intended for **advanced configuration** where you want to specify\nproperties that are not explicitly defined above.  \nIt can only be used when the other connection fields (account, user, password,\ndatabase, schema, warehouse, role, authenticator, privateKey) are **not used**.\n\nFor details on private key generation and encoding, see the `privateKey` property.\n"
                        type: string
                    parallel_fetch_limit:
                        description: Maximum number of concurrent fetches during query execution.
                        type: integer
                    password:
                        description: Password for the Snowflake connection. _(deprecated, use privateKey instead)_
                        type: string
                    privateKey:
                        description: |
                            Private key for JWT authentication.
                            :::tip
                            Private key must be generated as a **PKCS#8 (nocrypt) key**, since the Snowflake Go driver
                            only supports unencrypted private keys. After generating, it must be **base64 URL encoded**.

                            Example commands to generate and encode:

                            ```bash
                            # Generate a 2048-bit unencrypted PKCS#8 private key
                            openssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out rsa_key.p8 -nocrypt

                            # Convert URL safe format for Snowflake
                            cat rsa_key.p8 | grep -v "\----" | tr -d '\n' | tr '+/' '-_'
                            ```
                            See: https://docs.snowflake.com/en/guide/key-pair-auth
                            :::
                        type: string
                    role:
                        description: Snowflake role to use.
                        type: string
                    schema:
                        description: Schema within the database to use.
                        type: string
                    user:
                        description: Username for the Snowflake connection.
                        type: string
                    warehouse:
                        description: Compute warehouse to use for queries.
                        type: string
                  required:
                    - type
                    - driver
                  title: Snowflake
                  type: object
                - examples:
                    - driver: sqlite
                      dsn: file:mydatabase.db
                      type: connector
                  properties:
                    driver:
                        description: Refers to the driver type and must be driver `sqlite`
                        type: string
                    dsn:
                        description: DSN(Data Source Name) for the sqlite connection
                        type: string
                  required:
                    - driver
                    - dsn
                  title: SQLite
                  type: object
        description: |
            Connector YAML files define how Rill connects to external data sources and OLAP engines. Each connector specifies a driver type and its required connection parameters.

            ## Available Connector Types

            ### _OLAP Engines_
            - [**ClickHouse**](#clickhouse) - ClickHouse analytical database
            - [**Druid**](#druid) - Apache Druid
            - [**DuckDB**](#duckdb) - Embedded DuckDB engine (default)
            - [**MotherDuck**](#motherduck) - MotherDuck cloud database
            - [**Pinot**](#pinot) - Apache Pinot

            ### _Data Warehouses_
            - [**Athena**](#athena) - Amazon Athena
            - [**BigQuery**](#bigquery) - Google BigQuery
            - [**Redshift**](#redshift) - Amazon Redshift
            - [**Snowflake**](#snowflake) - Snowflake data warehouse

            ### _Databases_
            - [**MySQL**](#mysql) - MySQL databases
            - [**PostgreSQL**](#postgres) - PostgreSQL databases
            - [**SQLite**](#sqlite) - SQLite databases

            ### _Cloud Storage_
            - [**Azure**](#azure) - Azure Blob Storage
            - [**GCS**](#gcs) - Google Cloud Storage
            - [**S3**](#s3) - Amazon S3 storage

            ### _Other_
            - [**Extenral DuckDB**](#external-duckdb) - External DuckDB database
            - [**HTTPS**](#https) - Public files via HTTP/HTTPS
            - [**OpenAI**](#openai) - OpenAI data
            - [**Salesforce**](#salesforce) - Salesforce data
            - [**Slack**](#slack) - Slack data

            :::warning Security Recommendation
            For all credential parameters (passwords, tokens, keys), use environment variables with the syntax `{{.env.connector.<connector_driver>.<parameter_name>}}`. This keeps sensitive data out of your YAML files and version control. See our [credentials documentation](/developer/build/connectors/credentials/) for complete setup instructions.
            :::
        id: connectors
        title: Connector YAML
        type: object
        ```

        ## Examples

        ### DuckDB: Managed

        Explicit:

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb
        managed: true
        ```

        or relying on defaults:

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb
        ```

        ### DuckDB: With init_sql for S3 secrets

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb

        init_sql: |
          CREATE SECRET IF NOT EXISTS s3 (
            TYPE S3,
            KEY_ID '{{ .env.aws_access_key_id }}',
            SECRET '{{ .env.aws_secret_access_key }}',
            REGION 'us-east-1'
          )
        ```

        This is now deprecated in favor of creating a dedicated `s3.yaml` connector file, which Rill will automatically load and create as a secret in DuckDB.

        ### DuckDB: With extensions

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb

        init_sql: |
          INSTALL spatial;
          LOAD spatial;
        ```

        ### DuckDB: MotherDuck database with existing tables

        ```yaml
        # connectors/motherduck.yaml
        type: connector
        driver: duckdb
        path: "md:my_database"
        token: "{{ .env.motherduck_token }}"
        schema_name: main
        mode: read
        ```

        ### ClickHouse: Cloud with SSL

        ```yaml
        # connectors/clickhouse.yaml
        type: connector
        driver: clickhouse
        host: "abc123.us-east-1.aws.clickhouse.cloud"
        port: 8443
        username: "default"
        password: "{{ .env.clickhouse_password }}"
        database: "default"
        ssl: true
        ```

        ### ClickHouse: Readwrite with cluster

        ```yaml
        # connectors/clickhouse.yaml
        type: connector
        driver: clickhouse
        mode: readwrite

        host: "{{ .env.clickhouse_host }}"
        port: 9440
        username: "{{ .env.clickhouse_user }}"
        password: "{{ .env.clickhouse_password }}"
        database: "default"
        cluster: "my_cluster"
        ssl: true
        ```

        ### ClickHouse: Dev/prod configuration

        ```yaml
        # connectors/clickhouse.yaml
        type: connector
        driver: clickhouse
        mode: readwrite

        dev:
          managed: true

        prod:
          host: "{{ .env.clickhouse_host }}"
          port: 9440
          username: "{{ .env.clickhouse_user }}"
          password: "{{ .env.clickhouse_password }}"
          database: "default"
          cluster: "{{ .env.clickhouse_cluster }}"
          ssl: true
        ```

        ### S3: Basic with credentials and region

        ```yaml
        # connectors/s3.yaml
        type: connector
        driver: s3
        aws_access_key_id: "{{ .env.aws_access_key_id }}"
        aws_secret_access_key: "{{ .env.aws_secret_access_key }}"
        region: us-west-2
        ```

        ### S3: Cloudflare R2 (S3-compatible)

        ```yaml
        # connectors/r2.yaml
        type: connector
        driver: s3
        aws_access_key_id: "{{ .env.r2_access_key_id }}"
        aws_secret_access_key: "{{ .env.r2_secret_access_key }}"
        endpoint: "https://{{ .env.r2_account_id }}.r2.cloudflarestorage.com"
        region: auto
        ```

        ### GCS: Minimal (uses default credentials)

        ```yaml
        # connectors/gcs.yaml
        type: connector
        driver: gcs
        ```

        ### GCS: With explicit credentials

        ```yaml
        # connectors/gcs.yaml
        type: connector
        driver: gcs
        google_application_credentials: "{{ .env.gcs_service_account_json }}"
        ```

        ### BigQuery

        ```yaml
        # connectors/bigquery.yaml
        type: connector
        driver: bigquery
        project_id: "my-gcp-project"
        google_application_credentials: "{{ .env.bigquery_service_account_json }}"
        ```

        ### Snowflake: Basic with DSN

        ```yaml
        # connectors/snowflake.yaml
        type: connector
        driver: snowflake
        dsn: "{{ .env.snowflake_dsn }}"
        ```

        ### Postgres

        ```yaml
        # connectors/postgres.yaml
        type: connector
        driver: postgres
        host: "{{ .env.postgres_host }}"
        port: 5432
        user: "{{ .env.postgres_user }}"
        password: "{{ .env.postgres_password }}"
        dbname: "analytics"
        sslmode: require
        ```

        ### Druid: Host-based

        ```yaml
        # connectors/druid.yaml
        type: connector
        driver: druid
        host: "{{ .env.druid_host }}"
        port: 8888
        username: "{{ .env.druid_user }}"
        password: "{{ .env.druid_password }}"
        ssl: true
        ```

        ### Redshift: Serverless

        ```yaml
        # connectors/redshift.yaml
        type: connector
        driver: redshift
        aws_access_key_id: "{{ .env.aws_access_key_id }}"
        aws_secret_access_key: "{{ .env.aws_secret_access_key }}"
        workgroup: "my-workgroup"
        region: us-east-1
        database: "analytics"
        ```

        ### OpenAI

        ```yaml
        # connectors/openai.yaml
        type: connector
        driver: openai
        api_key: "{{ .env.openai_api_key }}"
        ```

        ### Slack

        ```yaml
        # connectors/slack.yaml
        type: connector
        driver: slack
        bot_token: "{{ .env.slack_bot_token }}"
        ```

        ### Pinot

        ```yaml
        # connectors/pinot.yaml
        type: connector
        driver: pinot
        broker_host: "{{ .env.pinot_broker_host }}"
        controller_host: "{{ .env.pinot_controller_host }}"
        username: "{{ .env.pinot_user }}"
        password: "{{ .env.pinot_password }}"
        ssl: true
        ```
    - role: user
      content_type: text
      content: |-
        You should develop a Rill project file based on the following task description:
        - Develop file at path: /connectors/s3.yaml
        - The file should be of type: connector
        - Task description: Create a new S3 connector resource.

        Requirements:
        - File path: connectors/s3.yaml
        - Resource name should be `s3` (derived from filename).
        - Use driver: s3.
        - Use env vars already present in the project status: `connector.s3.aws_access_key_id` and `connector.s3.aws_secret_access_key`.
        - Configure region conservatively (us-east-1) but allow override via env var if present (do not introduce new required env vars). If you include optional env var references, guard them with defaults if supported.
        - Keep minimal properties needed for Rill to access public/private buckets.

        Return valid YAML for a Rill connector.

        Here is some important context:
        - You are running as a sub-agent of a larger developer agent. Stay aligned on your specific task and avoid extra discovery.
        - When you call 'write_file', if it returns a parse or reconcile error, do your best to fix the issue and try again. If you think the error is unrelated to the current path, let the parent agent know to handle it.

        Here is some additional context that may or may not be relevant to your task:
        - The project's default OLAP connector is named duckdb (driver: duckdb).
    - role: assistant
      content_type: tool_call
      id: 3cf7cedf44504bb6
      tool_name: write_file
      content: '{"contents":"type: connector\ndriver: s3\n\n# Credentials are sourced from existing env vars (see project status)\naws_access_key_id: \"{{ .env.connector.s3.aws_access_key_id }}\"\naws_secret_access_key: \"{{ .env.connector.s3.aws_secret_access_key }}\"\n\n# Default conservatively to us-east-1; can be overridden by setting env var\nregion: \"{{ .env.connector.s3.region | default \\\"us-east-1\\\" }}\"\n","path":"connectors/s3.yaml"}'
    - role: tool
      content_type: tool_response
      id: 3cf7cedf44504bb6
      content: '{"diff":"--- /connectors/s3.yaml\n+++ /connectors/s3.yaml\n@@ -1 +1,10 @@\n+type: connector\n+driver: s3\n \n+# Credentials are sourced from existing env vars (see project status)\n+aws_access_key_id: \"{{ .env.connector.s3.aws_access_key_id }}\"\n+aws_secret_access_key: \"{{ .env.connector.s3.aws_secret_access_key }}\"\n+\n+# Default conservatively to us-east-1; can be overridden by setting env var\n+region: \"{{ .env.connector.s3.region | default \\\"us-east-1\\\" }}\"\n+\n","is_new_file":true,"resources":[{"kind":"rill.runtime.v1.Connector","name":"s3","reconcile_error":"failed to resolve template: template: :1:7: executing \"\" at \u003c.env.connector.s3.region\u003e: map has no entry for key \"region\"","reconcile_status":"RECONCILE_STATUS_IDLE"}]}'
  response:
    - role: assistant
      content_type: tool_call
      id: call_QVicDHSyjMLcVEKzwtYbt98O
      tool_name: write_file
      content: '{"contents":"type: connector\ndriver: s3\n\n# Credentials are sourced from existing env vars (see project status)\naws_access_key_id: \"{{ .env.connector.s3.aws_access_key_id }}\"\naws_secret_access_key: \"{{ .env.connector.s3.aws_secret_access_key }}\"\n\n# Default conservatively to us-east-1; override by setting env var connector.s3.aws_region\naws_region: \"{{ .env.connector.s3.aws_region | default \\\"us-east-1\\\" }}\"\n","path":"connectors/s3.yaml"}'
- index: 6
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Models as SQL files:** Always create new models as `.yaml` files, not `.sql` files (which are harder to extend later).\n- **Not creating connector files:** When Rill has native support for a connector (like S3 or BigQuery), always create a dedicated connector resource file for it.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. This also includes models that load external data using a native SQL function, like `read_parquet(...)` or `s3(...)`. Non-materialized models become views, which re-execute on every query.\n- **Referencing non-existant environment variables:** Only reference environment variables that are present in `.env` (returned in `env` from `project_status`). If you need the user to add another environment variable, stop and ask them to do so.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: system
      content_type: text
      content: |-
        # Instructions for developing a connector in Rill

        ## Introduction

        Connectors are resources that contain credentials and settings for connecting to external systems. They are typically found at the root of the project's DAG, providing access to data sources and services that power downstream resources like models and metrics views.

        Connectors are usually lightweight resources. When reconciled, they validate the connection to the external system but do not move or process data. The main exception is managed OLAP connectors (with `managed: true`), which trigger database provisioning.

        ### Driver capabilities

        Each connector uses a **driver** that implements one or more capabilities:

        - **OLAP database**: Can power metrics views and dashboards (e.g., `duckdb`, `clickhouse`)
        - **SQL database**: Can run SQL queries as model inputs (e.g., `postgres`, `bigquery`, `snowflake`)
        - **Information schema**: Can list tables and their schemas (e.g., `duckdb`, `bigquery`)
        - **Object store**: Can list, read, and write flat files (e.g., `s3`, `gcs`)
        - **Notifier**: Can send notifications and alerts (e.g., `slack`)
        - **AI**: Can generate embeddings or responses (e.g., `openai`)

        ## Core Concepts

        ### Naming conventions

        Connectors are typically named after their driver (e.g., a file `connectors/duckdb.yaml` creates a connector named `duckdb`). Use descriptive names when you have multiple connectors of the same type:
        - `connectors/s3_data.yaml` and `connectors/s3_staging.yaml`
        - `connectors/clickhouse_prod.yaml` and `connectors/clickhouse_dev.yaml`

        ### Secrets management

        Always store sensitive credentials in `.env` and reference them using template syntax:

        ```yaml
        type: connector
        driver: s3
        aws_access_key_id: "{{ .env.aws_access_key_id }}"
        aws_secret_access_key: "{{ .env.aws_secret_access_key }}"
        ```

        NOTE: Some legacy projects use the deprecated `.vars` instead of `.env`.

        ### Managed connectors

        OLAP connectors can be provisioned automatically by Rill using `managed: true`. This is supported for `duckdb` and `clickhouse` drivers:

        ```yaml
        type: connector
        driver: duckdb
        managed: true
        ```

        When a managed connector is reconciled, Rill provisions the database infrastructure. The user is billed for the CPU, memory, and disk usage of the provisioned database.

        ### Access modes

        Control read/write access using the `mode` property:

        - `mode: read`: Prevents Rill models from writing to this connector
        - `mode: readwrite`: Allows both reading and writing (default for managed connectors)

        Use `mode: read` when connecting to external databases with pre-existing tables to prevent unintended modifications.

        ### Dev/Prod configuration

        Use `dev:` and `prod:` blocks for environment-specific settings:

        ```yaml
        type: connector
        driver: clickhouse
        mode: readwrite

        # Use a local database in development to prevent overwriting data in the production cluster when iterating on changes.
        dev:
          managed: true

        prod:
          host: "{{ .env.clickhouse_host }}"
          port: 9440
          username: "{{ .env.clickhouse_user }}"
          password: "{{ .env.clickhouse_password }}"
          ssl: true
        ```

        ## Driver-Specific Notes

        ### DuckDB

        DuckDB is Rill's default embedded OLAP database. Key properties:

        - `managed: true`: Rill provisions and manages the database
        - `init_sql`: SQL to run at startup (install extensions, create secrets, attach databases)

        For MotherDuck (cloud DuckDB), use the `path` property with `md:` prefix:

        ```yaml
        type: connector
        driver: duckdb
        path: "md:my_database"
        token: "{{ .env.motherduck_token }}"
        ```

        ### ClickHouse

        ClickHouse can be user-managed or Rill-managed. Key properties:

        - `managed: true`: Rill provisions and manages an empty Clickhouse cluster. If set, don't set any other connector properties.
        - `host`, `port`, `username`, `password`: Connection credentials
        - `database`: Target database name
        - `ssl: true`: Required for ClickHouse Cloud
        - `cluster`: Cluster name for multi-node Clickhouse clusters
        - `dsn`: Alternative connection string format (format: `clickhouse://host:port?username=<username>&...`)

        Common ports:
        - `8443`: HTTPS native protocol (ClickHouse Cloud)
        - `9440`: Secure native protocol
        - `9000`: Native protocol (non-SSL)

        ### S3

        AWS S3 and S3-compatible storage. Key properties:

        - `aws_access_key_id`, `aws_secret_access_key`: AWS credentials
        - `region`: AWS region
        - `endpoint`: Custom endpoint for S3-compatible services (R2, MinIO, GCS interop)
        - `path_prefixes`: A list of bucket paths that the connector can access, e.g. `[s3://my-bucket]`; useful for improving bucket introspection

        ### GCS

        Google Cloud Storage. Key properties:

        - `google_application_credentials`: Service account JSON (must be a literal JSON string value)
        - `key_id`: HMAC key ID to use instead of a service account JSON; required for direct use with DuckDB and Clickhouse through S3 compatibility
        - `secret`: HMAC secret to use instead of a service account JSON; required for direct use with DuckDB and Clickhouse through S3 compatibility

        ### BigQuery

        Google BigQuery. Key properties:

        - `project_id`: GCP project ID
        - `google_application_credentials`: Service account JSON

        ### Snowflake

        Snowflake data warehouse. Key properties:

        - `account`, `user`, `privateKey`, `database`, `schema`, `warehouse`, `role`: Connection parameters
        - `dsn`: Connection string to use instead of separate connection parameters

        ### Postgres

        PostgreSQL database. Key properties:

        - `host`, `port`, `user`, `password`, `dbname`: Connection parameters
        - `sslmode`: SSL mode (`disable`, `require`, `verify-full`, etc.)

        ### Druid

        Apache Druid. Can be configured via host/port or DSN:

        - `host`, `port`, `username`, `password`, `ssl`: Direct connection
        - `dsn`: Full connection string

        ### Redshift

        Amazon Redshift. Key properties:

        - `aws_access_key_id`, `aws_secret_access_key`: AWS credentials
        - `workgroup`: Redshift Serverless workgroup name
        - `region`: AWS region
        - `database`: Database name

        ### Athena

        Amazon Athena. Key properties:

        - `aws_access_key_id`, `aws_secret_access_key`: AWS credentials
        - `workgroup`: Redshift Serverless workgroup name
        - `region`: AWS region
        - `output_location`: S3 path in format `s3://bucket/path` to store temporary query results in (Athena only)

        ### Other drivers

        - **Slack**: Use `bot_token` for alert notifications
        - **OpenAI**: Use `api_key` for AI-powered features
        - **HTTPS**: Simple connector for public HTTP sources
        - **Pinot**: Use `broker_host`, `controller_host`, `username`, `password`

        ## JSON Schema

        Here is a full JSON schema for the connector syntax:

        ```
        allOf:
            - properties:
                type:
                    const: connector
                    description: Refers to the resource type and must be `connector`
                    type: string
              required:
                - type
              title: Properties
              type: object
            - $ref: '#/definitions/common_properties'
            - oneOf:
                - examples:
                    - allow_host_access: true
                      aws_access_key_id: myawsaccesskey
                      aws_access_token: mytemporarytoken
                      aws_region: us-east-1
                      aws_secret_access_key: myawssecretkey
                      driver: athena
                      external_id: MyExternalID
                      output_location: s3://my-bucket/athena-output/
                      role_arn: arn:aws:iam::123456789012:role/MyRole
                      role_session_name: MySession
                      type: connector
                      workgroup: primary
                  properties:
                    allow_host_access:
                        description: Allow the Athena client to access host environment configurations such as environment variables or local AWS credential files. Defaults to true, enabling use of credentials and settings from the host environment unless explicitly disabled.
                        type: boolean
                    aws_access_key_id:
                        description: AWS Access Key ID used for authentication. Required when using static credentials directly or as base credentials for assuming a role.
                        type: string
                    aws_access_token:
                        description: AWS session token used with temporary credentials. Required only if the Access Key and Secret Key are part of a temporary session credentials.
                        type: string
                    aws_region:
                        description: AWS region where Athena and the result S3 bucket are located (e.g., us-east-1). Defaults to 'us-east-1' if not specified.
                        type: string
                    aws_secret_access_key:
                        description: AWS Secret Access Key paired with the Access Key ID. Required when using static credentials directly or as base credentials for assuming a role.
                        type: string
                    driver:
                        const: athena
                        description: Refers to the driver type and must be driver `athena`
                        type: string
                    external_id:
                        description: External ID required by some roles when assuming them, typically for cross-account access. Used only if 'role_arn' is specified and the role's trust policy requires it.
                        type: string
                    output_location:
                        description: S3 URI where Athena query results should be stored (e.g., s3://your-bucket/athena/results/). Optional if the selected workgroup has a default result configuration.
                        type: string
                    role_arn:
                        description: ARN of the IAM role to assume. When specified, the SDK uses the base credentials to call STS AssumeRole and obtain temporary credentials scoped to this role.
                        type: string
                    role_session_name:
                        description: Session name to associate with the STS AssumeRole session. Used only if 'role_arn' is specified. Useful for identifying and auditing the session.
                        type: string
                    workgroup:
                        description: Athena workgroup to use for query execution. Defaults to 'primary' if not specified.
                        type: string
                  required:
                    - driver
                  title: Athena
                  type: object
                - examples:
                    - azure_storage_account: mystorageaccount
                      azure_storage_key: credentialstring
                      driver: azure
                      type: connector
                  properties:
                    allow_host_access:
                        description: Allow access to host environment configuration
                        type: boolean
                    azure_storage_account:
                        description: Azure storage account name
                        type: string
                    azure_storage_connection_string:
                        description: Optional azure connection string for storage account
                        type: string
                    azure_storage_key:
                        description: Azure storage access key
                        type: string
                    azure_storage_sas_token:
                        description: Optional azure SAS token for authentication
                        type: string
                    driver:
                        const: azure
                        description: Refers to the driver type and must be driver `azure`
                        type: string
                    path_prefixes:
                        description: |
                            A list of container or virtual directory prefixes that this connector is allowed to access.
                            Useful when different containers or paths use different credentials, allowing the system
                            to route access through the appropriate connector based on the blob path.
                            Example: `azure://my-bucket/`, ` azure://my-bucket/path/` ,`azure://my-bucket/path/prefix`
                        type:
                            - string
                            - array
                  required:
                    - driver
                    - azure_storage_account
                    - azure_storage_key
                  title: Azure
                  type: object
                - examples:
                    - allow_host_access: true
                      driver: bigquery
                      google_application_credentials: credentialjsonstring
                      project_id: my-project-id
                      type: connector
                  properties:
                    allow_host_access:
                        description: Enable the BigQuery client to use credentials from the host environment when no service account JSON is provided. This includes Application Default Credentials from environment variables, local credential files, or the Google Compute Engine metadata server. Defaults to true, allowing seamless authentication in GCP environments.
                        type: boolean
                    dataset_id:
                        description: BigQuery dataset ID
                        type: string
                    driver:
                        const: bigquery
                        description: Refers to the driver type and must be driver `bigquery`
                        type: string
                    google_application_credentials:
                        description: Raw contents of the Google Cloud service account key (in JSON format) used for authentication.
                        type: string
                    location:
                        description: BigQuery dataset location
                        type: string
                    project_id:
                        description: Google Cloud project ID
                        type: string
                  required:
                    - driver
                  title: BigQuery
                  type: object
                - examples:
                    - cluster: mycluster
                      database: mydatabase
                      driver: clickhouse
                      host: localhost
                      managed: false
                      mode: readwrite
                      password: mypassword
                      port: 9000
                      ssl: true
                      type: connector
                      username: myusername
                  properties:
                    can_scale_to_zero:
                        description: Indicates if the database can scale to zero
                        type: boolean
                    cluster:
                        description: Cluster name, required for running distributed queries
                        type: string
                    conn_max_lifetime:
                        description: Maximum time a connection may be reused
                        type: string
                    database:
                        description: Name of the ClickHouse database within the cluster
                        type: string
                    dial_timeout:
                        description: Timeout for dialing the ClickHouse server
                        type: string
                    driver:
                        const: clickhouse
                        description: Refers to the driver type and must be driver `clickhouse`
                        type: string
                    dsn:
                        description: DSN(Data Source Name) for the ClickHouse connection
                        type: string
                    embed_port:
                        description: Port to run ClickHouse locally (0 for random port)
                        type: integer
                    host:
                        description: Host where the ClickHouse instance is running
                        type: string
                    log_queries:
                        description: Controls whether to log raw SQL queries
                        type: boolean
                    managed:
                        description: '`true` means Rill will provision the connector using the default provisioner. `false` disables automatic provisioning.'
                        type: boolean
                    max_idle_conns:
                        description: Maximum number of idle connections in the pool
                        type: integer
                    max_open_conns:
                        description: Maximum number of open connections to the database
                        type: integer
                    mode:
                        description: '`read` - Controls the operation mode for the ClickHouse connection. Defaults to ''read'' for safe operation with external databases. Set to ''readwrite'' to enable model creation and table mutations. Note: When ''managed: true'', this is automatically set to ''readwrite''.'
                        type: string
                    password:
                        description: Password for authentication
                        type: string
                    port:
                        description: Port where the ClickHouse instance is accessible
                        type: integer
                    query_settings:
                        description: query settings to be set on dashboard queries. `query_settings_override` takes precedence over these settings and if set these are ignored. Each setting must be separated by a comma. Example `max_threads = 8, max_memory_usage = 10000000000`
                        type: string
                    query_settings_override:
                        description: override the default settings used in queries. Changing the default settings can lead to incorrect query results and is generally not recommended. If you need to add settings, use `query_settings`
                        type: string
                    read_timeout:
                        description: Maximum time for a connection to read data
                        type: string
                    ssl:
                        description: Indicates whether a secured SSL connection is required
                        type: boolean
                    username:
                        description: Username for authentication
                        type: string
                  required:
                    - driver
                  title: ClickHouse
                  type: object
                - examples:
                    - driver: druid
                      host: localhost
                      password: mypassword
                      port: 8082
                      ssl: true
                      type: connector
                      username: myusername
                  properties:
                    driver:
                        const: druid
                        description: Refers to the driver type and must be driver `druid`
                        type: string
                    dsn:
                        description: Data Source Name (DSN) for connecting to Druid
                        type: string
                    host:
                        description: Hostname of the Druid coordinator or broker
                        type: string
                    log_queries:
                        description: Log raw SQL queries sent to Druid
                        type: boolean
                    max_open_conns:
                        description: Maximum number of open database connections (0 = default, -1 = unlimited)
                        type: integer
                    password:
                        description: Password for authenticating with Druid
                        type: string
                    port:
                        description: Port number of the Druid service
                        type: integer
                    skip_version_check:
                        description: Skip checking Druid version compatibility
                        type: boolean
                    ssl:
                        description: Enable SSL for secure connection
                        type: boolean
                    username:
                        description: Username for authenticating with Druid
                        type: string
                  required:
                    - driver
                    - dsn
                  title: Druid
                  type: object
                - examples:
                    - allow_host_access: true
                      cpu: 4
                      driver: duckdb
                      init_sql: INSTALL httpfs; LOAD httpfs;
                      log_queries: true
                      memory_limit_gb: 16
                      mode: readwrite
                      pool_size: 5
                      read_write_ratio: 0.7
                      type: connector
                  properties:
                    allow_host_access:
                        description: Whether access to local environment and file system is allowed
                        type: boolean
                    attach:
                        description: Full ATTACH statement to attach a DuckDB database
                        type: string
                    boot_queries:
                        description: Deprecated - Use init_sql instead
                        type: string
                    conn_init_sql:
                        description: SQL executed when a new connection is initialized
                        type: string
                    cpu:
                        description: Number of CPU cores available to the database
                        minimum: 1
                        type: integer
                    create_secrets_from_connectors:
                        description: List of connector names for which temporary secrets should be created before executing the SQL.
                        type:
                            - string
                            - array
                    database_name:
                        description: Name of the attached DuckDB database (auto-detected if not set)
                        type: string
                    driver:
                        const: duckdb
                        description: Must be "duckdb"
                        type: string
                    init_sql:
                        description: SQL executed during database initialization
                        type: string
                    log_queries:
                        description: Whether to log raw SQL queries executed through OLAP
                        type: boolean
                    memory_limit_gb:
                        description: Amount of memory in GB available to the database
                        minimum: 1
                        type: integer
                    mode:
                        default: read
                        description: Set the mode for the DuckDB connection.
                        enum:
                            - read
                            - readwrite
                        type: string
                    path:
                        description: Path to external DuckDB database
                        type: string
                    pool_size:
                        description: Number of concurrent connections and queries allowed
                        minimum: 1
                        type: integer
                    read_write_ratio:
                        default: 0.5
                        description: Ratio of resources allocated to read vs write operations
                        maximum: 1
                        minimum: 0
                        type: number
                    schema_name:
                        description: Default schema used by the DuckDB database
                        type: string
                  required:
                    - driver
                  title: DuckDB
                  type: object
                - examples:
                    - driver: duckdb
                      mode: read
                      path: /path/to/my-duckdb-database.db
                      type: connector
                  properties:
                    driver:
                        const: duckdb
                        description: Refers to the driver type and must be driver `duckdb`
                        type: string
                    mode:
                        default: read
                        description: Set the mode for the DuckDB connection.
                        enum:
                            - read
                            - readwrite
                        type: string
                    path:
                        description: Path to the DuckDB database
                        type: string
                  required:
                    - driver
                    - db
                  title: External DuckDB
                  type: object
                - examples:
                    - driver: gcs
                      google_application_credentials: credentialjsonstring
                      type: connector
                  properties:
                    allow_host_access:
                        description: Allow access to host environment configuration
                        type: boolean
                    driver:
                        const: gcs
                        description: Refers to the driver type and must be driver `gcs`
                        type: string
                    google_application_credentials:
                        description: Google Cloud credentials JSON string
                        type: string
                    key_id:
                        description: Optional S3-compatible Key ID when used in compatibility mode
                        type: string
                    path_prefixes:
                        description: "A list of bucket path prefixes that this connector is allowed to access. \nUseful when different buckets or bucket prefixes use different credentials, \nallowing the system to select the appropriate connector based on the bucket path.\nExample: `gs://my-bucket/`, ` gs://my-bucket/path/` ,`gs://my-bucket/path/prefix`\n"
                        type:
                            - string
                            - array
                    secret:
                        description: Optional S3-compatible Secret when used in compatibility mode
                        type: string
                  required:
                    - driver
                    - bucket
                  title: GCS
                  type: object
                - examples:
                    - driver: https
                      headers:
                        Authorization: Bearer my-token
                      type: connector
                  properties:
                    driver:
                        const: https
                        description: Refers to the driver type and must be driver `https`
                        type: string
                    headers:
                        additionalProperties:
                            type: string
                        description: HTTP headers to include in the request
                        type: object
                    path_prefixes:
                        description: |
                            A list of HTTP/HTTPS URL prefixes that this connector is allowed to access.
                            Useful when different URL namespaces use different credentials, enabling the
                            system to choose the appropriate connector based on the URL path.
                            Example: `https://example.com/`, ` https://example.com/path/` ,`https://example.com/path/prefix`
                        type:
                            - string
                            - array
                  required:
                    - driver
                    - path
                  title: HTTPS
                  type: object
                - examples:
                    - driver: duckdb
                      path: md:my_database
                      schema_name: my_schema
                      token: '{{ .env.connector.motherduck.token }}'
                      type: connector
                  properties:
                    create_secrets_from_connectors:
                        description: List of connector names for which temporary secrets should be created before executing the SQL.
                        type:
                            - string
                            - array
                    driver:
                        const: duckdb
                        description: Refers to the driver type and must be driver `duckdb`.
                        type: string
                    init_sql:
                        description: SQL executed during database initialization.
                        type: string
                    mode:
                        default: read
                        description: Set the mode for the MotherDuck connection. By default, it is set to 'read' which allows only read operations. Set to 'readwrite' to enable model creation and table mutations.
                        enum:
                            - read
                            - readwrite
                        type: string
                    path:
                        description: Path to your MD database
                        type: string
                    schema_name:
                        description: Define your schema if not main, uses main by default
                        type: string
                    token:
                        description: MotherDuck token
                        type: string
                  required:
                    - driver
                    - path
                    - token
                  title: MotherDuck
                  type: object
                - examples:
                    - database: mydb
                      driver: mysql
                      host: localhost
                      password: p@ss
                      port: 3306
                      ssl-mode: preferred
                      type: connector
                      user: user
                    - driver: mysql
                      dsn: mysql://user:p%40ss@localhost:3306/mydb?ssl-mode=preferred
                      type: connector
                  properties:
                    database:
                        description: Name of the MySQL database
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `mysql`
                        type: string
                    dsn:
                        description: |
                            **Data Source Name (DSN)** for the MySQL connection, provided in [MySQL URI format](https://dev.mysql.com/doc/refman/8.4/en/connecting-using-uri-or-key-value-pairs.html#connecting-using-uri).
                            The DSN must follow the standard MySQL URI scheme:
                            ```text
                            mysql://user:password@host:3306/my-db
                            ```
                            Rules for special characters in password:
                            - The following characters are allowed [unescaped in the URI](https://datatracker.ietf.org/doc/html/rfc3986#section-2.3): `~` `.` `_` `-`
                            - All other special characters must be percent-encoded (`%XX` format).
                            ```text
                            mysql://user:pa%40ss@localhost:3306/my-db   # password contains '@'
                            mysql://user:pa%3Ass@localhost:3306/my-db   # password contains ':'
                            ```
                        type: string
                    host:
                        description: Hostname of the MySQL server
                        type: string
                    password:
                        description: Password for authentication
                        type: string
                    port:
                        description: Port number for the MySQL server
                        type: integer
                    ssl-mode:
                        description: 'ssl mode options: `disabled`, `preferred`, or `required`.'
                        type: string
                    user:
                        description: Username for authentication
                        type: string
                  required:
                    - driver
                  title: MySQL
                  type: object
                - examples:
                    - api_key: my-api-key
                      api_type: openai
                      api_version: "2023-05-15"
                      base_url: https://api.openai.com/v1
                      driver: openai
                      model: gpt-4o
                      type: connector
                  properties:
                    api_key:
                        description: API key for connecting to OpenAI
                        type: string
                    api_type:
                        description: The type of OpenAI API to use
                        type: string
                    api_version:
                        description: The version of the OpenAI API to use (e.g., '2023-05-15'). Required when API Type is AZURE or AZURE_AD
                        type: string
                    base_url:
                        description: The base URL for the OpenAI API (e.g., 'https://api.openai.com/v1')
                        type: string
                    driver:
                        description: The driver type, must be set to "openai"
                        type: string
                    model:
                        description: The OpenAI model to use (e.g., 'gpt-4o')
                        type: string
                  required:
                    - api_key
                  title: OpenAI
                  type: object
                - examples:
                    - broker_host: localhost
                      broker_port: 9000
                      controller_host: localhost
                      controller_port: 9000
                      driver: pinot
                      log_queries: true
                      max_open_conns: 100
                      password: mypassword
                      ssl: true
                      type: connector
                      username: myusername
                  properties:
                    broker_host:
                        description: Hostname of the Pinot broker
                        type: string
                    broker_port:
                        description: Port number for the Pinot broker
                        type: integer
                    controller_host:
                        description: Hostname of the Pinot controller
                        type: string
                    controller_port:
                        description: Port number for the Pinot controller
                        type: integer
                    driver:
                        description: Refers to the driver type and must be driver `pinot`
                        type: string
                    dsn:
                        description: DSN(Data Source Name) for the Pinot connection
                        type: string
                    log_queries:
                        description: Log raw SQL queries executed through Pinot
                        type: boolean
                    max_open_conns:
                        description: Maximum number of open connections to the Pinot database
                        type: integer
                    password:
                        description: Password for authenticating with Pinot
                        type: string
                    ssl:
                        description: Enable SSL connection to Pinot
                        type: boolean
                    username:
                        description: Username for authenticating with Pinot
                        type: string
                  required:
                    - driver
                    - dsn
                    - broker_host
                    - controller_host
                  title: Pinot
                  type: object
                - examples:
                    - dbname: mydatabase
                      driver: postgres
                      host: localhost
                      password: mypassword
                      port: 5432
                      sslmode: prefer
                      type: connector
                      user: myusername
                    - driver: postgres
                      dsn: user=myusername password='my pass\'word' host=localhost port=5432 dbname=mydatabase sslmode=prefer
                      type: connector
                    - driver: postgres
                      dsn: postgres://myusername:p%40ss@localhost:5432/mydatabase?sslmode=prefer
                      type: connector
                  properties:
                    dbname:
                        description: Name of the Postgres database
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `postgres`
                        type: string
                    dsn:
                        description: |
                            **Data Source Name (DSN)** for the PostgreSQL connection, provided in
                            [PostgreSQL connection string format](https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING).
                            PostgreSQL supports both **key=value format** and **URI format**.

                            key=value format example:
                            ```text
                            user=user password=password host=host port=5432 dbname=mydb
                            ```
                            Rules for key=value format for special characters:
                            - To write an empty value, or a value containing spaces, `=`, single quotes, or backslashes, surround it with single quotes.
                            - Single quotes and backslashes inside a value must be escaped with a backslash (`\'` and `\\`).

                            URI format example:
                            ```text
                            postgres://user:password@host:5432/mydb
                            ```

                            Rules for URI format:
                            - The following characters are allowed [unescaped in the URI](https://datatracker.ietf.org/doc/html/rfc3986#section-2.3): `~` `.` `_` `-`
                            - All other special characters must be percent-encoded (`%XX` format).

                            Examples (URI format with encoded characters):
                            ```text
                            postgres://user:pa%40ss@localhost:5432/my-db   # '@' is encoded as %40
                            postgres://user:pa%3Ass@localhost:5432/my-db   # ':' is encoded as %3A
                            ```
                        type: string
                    host:
                        description: Hostname of the Postgres server
                        type: string
                    password:
                        description: Password for authentication
                        type: string
                    port:
                        description: Port number for the Postgres server
                        type: string
                    sslmode:
                        description: 'ssl mode options: `disable`, `allow`, `prefer` or `require`.'
                        type: string
                    user:
                        description: Username for authentication
                        type: string
                  required:
                    - driver
                  title: Postgres
                  type: object
                - examples:
                    - aws_access_key_id: my-access-key-id
                      aws_access_token: my-access-token
                      aws_secret_access_key: my-secret-access-key
                      cluster_identifier: my-cluster-identifier
                      database: mydatabase
                      driver: redshift
                      region: us-east-1
                      type: connector
                      workgroup: my-workgroup
                  properties:
                    aws_access_key_id:
                        description: AWS Access Key ID used for authenticating with Redshift.
                        type: string
                    aws_access_token:
                        description: AWS Session Token for temporary credentials (optional).
                        type: string
                    aws_secret_access_key:
                        description: AWS Secret Access Key used for authenticating with Redshift.
                        type: string
                    cluster_identifier:
                        description: Cluster identifier for provisioned Redshift clusters, in case of Redshift Serverless use 'workgroup' .
                        type: string
                    database:
                        description: Name of the Redshift database to query.
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `redshift`
                        type: string
                    region:
                        description: AWS region where the Redshift cluster or workgroup is hosted (e.g., 'us-east-1').
                        type: string
                    workgroup:
                        description: Workgroup name for Redshift Serverless, in case of provisioned Redshift clusters use 'cluster_identifier'.
                        type: string
                  required:
                    - driver
                    - aws_access_key_id
                    - aws_secret_access_key
                    - database
                  title: Redshift
                  type: object
                - examples:
                    - aws_access_key_id: my-access-key-id
                      aws_access_token: my-access-token
                      aws_secret_access_key: my-secret-access-key
                      driver: s3
                      endpoint: https://my-s3-endpoint.com
                      region: us-east-1
                      type: connector
                  properties:
                    allow_host_access:
                        description: Allow access to host environment configuration
                        type: boolean
                    aws_access_key_id:
                        description: AWS Access Key ID used for authentication
                        type: string
                    aws_access_token:
                        description: Optional AWS session token for temporary credentials
                        type: string
                    aws_secret_access_key:
                        description: AWS Secret Access Key used for authentication
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `s3`
                        type: string
                    endpoint:
                        description: Optional custom endpoint URL for S3-compatible storage
                        type: string
                    path_prefixes:
                        description: |
                            A list of bucket path prefixes that this connector is allowed to access.
                            Useful when different buckets or bucket prefixes use different credentials,
                            allowing the system to select the appropriate connector based on the bucket path.
                            Example: `s3://my-bucket/`, ` s3://my-bucket/path/` ,`s3://my-bucket/path/prefix`
                        type:
                            - string
                            - array
                    region:
                        description: AWS region of the S3 bucket
                        type: string
                  required:
                    - driver
                    - bucket
                  title: S3
                  type: object
                - examples:
                    - client_id: my-client-id
                      driver: salesforce
                      endpoint: https://login.salesforce.com
                      password: mypassword
                      type: connector
                      username: myusername
                  properties:
                    client_id:
                        description: Client ID used for Salesforce OAuth authentication
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `salesforce`
                        type: string
                    endpoint:
                        description: Salesforce API endpoint URL
                        type: string
                    key:
                        description: Authentication key for Salesforce (secret)
                        type: string
                    password:
                        description: Salesforce account password (secret)
                        type: string
                    username:
                        description: Salesforce account username
                        type: string
                  required:
                    - driver
                    - username
                    - endpoint
                    - client_id
                  title: Salesforce
                  type: object
                - examples:
                    - bot_token: xoxb-my-bot-token
                      driver: slack
                      type: connector
                  properties:
                    bot_token:
                        description: Bot token used for authenticating Slack API requests
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `slack`
                        type: string
                  required:
                    - driver
                    - bot_token
                  title: Slack
                  type: object
                - examples:
                    - account: my_account_identifier
                      database: my_db
                      driver: snowflake
                      parallel_fetch_limit: 2
                      privateKey: '{{ .env.SNOWFLAKE_PRIVATE_KEY }}'
                      role: my_role
                      schema: my_schema
                      type: connector
                      user: my_user
                      warehouse: my_wh
                    - driver: snowflake
                      dsn: '{{ .env.SNOWFLAKE_DSN }}'
                      parallel_fetch_limit: 2
                      type: connector
                  properties:
                    account:
                        description: Snowflake account identifier. To find your Snowflake account identifier, look at your Snowflake account URL. The account identifier is everything before .snowflakecomputing.com
                        type: string
                    authenticator:
                        description: Optional authenticator type (e.g., SNOWFLAKE_JWT).
                        type: string
                    database:
                        description: Name of the Snowflake database.
                        type: string
                    driver:
                        const: snowflake
                        description: Refers to the driver type and must be driver `snowflake`
                        type: string
                    dsn:
                        description: "DSN (Data Source Name) for the Snowflake connection.\n\nThis is intended for **advanced configuration** where you want to specify\nproperties that are not explicitly defined above.  \nIt can only be used when the other connection fields (account, user, password,\ndatabase, schema, warehouse, role, authenticator, privateKey) are **not used**.\n\nFor details on private key generation and encoding, see the `privateKey` property.\n"
                        type: string
                    parallel_fetch_limit:
                        description: Maximum number of concurrent fetches during query execution.
                        type: integer
                    password:
                        description: Password for the Snowflake connection. _(deprecated, use privateKey instead)_
                        type: string
                    privateKey:
                        description: |
                            Private key for JWT authentication.
                            :::tip
                            Private key must be generated as a **PKCS#8 (nocrypt) key**, since the Snowflake Go driver
                            only supports unencrypted private keys. After generating, it must be **base64 URL encoded**.

                            Example commands to generate and encode:

                            ```bash
                            # Generate a 2048-bit unencrypted PKCS#8 private key
                            openssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out rsa_key.p8 -nocrypt

                            # Convert URL safe format for Snowflake
                            cat rsa_key.p8 | grep -v "\----" | tr -d '\n' | tr '+/' '-_'
                            ```
                            See: https://docs.snowflake.com/en/guide/key-pair-auth
                            :::
                        type: string
                    role:
                        description: Snowflake role to use.
                        type: string
                    schema:
                        description: Schema within the database to use.
                        type: string
                    user:
                        description: Username for the Snowflake connection.
                        type: string
                    warehouse:
                        description: Compute warehouse to use for queries.
                        type: string
                  required:
                    - type
                    - driver
                  title: Snowflake
                  type: object
                - examples:
                    - driver: sqlite
                      dsn: file:mydatabase.db
                      type: connector
                  properties:
                    driver:
                        description: Refers to the driver type and must be driver `sqlite`
                        type: string
                    dsn:
                        description: DSN(Data Source Name) for the sqlite connection
                        type: string
                  required:
                    - driver
                    - dsn
                  title: SQLite
                  type: object
        description: |
            Connector YAML files define how Rill connects to external data sources and OLAP engines. Each connector specifies a driver type and its required connection parameters.

            ## Available Connector Types

            ### _OLAP Engines_
            - [**ClickHouse**](#clickhouse) - ClickHouse analytical database
            - [**Druid**](#druid) - Apache Druid
            - [**DuckDB**](#duckdb) - Embedded DuckDB engine (default)
            - [**MotherDuck**](#motherduck) - MotherDuck cloud database
            - [**Pinot**](#pinot) - Apache Pinot

            ### _Data Warehouses_
            - [**Athena**](#athena) - Amazon Athena
            - [**BigQuery**](#bigquery) - Google BigQuery
            - [**Redshift**](#redshift) - Amazon Redshift
            - [**Snowflake**](#snowflake) - Snowflake data warehouse

            ### _Databases_
            - [**MySQL**](#mysql) - MySQL databases
            - [**PostgreSQL**](#postgres) - PostgreSQL databases
            - [**SQLite**](#sqlite) - SQLite databases

            ### _Cloud Storage_
            - [**Azure**](#azure) - Azure Blob Storage
            - [**GCS**](#gcs) - Google Cloud Storage
            - [**S3**](#s3) - Amazon S3 storage

            ### _Other_
            - [**Extenral DuckDB**](#external-duckdb) - External DuckDB database
            - [**HTTPS**](#https) - Public files via HTTP/HTTPS
            - [**OpenAI**](#openai) - OpenAI data
            - [**Salesforce**](#salesforce) - Salesforce data
            - [**Slack**](#slack) - Slack data

            :::warning Security Recommendation
            For all credential parameters (passwords, tokens, keys), use environment variables with the syntax `{{.env.connector.<connector_driver>.<parameter_name>}}`. This keeps sensitive data out of your YAML files and version control. See our [credentials documentation](/developer/build/connectors/credentials/) for complete setup instructions.
            :::
        id: connectors
        title: Connector YAML
        type: object
        ```

        ## Examples

        ### DuckDB: Managed

        Explicit:

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb
        managed: true
        ```

        or relying on defaults:

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb
        ```

        ### DuckDB: With init_sql for S3 secrets

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb

        init_sql: |
          CREATE SECRET IF NOT EXISTS s3 (
            TYPE S3,
            KEY_ID '{{ .env.aws_access_key_id }}',
            SECRET '{{ .env.aws_secret_access_key }}',
            REGION 'us-east-1'
          )
        ```

        This is now deprecated in favor of creating a dedicated `s3.yaml` connector file, which Rill will automatically load and create as a secret in DuckDB.

        ### DuckDB: With extensions

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb

        init_sql: |
          INSTALL spatial;
          LOAD spatial;
        ```

        ### DuckDB: MotherDuck database with existing tables

        ```yaml
        # connectors/motherduck.yaml
        type: connector
        driver: duckdb
        path: "md:my_database"
        token: "{{ .env.motherduck_token }}"
        schema_name: main
        mode: read
        ```

        ### ClickHouse: Cloud with SSL

        ```yaml
        # connectors/clickhouse.yaml
        type: connector
        driver: clickhouse
        host: "abc123.us-east-1.aws.clickhouse.cloud"
        port: 8443
        username: "default"
        password: "{{ .env.clickhouse_password }}"
        database: "default"
        ssl: true
        ```

        ### ClickHouse: Readwrite with cluster

        ```yaml
        # connectors/clickhouse.yaml
        type: connector
        driver: clickhouse
        mode: readwrite

        host: "{{ .env.clickhouse_host }}"
        port: 9440
        username: "{{ .env.clickhouse_user }}"
        password: "{{ .env.clickhouse_password }}"
        database: "default"
        cluster: "my_cluster"
        ssl: true
        ```

        ### ClickHouse: Dev/prod configuration

        ```yaml
        # connectors/clickhouse.yaml
        type: connector
        driver: clickhouse
        mode: readwrite

        dev:
          managed: true

        prod:
          host: "{{ .env.clickhouse_host }}"
          port: 9440
          username: "{{ .env.clickhouse_user }}"
          password: "{{ .env.clickhouse_password }}"
          database: "default"
          cluster: "{{ .env.clickhouse_cluster }}"
          ssl: true
        ```

        ### S3: Basic with credentials and region

        ```yaml
        # connectors/s3.yaml
        type: connector
        driver: s3
        aws_access_key_id: "{{ .env.aws_access_key_id }}"
        aws_secret_access_key: "{{ .env.aws_secret_access_key }}"
        region: us-west-2
        ```

        ### S3: Cloudflare R2 (S3-compatible)

        ```yaml
        # connectors/r2.yaml
        type: connector
        driver: s3
        aws_access_key_id: "{{ .env.r2_access_key_id }}"
        aws_secret_access_key: "{{ .env.r2_secret_access_key }}"
        endpoint: "https://{{ .env.r2_account_id }}.r2.cloudflarestorage.com"
        region: auto
        ```

        ### GCS: Minimal (uses default credentials)

        ```yaml
        # connectors/gcs.yaml
        type: connector
        driver: gcs
        ```

        ### GCS: With explicit credentials

        ```yaml
        # connectors/gcs.yaml
        type: connector
        driver: gcs
        google_application_credentials: "{{ .env.gcs_service_account_json }}"
        ```

        ### BigQuery

        ```yaml
        # connectors/bigquery.yaml
        type: connector
        driver: bigquery
        project_id: "my-gcp-project"
        google_application_credentials: "{{ .env.bigquery_service_account_json }}"
        ```

        ### Snowflake: Basic with DSN

        ```yaml
        # connectors/snowflake.yaml
        type: connector
        driver: snowflake
        dsn: "{{ .env.snowflake_dsn }}"
        ```

        ### Postgres

        ```yaml
        # connectors/postgres.yaml
        type: connector
        driver: postgres
        host: "{{ .env.postgres_host }}"
        port: 5432
        user: "{{ .env.postgres_user }}"
        password: "{{ .env.postgres_password }}"
        dbname: "analytics"
        sslmode: require
        ```

        ### Druid: Host-based

        ```yaml
        # connectors/druid.yaml
        type: connector
        driver: druid
        host: "{{ .env.druid_host }}"
        port: 8888
        username: "{{ .env.druid_user }}"
        password: "{{ .env.druid_password }}"
        ssl: true
        ```

        ### Redshift: Serverless

        ```yaml
        # connectors/redshift.yaml
        type: connector
        driver: redshift
        aws_access_key_id: "{{ .env.aws_access_key_id }}"
        aws_secret_access_key: "{{ .env.aws_secret_access_key }}"
        workgroup: "my-workgroup"
        region: us-east-1
        database: "analytics"
        ```

        ### OpenAI

        ```yaml
        # connectors/openai.yaml
        type: connector
        driver: openai
        api_key: "{{ .env.openai_api_key }}"
        ```

        ### Slack

        ```yaml
        # connectors/slack.yaml
        type: connector
        driver: slack
        bot_token: "{{ .env.slack_bot_token }}"
        ```

        ### Pinot

        ```yaml
        # connectors/pinot.yaml
        type: connector
        driver: pinot
        broker_host: "{{ .env.pinot_broker_host }}"
        controller_host: "{{ .env.pinot_controller_host }}"
        username: "{{ .env.pinot_user }}"
        password: "{{ .env.pinot_password }}"
        ssl: true
        ```
    - role: user
      content_type: text
      content: |-
        You should develop a Rill project file based on the following task description:
        - Develop file at path: /connectors/s3.yaml
        - The file should be of type: connector
        - Task description: Create a new S3 connector resource.

        Requirements:
        - File path: connectors/s3.yaml
        - Resource name should be `s3` (derived from filename).
        - Use driver: s3.
        - Use env vars already present in the project status: `connector.s3.aws_access_key_id` and `connector.s3.aws_secret_access_key`.
        - Configure region conservatively (us-east-1) but allow override via env var if present (do not introduce new required env vars). If you include optional env var references, guard them with defaults if supported.
        - Keep minimal properties needed for Rill to access public/private buckets.

        Return valid YAML for a Rill connector.

        Here is some important context:
        - You are running as a sub-agent of a larger developer agent. Stay aligned on your specific task and avoid extra discovery.
        - When you call 'write_file', if it returns a parse or reconcile error, do your best to fix the issue and try again. If you think the error is unrelated to the current path, let the parent agent know to handle it.

        Here is some additional context that may or may not be relevant to your task:
        - The project's default OLAP connector is named duckdb (driver: duckdb).
    - role: assistant
      content_type: tool_call
      id: 3cf7cedf44504bb6
      tool_name: write_file
      content: '{"contents":"type: connector\ndriver: s3\n\n# Credentials are sourced from existing env vars (see project status)\naws_access_key_id: \"{{ .env.connector.s3.aws_access_key_id }}\"\naws_secret_access_key: \"{{ .env.connector.s3.aws_secret_access_key }}\"\n\n# Default conservatively to us-east-1; can be overridden by setting env var\nregion: \"{{ .env.connector.s3.region | default \\\"us-east-1\\\" }}\"\n","path":"connectors/s3.yaml"}'
    - role: tool
      content_type: tool_response
      id: 3cf7cedf44504bb6
      content: '{"diff":"--- /connectors/s3.yaml\n+++ /connectors/s3.yaml\n@@ -1 +1,10 @@\n+type: connector\n+driver: s3\n \n+# Credentials are sourced from existing env vars (see project status)\n+aws_access_key_id: \"{{ .env.connector.s3.aws_access_key_id }}\"\n+aws_secret_access_key: \"{{ .env.connector.s3.aws_secret_access_key }}\"\n+\n+# Default conservatively to us-east-1; can be overridden by setting env var\n+region: \"{{ .env.connector.s3.region | default \\\"us-east-1\\\" }}\"\n+\n","is_new_file":true,"resources":[{"kind":"rill.runtime.v1.Connector","name":"s3","reconcile_error":"failed to resolve template: template: :1:7: executing \"\" at \u003c.env.connector.s3.region\u003e: map has no entry for key \"region\"","reconcile_status":"RECONCILE_STATUS_IDLE"}]}'
    - role: assistant
      content_type: tool_call
      id: 4c256b092553439e
      tool_name: write_file
      content: '{"contents":"type: connector\ndriver: s3\n\n# Credentials are sourced from existing env vars (see project status)\naws_access_key_id: \"{{ .env.connector.s3.aws_access_key_id }}\"\naws_secret_access_key: \"{{ .env.connector.s3.aws_secret_access_key }}\"\n\n# Default conservatively to us-east-1; override by setting env var connector.s3.aws_region\naws_region: \"{{ .env.connector.s3.aws_region | default \\\"us-east-1\\\" }}\"\n","path":"connectors/s3.yaml"}'
    - role: tool
      content_type: tool_response
      id: 4c256b092553439e
      content: '{"diff":"--- /connectors/s3.yaml\n+++ /connectors/s3.yaml\n@@ -5,6 +5,6 @@\n aws_access_key_id: \"{{ .env.connector.s3.aws_access_key_id }}\"\n aws_secret_access_key: \"{{ .env.connector.s3.aws_secret_access_key }}\"\n \n-# Default conservatively to us-east-1; can be overridden by setting env var\n-region: \"{{ .env.connector.s3.region | default \\\"us-east-1\\\" }}\"\n+# Default conservatively to us-east-1; override by setting env var connector.s3.aws_region\n+aws_region: \"{{ .env.connector.s3.aws_region | default \\\"us-east-1\\\" }}\"\n \n","resources":[{"kind":"rill.runtime.v1.Connector","name":"s3","reconcile_error":"failed to resolve template: template: :1:7: executing \"\" at \u003c.env.connector.s3.aws_region\u003e: map has no entry for key \"aws_region\"","reconcile_status":"RECONCILE_STATUS_IDLE"}]}'
  response:
    - role: assistant
      content_type: tool_call
      id: call_P1hr4ilIn7ErPrZKu2zcZht4
      tool_name: write_file
      content: '{"contents":"type: connector\ndriver: s3\n\n# Credentials are sourced from existing env vars (see project status)\naws_access_key_id: \"{{ .env.connector.s3.aws_access_key_id }}\"\naws_secret_access_key: \"{{ .env.connector.s3.aws_secret_access_key }}\"\n\n# Conservative default. If you need a different region, update this value.\nregion: us-east-1\n","path":"connectors/s3.yaml"}'
- index: 7
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Models as SQL files:** Always create new models as `.yaml` files, not `.sql` files (which are harder to extend later).\n- **Not creating connector files:** When Rill has native support for a connector (like S3 or BigQuery), always create a dedicated connector resource file for it.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. This also includes models that load external data using a native SQL function, like `read_parquet(...)` or `s3(...)`. Non-materialized models become views, which re-execute on every query.\n- **Referencing non-existant environment variables:** Only reference environment variables that are present in `.env` (returned in `env` from `project_status`). If you need the user to add another environment variable, stop and ask them to do so.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: system
      content_type: text
      content: |-
        # Instructions for developing a connector in Rill

        ## Introduction

        Connectors are resources that contain credentials and settings for connecting to external systems. They are typically found at the root of the project's DAG, providing access to data sources and services that power downstream resources like models and metrics views.

        Connectors are usually lightweight resources. When reconciled, they validate the connection to the external system but do not move or process data. The main exception is managed OLAP connectors (with `managed: true`), which trigger database provisioning.

        ### Driver capabilities

        Each connector uses a **driver** that implements one or more capabilities:

        - **OLAP database**: Can power metrics views and dashboards (e.g., `duckdb`, `clickhouse`)
        - **SQL database**: Can run SQL queries as model inputs (e.g., `postgres`, `bigquery`, `snowflake`)
        - **Information schema**: Can list tables and their schemas (e.g., `duckdb`, `bigquery`)
        - **Object store**: Can list, read, and write flat files (e.g., `s3`, `gcs`)
        - **Notifier**: Can send notifications and alerts (e.g., `slack`)
        - **AI**: Can generate embeddings or responses (e.g., `openai`)

        ## Core Concepts

        ### Naming conventions

        Connectors are typically named after their driver (e.g., a file `connectors/duckdb.yaml` creates a connector named `duckdb`). Use descriptive names when you have multiple connectors of the same type:
        - `connectors/s3_data.yaml` and `connectors/s3_staging.yaml`
        - `connectors/clickhouse_prod.yaml` and `connectors/clickhouse_dev.yaml`

        ### Secrets management

        Always store sensitive credentials in `.env` and reference them using template syntax:

        ```yaml
        type: connector
        driver: s3
        aws_access_key_id: "{{ .env.aws_access_key_id }}"
        aws_secret_access_key: "{{ .env.aws_secret_access_key }}"
        ```

        NOTE: Some legacy projects use the deprecated `.vars` instead of `.env`.

        ### Managed connectors

        OLAP connectors can be provisioned automatically by Rill using `managed: true`. This is supported for `duckdb` and `clickhouse` drivers:

        ```yaml
        type: connector
        driver: duckdb
        managed: true
        ```

        When a managed connector is reconciled, Rill provisions the database infrastructure. The user is billed for the CPU, memory, and disk usage of the provisioned database.

        ### Access modes

        Control read/write access using the `mode` property:

        - `mode: read`: Prevents Rill models from writing to this connector
        - `mode: readwrite`: Allows both reading and writing (default for managed connectors)

        Use `mode: read` when connecting to external databases with pre-existing tables to prevent unintended modifications.

        ### Dev/Prod configuration

        Use `dev:` and `prod:` blocks for environment-specific settings:

        ```yaml
        type: connector
        driver: clickhouse
        mode: readwrite

        # Use a local database in development to prevent overwriting data in the production cluster when iterating on changes.
        dev:
          managed: true

        prod:
          host: "{{ .env.clickhouse_host }}"
          port: 9440
          username: "{{ .env.clickhouse_user }}"
          password: "{{ .env.clickhouse_password }}"
          ssl: true
        ```

        ## Driver-Specific Notes

        ### DuckDB

        DuckDB is Rill's default embedded OLAP database. Key properties:

        - `managed: true`: Rill provisions and manages the database
        - `init_sql`: SQL to run at startup (install extensions, create secrets, attach databases)

        For MotherDuck (cloud DuckDB), use the `path` property with `md:` prefix:

        ```yaml
        type: connector
        driver: duckdb
        path: "md:my_database"
        token: "{{ .env.motherduck_token }}"
        ```

        ### ClickHouse

        ClickHouse can be user-managed or Rill-managed. Key properties:

        - `managed: true`: Rill provisions and manages an empty Clickhouse cluster. If set, don't set any other connector properties.
        - `host`, `port`, `username`, `password`: Connection credentials
        - `database`: Target database name
        - `ssl: true`: Required for ClickHouse Cloud
        - `cluster`: Cluster name for multi-node Clickhouse clusters
        - `dsn`: Alternative connection string format (format: `clickhouse://host:port?username=<username>&...`)

        Common ports:
        - `8443`: HTTPS native protocol (ClickHouse Cloud)
        - `9440`: Secure native protocol
        - `9000`: Native protocol (non-SSL)

        ### S3

        AWS S3 and S3-compatible storage. Key properties:

        - `aws_access_key_id`, `aws_secret_access_key`: AWS credentials
        - `region`: AWS region
        - `endpoint`: Custom endpoint for S3-compatible services (R2, MinIO, GCS interop)
        - `path_prefixes`: A list of bucket paths that the connector can access, e.g. `[s3://my-bucket]`; useful for improving bucket introspection

        ### GCS

        Google Cloud Storage. Key properties:

        - `google_application_credentials`: Service account JSON (must be a literal JSON string value)
        - `key_id`: HMAC key ID to use instead of a service account JSON; required for direct use with DuckDB and Clickhouse through S3 compatibility
        - `secret`: HMAC secret to use instead of a service account JSON; required for direct use with DuckDB and Clickhouse through S3 compatibility

        ### BigQuery

        Google BigQuery. Key properties:

        - `project_id`: GCP project ID
        - `google_application_credentials`: Service account JSON

        ### Snowflake

        Snowflake data warehouse. Key properties:

        - `account`, `user`, `privateKey`, `database`, `schema`, `warehouse`, `role`: Connection parameters
        - `dsn`: Connection string to use instead of separate connection parameters

        ### Postgres

        PostgreSQL database. Key properties:

        - `host`, `port`, `user`, `password`, `dbname`: Connection parameters
        - `sslmode`: SSL mode (`disable`, `require`, `verify-full`, etc.)

        ### Druid

        Apache Druid. Can be configured via host/port or DSN:

        - `host`, `port`, `username`, `password`, `ssl`: Direct connection
        - `dsn`: Full connection string

        ### Redshift

        Amazon Redshift. Key properties:

        - `aws_access_key_id`, `aws_secret_access_key`: AWS credentials
        - `workgroup`: Redshift Serverless workgroup name
        - `region`: AWS region
        - `database`: Database name

        ### Athena

        Amazon Athena. Key properties:

        - `aws_access_key_id`, `aws_secret_access_key`: AWS credentials
        - `workgroup`: Redshift Serverless workgroup name
        - `region`: AWS region
        - `output_location`: S3 path in format `s3://bucket/path` to store temporary query results in (Athena only)

        ### Other drivers

        - **Slack**: Use `bot_token` for alert notifications
        - **OpenAI**: Use `api_key` for AI-powered features
        - **HTTPS**: Simple connector for public HTTP sources
        - **Pinot**: Use `broker_host`, `controller_host`, `username`, `password`

        ## JSON Schema

        Here is a full JSON schema for the connector syntax:

        ```
        allOf:
            - properties:
                type:
                    const: connector
                    description: Refers to the resource type and must be `connector`
                    type: string
              required:
                - type
              title: Properties
              type: object
            - $ref: '#/definitions/common_properties'
            - oneOf:
                - examples:
                    - allow_host_access: true
                      aws_access_key_id: myawsaccesskey
                      aws_access_token: mytemporarytoken
                      aws_region: us-east-1
                      aws_secret_access_key: myawssecretkey
                      driver: athena
                      external_id: MyExternalID
                      output_location: s3://my-bucket/athena-output/
                      role_arn: arn:aws:iam::123456789012:role/MyRole
                      role_session_name: MySession
                      type: connector
                      workgroup: primary
                  properties:
                    allow_host_access:
                        description: Allow the Athena client to access host environment configurations such as environment variables or local AWS credential files. Defaults to true, enabling use of credentials and settings from the host environment unless explicitly disabled.
                        type: boolean
                    aws_access_key_id:
                        description: AWS Access Key ID used for authentication. Required when using static credentials directly or as base credentials for assuming a role.
                        type: string
                    aws_access_token:
                        description: AWS session token used with temporary credentials. Required only if the Access Key and Secret Key are part of a temporary session credentials.
                        type: string
                    aws_region:
                        description: AWS region where Athena and the result S3 bucket are located (e.g., us-east-1). Defaults to 'us-east-1' if not specified.
                        type: string
                    aws_secret_access_key:
                        description: AWS Secret Access Key paired with the Access Key ID. Required when using static credentials directly or as base credentials for assuming a role.
                        type: string
                    driver:
                        const: athena
                        description: Refers to the driver type and must be driver `athena`
                        type: string
                    external_id:
                        description: External ID required by some roles when assuming them, typically for cross-account access. Used only if 'role_arn' is specified and the role's trust policy requires it.
                        type: string
                    output_location:
                        description: S3 URI where Athena query results should be stored (e.g., s3://your-bucket/athena/results/). Optional if the selected workgroup has a default result configuration.
                        type: string
                    role_arn:
                        description: ARN of the IAM role to assume. When specified, the SDK uses the base credentials to call STS AssumeRole and obtain temporary credentials scoped to this role.
                        type: string
                    role_session_name:
                        description: Session name to associate with the STS AssumeRole session. Used only if 'role_arn' is specified. Useful for identifying and auditing the session.
                        type: string
                    workgroup:
                        description: Athena workgroup to use for query execution. Defaults to 'primary' if not specified.
                        type: string
                  required:
                    - driver
                  title: Athena
                  type: object
                - examples:
                    - azure_storage_account: mystorageaccount
                      azure_storage_key: credentialstring
                      driver: azure
                      type: connector
                  properties:
                    allow_host_access:
                        description: Allow access to host environment configuration
                        type: boolean
                    azure_storage_account:
                        description: Azure storage account name
                        type: string
                    azure_storage_connection_string:
                        description: Optional azure connection string for storage account
                        type: string
                    azure_storage_key:
                        description: Azure storage access key
                        type: string
                    azure_storage_sas_token:
                        description: Optional azure SAS token for authentication
                        type: string
                    driver:
                        const: azure
                        description: Refers to the driver type and must be driver `azure`
                        type: string
                    path_prefixes:
                        description: |
                            A list of container or virtual directory prefixes that this connector is allowed to access.
                            Useful when different containers or paths use different credentials, allowing the system
                            to route access through the appropriate connector based on the blob path.
                            Example: `azure://my-bucket/`, ` azure://my-bucket/path/` ,`azure://my-bucket/path/prefix`
                        type:
                            - string
                            - array
                  required:
                    - driver
                    - azure_storage_account
                    - azure_storage_key
                  title: Azure
                  type: object
                - examples:
                    - allow_host_access: true
                      driver: bigquery
                      google_application_credentials: credentialjsonstring
                      project_id: my-project-id
                      type: connector
                  properties:
                    allow_host_access:
                        description: Enable the BigQuery client to use credentials from the host environment when no service account JSON is provided. This includes Application Default Credentials from environment variables, local credential files, or the Google Compute Engine metadata server. Defaults to true, allowing seamless authentication in GCP environments.
                        type: boolean
                    dataset_id:
                        description: BigQuery dataset ID
                        type: string
                    driver:
                        const: bigquery
                        description: Refers to the driver type and must be driver `bigquery`
                        type: string
                    google_application_credentials:
                        description: Raw contents of the Google Cloud service account key (in JSON format) used for authentication.
                        type: string
                    location:
                        description: BigQuery dataset location
                        type: string
                    project_id:
                        description: Google Cloud project ID
                        type: string
                  required:
                    - driver
                  title: BigQuery
                  type: object
                - examples:
                    - cluster: mycluster
                      database: mydatabase
                      driver: clickhouse
                      host: localhost
                      managed: false
                      mode: readwrite
                      password: mypassword
                      port: 9000
                      ssl: true
                      type: connector
                      username: myusername
                  properties:
                    can_scale_to_zero:
                        description: Indicates if the database can scale to zero
                        type: boolean
                    cluster:
                        description: Cluster name, required for running distributed queries
                        type: string
                    conn_max_lifetime:
                        description: Maximum time a connection may be reused
                        type: string
                    database:
                        description: Name of the ClickHouse database within the cluster
                        type: string
                    dial_timeout:
                        description: Timeout for dialing the ClickHouse server
                        type: string
                    driver:
                        const: clickhouse
                        description: Refers to the driver type and must be driver `clickhouse`
                        type: string
                    dsn:
                        description: DSN(Data Source Name) for the ClickHouse connection
                        type: string
                    embed_port:
                        description: Port to run ClickHouse locally (0 for random port)
                        type: integer
                    host:
                        description: Host where the ClickHouse instance is running
                        type: string
                    log_queries:
                        description: Controls whether to log raw SQL queries
                        type: boolean
                    managed:
                        description: '`true` means Rill will provision the connector using the default provisioner. `false` disables automatic provisioning.'
                        type: boolean
                    max_idle_conns:
                        description: Maximum number of idle connections in the pool
                        type: integer
                    max_open_conns:
                        description: Maximum number of open connections to the database
                        type: integer
                    mode:
                        description: '`read` - Controls the operation mode for the ClickHouse connection. Defaults to ''read'' for safe operation with external databases. Set to ''readwrite'' to enable model creation and table mutations. Note: When ''managed: true'', this is automatically set to ''readwrite''.'
                        type: string
                    password:
                        description: Password for authentication
                        type: string
                    port:
                        description: Port where the ClickHouse instance is accessible
                        type: integer
                    query_settings:
                        description: query settings to be set on dashboard queries. `query_settings_override` takes precedence over these settings and if set these are ignored. Each setting must be separated by a comma. Example `max_threads = 8, max_memory_usage = 10000000000`
                        type: string
                    query_settings_override:
                        description: override the default settings used in queries. Changing the default settings can lead to incorrect query results and is generally not recommended. If you need to add settings, use `query_settings`
                        type: string
                    read_timeout:
                        description: Maximum time for a connection to read data
                        type: string
                    ssl:
                        description: Indicates whether a secured SSL connection is required
                        type: boolean
                    username:
                        description: Username for authentication
                        type: string
                  required:
                    - driver
                  title: ClickHouse
                  type: object
                - examples:
                    - driver: druid
                      host: localhost
                      password: mypassword
                      port: 8082
                      ssl: true
                      type: connector
                      username: myusername
                  properties:
                    driver:
                        const: druid
                        description: Refers to the driver type and must be driver `druid`
                        type: string
                    dsn:
                        description: Data Source Name (DSN) for connecting to Druid
                        type: string
                    host:
                        description: Hostname of the Druid coordinator or broker
                        type: string
                    log_queries:
                        description: Log raw SQL queries sent to Druid
                        type: boolean
                    max_open_conns:
                        description: Maximum number of open database connections (0 = default, -1 = unlimited)
                        type: integer
                    password:
                        description: Password for authenticating with Druid
                        type: string
                    port:
                        description: Port number of the Druid service
                        type: integer
                    skip_version_check:
                        description: Skip checking Druid version compatibility
                        type: boolean
                    ssl:
                        description: Enable SSL for secure connection
                        type: boolean
                    username:
                        description: Username for authenticating with Druid
                        type: string
                  required:
                    - driver
                    - dsn
                  title: Druid
                  type: object
                - examples:
                    - allow_host_access: true
                      cpu: 4
                      driver: duckdb
                      init_sql: INSTALL httpfs; LOAD httpfs;
                      log_queries: true
                      memory_limit_gb: 16
                      mode: readwrite
                      pool_size: 5
                      read_write_ratio: 0.7
                      type: connector
                  properties:
                    allow_host_access:
                        description: Whether access to local environment and file system is allowed
                        type: boolean
                    attach:
                        description: Full ATTACH statement to attach a DuckDB database
                        type: string
                    boot_queries:
                        description: Deprecated - Use init_sql instead
                        type: string
                    conn_init_sql:
                        description: SQL executed when a new connection is initialized
                        type: string
                    cpu:
                        description: Number of CPU cores available to the database
                        minimum: 1
                        type: integer
                    create_secrets_from_connectors:
                        description: List of connector names for which temporary secrets should be created before executing the SQL.
                        type:
                            - string
                            - array
                    database_name:
                        description: Name of the attached DuckDB database (auto-detected if not set)
                        type: string
                    driver:
                        const: duckdb
                        description: Must be "duckdb"
                        type: string
                    init_sql:
                        description: SQL executed during database initialization
                        type: string
                    log_queries:
                        description: Whether to log raw SQL queries executed through OLAP
                        type: boolean
                    memory_limit_gb:
                        description: Amount of memory in GB available to the database
                        minimum: 1
                        type: integer
                    mode:
                        default: read
                        description: Set the mode for the DuckDB connection.
                        enum:
                            - read
                            - readwrite
                        type: string
                    path:
                        description: Path to external DuckDB database
                        type: string
                    pool_size:
                        description: Number of concurrent connections and queries allowed
                        minimum: 1
                        type: integer
                    read_write_ratio:
                        default: 0.5
                        description: Ratio of resources allocated to read vs write operations
                        maximum: 1
                        minimum: 0
                        type: number
                    schema_name:
                        description: Default schema used by the DuckDB database
                        type: string
                  required:
                    - driver
                  title: DuckDB
                  type: object
                - examples:
                    - driver: duckdb
                      mode: read
                      path: /path/to/my-duckdb-database.db
                      type: connector
                  properties:
                    driver:
                        const: duckdb
                        description: Refers to the driver type and must be driver `duckdb`
                        type: string
                    mode:
                        default: read
                        description: Set the mode for the DuckDB connection.
                        enum:
                            - read
                            - readwrite
                        type: string
                    path:
                        description: Path to the DuckDB database
                        type: string
                  required:
                    - driver
                    - db
                  title: External DuckDB
                  type: object
                - examples:
                    - driver: gcs
                      google_application_credentials: credentialjsonstring
                      type: connector
                  properties:
                    allow_host_access:
                        description: Allow access to host environment configuration
                        type: boolean
                    driver:
                        const: gcs
                        description: Refers to the driver type and must be driver `gcs`
                        type: string
                    google_application_credentials:
                        description: Google Cloud credentials JSON string
                        type: string
                    key_id:
                        description: Optional S3-compatible Key ID when used in compatibility mode
                        type: string
                    path_prefixes:
                        description: "A list of bucket path prefixes that this connector is allowed to access. \nUseful when different buckets or bucket prefixes use different credentials, \nallowing the system to select the appropriate connector based on the bucket path.\nExample: `gs://my-bucket/`, ` gs://my-bucket/path/` ,`gs://my-bucket/path/prefix`\n"
                        type:
                            - string
                            - array
                    secret:
                        description: Optional S3-compatible Secret when used in compatibility mode
                        type: string
                  required:
                    - driver
                    - bucket
                  title: GCS
                  type: object
                - examples:
                    - driver: https
                      headers:
                        Authorization: Bearer my-token
                      type: connector
                  properties:
                    driver:
                        const: https
                        description: Refers to the driver type and must be driver `https`
                        type: string
                    headers:
                        additionalProperties:
                            type: string
                        description: HTTP headers to include in the request
                        type: object
                    path_prefixes:
                        description: |
                            A list of HTTP/HTTPS URL prefixes that this connector is allowed to access.
                            Useful when different URL namespaces use different credentials, enabling the
                            system to choose the appropriate connector based on the URL path.
                            Example: `https://example.com/`, ` https://example.com/path/` ,`https://example.com/path/prefix`
                        type:
                            - string
                            - array
                  required:
                    - driver
                    - path
                  title: HTTPS
                  type: object
                - examples:
                    - driver: duckdb
                      path: md:my_database
                      schema_name: my_schema
                      token: '{{ .env.connector.motherduck.token }}'
                      type: connector
                  properties:
                    create_secrets_from_connectors:
                        description: List of connector names for which temporary secrets should be created before executing the SQL.
                        type:
                            - string
                            - array
                    driver:
                        const: duckdb
                        description: Refers to the driver type and must be driver `duckdb`.
                        type: string
                    init_sql:
                        description: SQL executed during database initialization.
                        type: string
                    mode:
                        default: read
                        description: Set the mode for the MotherDuck connection. By default, it is set to 'read' which allows only read operations. Set to 'readwrite' to enable model creation and table mutations.
                        enum:
                            - read
                            - readwrite
                        type: string
                    path:
                        description: Path to your MD database
                        type: string
                    schema_name:
                        description: Define your schema if not main, uses main by default
                        type: string
                    token:
                        description: MotherDuck token
                        type: string
                  required:
                    - driver
                    - path
                    - token
                  title: MotherDuck
                  type: object
                - examples:
                    - database: mydb
                      driver: mysql
                      host: localhost
                      password: p@ss
                      port: 3306
                      ssl-mode: preferred
                      type: connector
                      user: user
                    - driver: mysql
                      dsn: mysql://user:p%40ss@localhost:3306/mydb?ssl-mode=preferred
                      type: connector
                  properties:
                    database:
                        description: Name of the MySQL database
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `mysql`
                        type: string
                    dsn:
                        description: |
                            **Data Source Name (DSN)** for the MySQL connection, provided in [MySQL URI format](https://dev.mysql.com/doc/refman/8.4/en/connecting-using-uri-or-key-value-pairs.html#connecting-using-uri).
                            The DSN must follow the standard MySQL URI scheme:
                            ```text
                            mysql://user:password@host:3306/my-db
                            ```
                            Rules for special characters in password:
                            - The following characters are allowed [unescaped in the URI](https://datatracker.ietf.org/doc/html/rfc3986#section-2.3): `~` `.` `_` `-`
                            - All other special characters must be percent-encoded (`%XX` format).
                            ```text
                            mysql://user:pa%40ss@localhost:3306/my-db   # password contains '@'
                            mysql://user:pa%3Ass@localhost:3306/my-db   # password contains ':'
                            ```
                        type: string
                    host:
                        description: Hostname of the MySQL server
                        type: string
                    password:
                        description: Password for authentication
                        type: string
                    port:
                        description: Port number for the MySQL server
                        type: integer
                    ssl-mode:
                        description: 'ssl mode options: `disabled`, `preferred`, or `required`.'
                        type: string
                    user:
                        description: Username for authentication
                        type: string
                  required:
                    - driver
                  title: MySQL
                  type: object
                - examples:
                    - api_key: my-api-key
                      api_type: openai
                      api_version: "2023-05-15"
                      base_url: https://api.openai.com/v1
                      driver: openai
                      model: gpt-4o
                      type: connector
                  properties:
                    api_key:
                        description: API key for connecting to OpenAI
                        type: string
                    api_type:
                        description: The type of OpenAI API to use
                        type: string
                    api_version:
                        description: The version of the OpenAI API to use (e.g., '2023-05-15'). Required when API Type is AZURE or AZURE_AD
                        type: string
                    base_url:
                        description: The base URL for the OpenAI API (e.g., 'https://api.openai.com/v1')
                        type: string
                    driver:
                        description: The driver type, must be set to "openai"
                        type: string
                    model:
                        description: The OpenAI model to use (e.g., 'gpt-4o')
                        type: string
                  required:
                    - api_key
                  title: OpenAI
                  type: object
                - examples:
                    - broker_host: localhost
                      broker_port: 9000
                      controller_host: localhost
                      controller_port: 9000
                      driver: pinot
                      log_queries: true
                      max_open_conns: 100
                      password: mypassword
                      ssl: true
                      type: connector
                      username: myusername
                  properties:
                    broker_host:
                        description: Hostname of the Pinot broker
                        type: string
                    broker_port:
                        description: Port number for the Pinot broker
                        type: integer
                    controller_host:
                        description: Hostname of the Pinot controller
                        type: string
                    controller_port:
                        description: Port number for the Pinot controller
                        type: integer
                    driver:
                        description: Refers to the driver type and must be driver `pinot`
                        type: string
                    dsn:
                        description: DSN(Data Source Name) for the Pinot connection
                        type: string
                    log_queries:
                        description: Log raw SQL queries executed through Pinot
                        type: boolean
                    max_open_conns:
                        description: Maximum number of open connections to the Pinot database
                        type: integer
                    password:
                        description: Password for authenticating with Pinot
                        type: string
                    ssl:
                        description: Enable SSL connection to Pinot
                        type: boolean
                    username:
                        description: Username for authenticating with Pinot
                        type: string
                  required:
                    - driver
                    - dsn
                    - broker_host
                    - controller_host
                  title: Pinot
                  type: object
                - examples:
                    - dbname: mydatabase
                      driver: postgres
                      host: localhost
                      password: mypassword
                      port: 5432
                      sslmode: prefer
                      type: connector
                      user: myusername
                    - driver: postgres
                      dsn: user=myusername password='my pass\'word' host=localhost port=5432 dbname=mydatabase sslmode=prefer
                      type: connector
                    - driver: postgres
                      dsn: postgres://myusername:p%40ss@localhost:5432/mydatabase?sslmode=prefer
                      type: connector
                  properties:
                    dbname:
                        description: Name of the Postgres database
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `postgres`
                        type: string
                    dsn:
                        description: |
                            **Data Source Name (DSN)** for the PostgreSQL connection, provided in
                            [PostgreSQL connection string format](https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING).
                            PostgreSQL supports both **key=value format** and **URI format**.

                            key=value format example:
                            ```text
                            user=user password=password host=host port=5432 dbname=mydb
                            ```
                            Rules for key=value format for special characters:
                            - To write an empty value, or a value containing spaces, `=`, single quotes, or backslashes, surround it with single quotes.
                            - Single quotes and backslashes inside a value must be escaped with a backslash (`\'` and `\\`).

                            URI format example:
                            ```text
                            postgres://user:password@host:5432/mydb
                            ```

                            Rules for URI format:
                            - The following characters are allowed [unescaped in the URI](https://datatracker.ietf.org/doc/html/rfc3986#section-2.3): `~` `.` `_` `-`
                            - All other special characters must be percent-encoded (`%XX` format).

                            Examples (URI format with encoded characters):
                            ```text
                            postgres://user:pa%40ss@localhost:5432/my-db   # '@' is encoded as %40
                            postgres://user:pa%3Ass@localhost:5432/my-db   # ':' is encoded as %3A
                            ```
                        type: string
                    host:
                        description: Hostname of the Postgres server
                        type: string
                    password:
                        description: Password for authentication
                        type: string
                    port:
                        description: Port number for the Postgres server
                        type: string
                    sslmode:
                        description: 'ssl mode options: `disable`, `allow`, `prefer` or `require`.'
                        type: string
                    user:
                        description: Username for authentication
                        type: string
                  required:
                    - driver
                  title: Postgres
                  type: object
                - examples:
                    - aws_access_key_id: my-access-key-id
                      aws_access_token: my-access-token
                      aws_secret_access_key: my-secret-access-key
                      cluster_identifier: my-cluster-identifier
                      database: mydatabase
                      driver: redshift
                      region: us-east-1
                      type: connector
                      workgroup: my-workgroup
                  properties:
                    aws_access_key_id:
                        description: AWS Access Key ID used for authenticating with Redshift.
                        type: string
                    aws_access_token:
                        description: AWS Session Token for temporary credentials (optional).
                        type: string
                    aws_secret_access_key:
                        description: AWS Secret Access Key used for authenticating with Redshift.
                        type: string
                    cluster_identifier:
                        description: Cluster identifier for provisioned Redshift clusters, in case of Redshift Serverless use 'workgroup' .
                        type: string
                    database:
                        description: Name of the Redshift database to query.
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `redshift`
                        type: string
                    region:
                        description: AWS region where the Redshift cluster or workgroup is hosted (e.g., 'us-east-1').
                        type: string
                    workgroup:
                        description: Workgroup name for Redshift Serverless, in case of provisioned Redshift clusters use 'cluster_identifier'.
                        type: string
                  required:
                    - driver
                    - aws_access_key_id
                    - aws_secret_access_key
                    - database
                  title: Redshift
                  type: object
                - examples:
                    - aws_access_key_id: my-access-key-id
                      aws_access_token: my-access-token
                      aws_secret_access_key: my-secret-access-key
                      driver: s3
                      endpoint: https://my-s3-endpoint.com
                      region: us-east-1
                      type: connector
                  properties:
                    allow_host_access:
                        description: Allow access to host environment configuration
                        type: boolean
                    aws_access_key_id:
                        description: AWS Access Key ID used for authentication
                        type: string
                    aws_access_token:
                        description: Optional AWS session token for temporary credentials
                        type: string
                    aws_secret_access_key:
                        description: AWS Secret Access Key used for authentication
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `s3`
                        type: string
                    endpoint:
                        description: Optional custom endpoint URL for S3-compatible storage
                        type: string
                    path_prefixes:
                        description: |
                            A list of bucket path prefixes that this connector is allowed to access.
                            Useful when different buckets or bucket prefixes use different credentials,
                            allowing the system to select the appropriate connector based on the bucket path.
                            Example: `s3://my-bucket/`, ` s3://my-bucket/path/` ,`s3://my-bucket/path/prefix`
                        type:
                            - string
                            - array
                    region:
                        description: AWS region of the S3 bucket
                        type: string
                  required:
                    - driver
                    - bucket
                  title: S3
                  type: object
                - examples:
                    - client_id: my-client-id
                      driver: salesforce
                      endpoint: https://login.salesforce.com
                      password: mypassword
                      type: connector
                      username: myusername
                  properties:
                    client_id:
                        description: Client ID used for Salesforce OAuth authentication
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `salesforce`
                        type: string
                    endpoint:
                        description: Salesforce API endpoint URL
                        type: string
                    key:
                        description: Authentication key for Salesforce (secret)
                        type: string
                    password:
                        description: Salesforce account password (secret)
                        type: string
                    username:
                        description: Salesforce account username
                        type: string
                  required:
                    - driver
                    - username
                    - endpoint
                    - client_id
                  title: Salesforce
                  type: object
                - examples:
                    - bot_token: xoxb-my-bot-token
                      driver: slack
                      type: connector
                  properties:
                    bot_token:
                        description: Bot token used for authenticating Slack API requests
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `slack`
                        type: string
                  required:
                    - driver
                    - bot_token
                  title: Slack
                  type: object
                - examples:
                    - account: my_account_identifier
                      database: my_db
                      driver: snowflake
                      parallel_fetch_limit: 2
                      privateKey: '{{ .env.SNOWFLAKE_PRIVATE_KEY }}'
                      role: my_role
                      schema: my_schema
                      type: connector
                      user: my_user
                      warehouse: my_wh
                    - driver: snowflake
                      dsn: '{{ .env.SNOWFLAKE_DSN }}'
                      parallel_fetch_limit: 2
                      type: connector
                  properties:
                    account:
                        description: Snowflake account identifier. To find your Snowflake account identifier, look at your Snowflake account URL. The account identifier is everything before .snowflakecomputing.com
                        type: string
                    authenticator:
                        description: Optional authenticator type (e.g., SNOWFLAKE_JWT).
                        type: string
                    database:
                        description: Name of the Snowflake database.
                        type: string
                    driver:
                        const: snowflake
                        description: Refers to the driver type and must be driver `snowflake`
                        type: string
                    dsn:
                        description: "DSN (Data Source Name) for the Snowflake connection.\n\nThis is intended for **advanced configuration** where you want to specify\nproperties that are not explicitly defined above.  \nIt can only be used when the other connection fields (account, user, password,\ndatabase, schema, warehouse, role, authenticator, privateKey) are **not used**.\n\nFor details on private key generation and encoding, see the `privateKey` property.\n"
                        type: string
                    parallel_fetch_limit:
                        description: Maximum number of concurrent fetches during query execution.
                        type: integer
                    password:
                        description: Password for the Snowflake connection. _(deprecated, use privateKey instead)_
                        type: string
                    privateKey:
                        description: |
                            Private key for JWT authentication.
                            :::tip
                            Private key must be generated as a **PKCS#8 (nocrypt) key**, since the Snowflake Go driver
                            only supports unencrypted private keys. After generating, it must be **base64 URL encoded**.

                            Example commands to generate and encode:

                            ```bash
                            # Generate a 2048-bit unencrypted PKCS#8 private key
                            openssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out rsa_key.p8 -nocrypt

                            # Convert URL safe format for Snowflake
                            cat rsa_key.p8 | grep -v "\----" | tr -d '\n' | tr '+/' '-_'
                            ```
                            See: https://docs.snowflake.com/en/guide/key-pair-auth
                            :::
                        type: string
                    role:
                        description: Snowflake role to use.
                        type: string
                    schema:
                        description: Schema within the database to use.
                        type: string
                    user:
                        description: Username for the Snowflake connection.
                        type: string
                    warehouse:
                        description: Compute warehouse to use for queries.
                        type: string
                  required:
                    - type
                    - driver
                  title: Snowflake
                  type: object
                - examples:
                    - driver: sqlite
                      dsn: file:mydatabase.db
                      type: connector
                  properties:
                    driver:
                        description: Refers to the driver type and must be driver `sqlite`
                        type: string
                    dsn:
                        description: DSN(Data Source Name) for the sqlite connection
                        type: string
                  required:
                    - driver
                    - dsn
                  title: SQLite
                  type: object
        description: |
            Connector YAML files define how Rill connects to external data sources and OLAP engines. Each connector specifies a driver type and its required connection parameters.

            ## Available Connector Types

            ### _OLAP Engines_
            - [**ClickHouse**](#clickhouse) - ClickHouse analytical database
            - [**Druid**](#druid) - Apache Druid
            - [**DuckDB**](#duckdb) - Embedded DuckDB engine (default)
            - [**MotherDuck**](#motherduck) - MotherDuck cloud database
            - [**Pinot**](#pinot) - Apache Pinot

            ### _Data Warehouses_
            - [**Athena**](#athena) - Amazon Athena
            - [**BigQuery**](#bigquery) - Google BigQuery
            - [**Redshift**](#redshift) - Amazon Redshift
            - [**Snowflake**](#snowflake) - Snowflake data warehouse

            ### _Databases_
            - [**MySQL**](#mysql) - MySQL databases
            - [**PostgreSQL**](#postgres) - PostgreSQL databases
            - [**SQLite**](#sqlite) - SQLite databases

            ### _Cloud Storage_
            - [**Azure**](#azure) - Azure Blob Storage
            - [**GCS**](#gcs) - Google Cloud Storage
            - [**S3**](#s3) - Amazon S3 storage

            ### _Other_
            - [**Extenral DuckDB**](#external-duckdb) - External DuckDB database
            - [**HTTPS**](#https) - Public files via HTTP/HTTPS
            - [**OpenAI**](#openai) - OpenAI data
            - [**Salesforce**](#salesforce) - Salesforce data
            - [**Slack**](#slack) - Slack data

            :::warning Security Recommendation
            For all credential parameters (passwords, tokens, keys), use environment variables with the syntax `{{.env.connector.<connector_driver>.<parameter_name>}}`. This keeps sensitive data out of your YAML files and version control. See our [credentials documentation](/developer/build/connectors/credentials/) for complete setup instructions.
            :::
        id: connectors
        title: Connector YAML
        type: object
        ```

        ## Examples

        ### DuckDB: Managed

        Explicit:

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb
        managed: true
        ```

        or relying on defaults:

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb
        ```

        ### DuckDB: With init_sql for S3 secrets

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb

        init_sql: |
          CREATE SECRET IF NOT EXISTS s3 (
            TYPE S3,
            KEY_ID '{{ .env.aws_access_key_id }}',
            SECRET '{{ .env.aws_secret_access_key }}',
            REGION 'us-east-1'
          )
        ```

        This is now deprecated in favor of creating a dedicated `s3.yaml` connector file, which Rill will automatically load and create as a secret in DuckDB.

        ### DuckDB: With extensions

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb

        init_sql: |
          INSTALL spatial;
          LOAD spatial;
        ```

        ### DuckDB: MotherDuck database with existing tables

        ```yaml
        # connectors/motherduck.yaml
        type: connector
        driver: duckdb
        path: "md:my_database"
        token: "{{ .env.motherduck_token }}"
        schema_name: main
        mode: read
        ```

        ### ClickHouse: Cloud with SSL

        ```yaml
        # connectors/clickhouse.yaml
        type: connector
        driver: clickhouse
        host: "abc123.us-east-1.aws.clickhouse.cloud"
        port: 8443
        username: "default"
        password: "{{ .env.clickhouse_password }}"
        database: "default"
        ssl: true
        ```

        ### ClickHouse: Readwrite with cluster

        ```yaml
        # connectors/clickhouse.yaml
        type: connector
        driver: clickhouse
        mode: readwrite

        host: "{{ .env.clickhouse_host }}"
        port: 9440
        username: "{{ .env.clickhouse_user }}"
        password: "{{ .env.clickhouse_password }}"
        database: "default"
        cluster: "my_cluster"
        ssl: true
        ```

        ### ClickHouse: Dev/prod configuration

        ```yaml
        # connectors/clickhouse.yaml
        type: connector
        driver: clickhouse
        mode: readwrite

        dev:
          managed: true

        prod:
          host: "{{ .env.clickhouse_host }}"
          port: 9440
          username: "{{ .env.clickhouse_user }}"
          password: "{{ .env.clickhouse_password }}"
          database: "default"
          cluster: "{{ .env.clickhouse_cluster }}"
          ssl: true
        ```

        ### S3: Basic with credentials and region

        ```yaml
        # connectors/s3.yaml
        type: connector
        driver: s3
        aws_access_key_id: "{{ .env.aws_access_key_id }}"
        aws_secret_access_key: "{{ .env.aws_secret_access_key }}"
        region: us-west-2
        ```

        ### S3: Cloudflare R2 (S3-compatible)

        ```yaml
        # connectors/r2.yaml
        type: connector
        driver: s3
        aws_access_key_id: "{{ .env.r2_access_key_id }}"
        aws_secret_access_key: "{{ .env.r2_secret_access_key }}"
        endpoint: "https://{{ .env.r2_account_id }}.r2.cloudflarestorage.com"
        region: auto
        ```

        ### GCS: Minimal (uses default credentials)

        ```yaml
        # connectors/gcs.yaml
        type: connector
        driver: gcs
        ```

        ### GCS: With explicit credentials

        ```yaml
        # connectors/gcs.yaml
        type: connector
        driver: gcs
        google_application_credentials: "{{ .env.gcs_service_account_json }}"
        ```

        ### BigQuery

        ```yaml
        # connectors/bigquery.yaml
        type: connector
        driver: bigquery
        project_id: "my-gcp-project"
        google_application_credentials: "{{ .env.bigquery_service_account_json }}"
        ```

        ### Snowflake: Basic with DSN

        ```yaml
        # connectors/snowflake.yaml
        type: connector
        driver: snowflake
        dsn: "{{ .env.snowflake_dsn }}"
        ```

        ### Postgres

        ```yaml
        # connectors/postgres.yaml
        type: connector
        driver: postgres
        host: "{{ .env.postgres_host }}"
        port: 5432
        user: "{{ .env.postgres_user }}"
        password: "{{ .env.postgres_password }}"
        dbname: "analytics"
        sslmode: require
        ```

        ### Druid: Host-based

        ```yaml
        # connectors/druid.yaml
        type: connector
        driver: druid
        host: "{{ .env.druid_host }}"
        port: 8888
        username: "{{ .env.druid_user }}"
        password: "{{ .env.druid_password }}"
        ssl: true
        ```

        ### Redshift: Serverless

        ```yaml
        # connectors/redshift.yaml
        type: connector
        driver: redshift
        aws_access_key_id: "{{ .env.aws_access_key_id }}"
        aws_secret_access_key: "{{ .env.aws_secret_access_key }}"
        workgroup: "my-workgroup"
        region: us-east-1
        database: "analytics"
        ```

        ### OpenAI

        ```yaml
        # connectors/openai.yaml
        type: connector
        driver: openai
        api_key: "{{ .env.openai_api_key }}"
        ```

        ### Slack

        ```yaml
        # connectors/slack.yaml
        type: connector
        driver: slack
        bot_token: "{{ .env.slack_bot_token }}"
        ```

        ### Pinot

        ```yaml
        # connectors/pinot.yaml
        type: connector
        driver: pinot
        broker_host: "{{ .env.pinot_broker_host }}"
        controller_host: "{{ .env.pinot_controller_host }}"
        username: "{{ .env.pinot_user }}"
        password: "{{ .env.pinot_password }}"
        ssl: true
        ```
    - role: user
      content_type: text
      content: |-
        You should develop a Rill project file based on the following task description:
        - Develop file at path: /connectors/s3.yaml
        - The file should be of type: connector
        - Task description: Create a new S3 connector resource.

        Requirements:
        - File path: connectors/s3.yaml
        - Resource name should be `s3` (derived from filename).
        - Use driver: s3.
        - Use env vars already present in the project status: `connector.s3.aws_access_key_id` and `connector.s3.aws_secret_access_key`.
        - Configure region conservatively (us-east-1) but allow override via env var if present (do not introduce new required env vars). If you include optional env var references, guard them with defaults if supported.
        - Keep minimal properties needed for Rill to access public/private buckets.

        Return valid YAML for a Rill connector.

        Here is some important context:
        - You are running as a sub-agent of a larger developer agent. Stay aligned on your specific task and avoid extra discovery.
        - When you call 'write_file', if it returns a parse or reconcile error, do your best to fix the issue and try again. If you think the error is unrelated to the current path, let the parent agent know to handle it.

        Here is some additional context that may or may not be relevant to your task:
        - The project's default OLAP connector is named duckdb (driver: duckdb).
    - role: assistant
      content_type: tool_call
      id: 3cf7cedf44504bb6
      tool_name: write_file
      content: '{"contents":"type: connector\ndriver: s3\n\n# Credentials are sourced from existing env vars (see project status)\naws_access_key_id: \"{{ .env.connector.s3.aws_access_key_id }}\"\naws_secret_access_key: \"{{ .env.connector.s3.aws_secret_access_key }}\"\n\n# Default conservatively to us-east-1; can be overridden by setting env var\nregion: \"{{ .env.connector.s3.region | default \\\"us-east-1\\\" }}\"\n","path":"connectors/s3.yaml"}'
    - role: tool
      content_type: tool_response
      id: 3cf7cedf44504bb6
      content: '{"diff":"--- /connectors/s3.yaml\n+++ /connectors/s3.yaml\n@@ -1 +1,10 @@\n+type: connector\n+driver: s3\n \n+# Credentials are sourced from existing env vars (see project status)\n+aws_access_key_id: \"{{ .env.connector.s3.aws_access_key_id }}\"\n+aws_secret_access_key: \"{{ .env.connector.s3.aws_secret_access_key }}\"\n+\n+# Default conservatively to us-east-1; can be overridden by setting env var\n+region: \"{{ .env.connector.s3.region | default \\\"us-east-1\\\" }}\"\n+\n","is_new_file":true,"resources":[{"kind":"rill.runtime.v1.Connector","name":"s3","reconcile_error":"failed to resolve template: template: :1:7: executing \"\" at \u003c.env.connector.s3.region\u003e: map has no entry for key \"region\"","reconcile_status":"RECONCILE_STATUS_IDLE"}]}'
    - role: assistant
      content_type: tool_call
      id: 4c256b092553439e
      tool_name: write_file
      content: '{"contents":"type: connector\ndriver: s3\n\n# Credentials are sourced from existing env vars (see project status)\naws_access_key_id: \"{{ .env.connector.s3.aws_access_key_id }}\"\naws_secret_access_key: \"{{ .env.connector.s3.aws_secret_access_key }}\"\n\n# Default conservatively to us-east-1; override by setting env var connector.s3.aws_region\naws_region: \"{{ .env.connector.s3.aws_region | default \\\"us-east-1\\\" }}\"\n","path":"connectors/s3.yaml"}'
    - role: tool
      content_type: tool_response
      id: 4c256b092553439e
      content: '{"diff":"--- /connectors/s3.yaml\n+++ /connectors/s3.yaml\n@@ -5,6 +5,6 @@\n aws_access_key_id: \"{{ .env.connector.s3.aws_access_key_id }}\"\n aws_secret_access_key: \"{{ .env.connector.s3.aws_secret_access_key }}\"\n \n-# Default conservatively to us-east-1; can be overridden by setting env var\n-region: \"{{ .env.connector.s3.region | default \\\"us-east-1\\\" }}\"\n+# Default conservatively to us-east-1; override by setting env var connector.s3.aws_region\n+aws_region: \"{{ .env.connector.s3.aws_region | default \\\"us-east-1\\\" }}\"\n \n","resources":[{"kind":"rill.runtime.v1.Connector","name":"s3","reconcile_error":"failed to resolve template: template: :1:7: executing \"\" at \u003c.env.connector.s3.aws_region\u003e: map has no entry for key \"aws_region\"","reconcile_status":"RECONCILE_STATUS_IDLE"}]}'
    - role: assistant
      content_type: tool_call
      id: 4fa385fa2acc45a4
      tool_name: write_file
      content: '{"contents":"type: connector\ndriver: s3\n\n# Credentials are sourced from existing env vars (see project status)\naws_access_key_id: \"{{ .env.connector.s3.aws_access_key_id }}\"\naws_secret_access_key: \"{{ .env.connector.s3.aws_secret_access_key }}\"\n\n# Conservative default. If you need a different region, update this value.\nregion: us-east-1\n","path":"connectors/s3.yaml"}'
    - role: tool
      content_type: tool_response
      id: 4fa385fa2acc45a4
      content: '{"diff":"--- /connectors/s3.yaml\n+++ /connectors/s3.yaml\n@@ -5,6 +5,6 @@\n aws_access_key_id: \"{{ .env.connector.s3.aws_access_key_id }}\"\n aws_secret_access_key: \"{{ .env.connector.s3.aws_secret_access_key }}\"\n \n-# Default conservatively to us-east-1; override by setting env var connector.s3.aws_region\n-aws_region: \"{{ .env.connector.s3.aws_region | default \\\"us-east-1\\\" }}\"\n+# Conservative default. If you need a different region, update this value.\n+region: us-east-1\n \n","resources":[{"kind":"rill.runtime.v1.Connector","name":"s3","reconcile_error":"","reconcile_status":"RECONCILE_STATUS_IDLE"}]}'
  response:
    - role: assistant
      content_type: text
      content: |-
        ```yaml
        type: connector
        driver: s3

        # Credentials (from existing env vars)
        aws_access_key_id: "{{ .env.connector.s3.aws_access_key_id }}"
        aws_secret_access_key: "{{ .env.connector.s3.aws_secret_access_key }}"

        # Conservative default region
        region: us-east-1
        ```
- index: 8
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Models as SQL files:** Always create new models as `.yaml` files, not `.sql` files (which are harder to extend later).\n- **Not creating connector files:** When Rill has native support for a connector (like S3 or BigQuery), always create a dedicated connector resource file for it.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. This also includes models that load external data using a native SQL function, like `read_parquet(...)` or `s3(...)`. Non-materialized models become views, which re-execute on every query.\n- **Referencing non-existant environment variables:** Only reference environment variables that are present in `.env` (returned in `env` from `project_status`). If you need the user to add another environment variable, stop and ask them to do so.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: user
      content_type: text
      content: |-
        This may not relate to the user's task, but for context, the project's default OLAP connector is named duckdb (driver: duckdb).
         The default OLAP is in readwrite mode, so you can use it in models if you want.


        Task: I have some CSV files in S3. Can you create a connector for S3 and a DuckDB model that loads the data at s3://integration-test.rilldata.com/glob_test/y=*/*.csv? I've already added environment variables for S3 access. Please proceed without asking clarifying questions.
    - role: assistant
      content_type: tool_call
      id: 96eedb8a328f4b8c
      tool_name: list_files
      content: '{}'
    - role: tool
      content_type: tool_response
      id: 96eedb8a328f4b8c
      content: '{"files":[{"path":"/connectors/duckdb.yaml","resources":[{"kind":"rill.runtime.v1.Connector","name":"duckdb","reconcile_error":"","reconcile_status":"RECONCILE_STATUS_IDLE"}]},{"path":"/rill.yaml"}]}'
    - role: assistant
      content_type: tool_call
      id: 12392148d788479a
      tool_name: project_status
      content: '{}'
    - role: tool
      content_type: tool_response
      id: 12392148d788479a
      content: '{"default_olap_connector":"duckdb","env":["connector.openai.api_key","connector.s3.aws_access_key_id","connector.s3.aws_secret_access_key"],"resources":[{"kind":"rill.runtime.v1.ProjectParser","name":"parser","path":"","reconcile_error":"","reconcile_status":"RECONCILE_STATUS_IDLE","refs":[]},{"kind":"rill.runtime.v1.Connector","name":"duckdb","path":"/connectors/duckdb.yaml","reconcile_error":"","reconcile_status":"RECONCILE_STATUS_IDLE","refs":[]}],"parse_errors":[]}'
    - role: assistant
      content_type: tool_call
      id: e4a7881f2a234928
      tool_name: search_files
      content: '{"glob_pattern":"**/*.yaml","pattern":"type:\\s*connector|driver:\\s*s3|s3"}'
    - role: tool
      content_type: tool_response
      id: e4a7881f2a234928
      content: '{"matches":[{"path":"/connectors/duckdb.yaml","lines":[2],"snippets":["  1: \n\u003e 2: type: connector\n  3: driver: duckdb\n  4: managed: true"]}]}'
    - role: assistant
      content_type: tool_call
      id: c2809355b0b942e6
      tool_name: develop_file
      content: '{"path":"connectors/s3.yaml","prompt":"Create a new S3 connector resource.\n\nRequirements:\n- File path: connectors/s3.yaml\n- Resource name should be `s3` (derived from filename).\n- Use driver: s3.\n- Use env vars already present in the project status: `connector.s3.aws_access_key_id` and `connector.s3.aws_secret_access_key`.\n- Configure region conservatively (us-east-1) but allow override via env var if present (do not introduce new required env vars). If you include optional env var references, guard them with defaults if supported.\n- Keep minimal properties needed for Rill to access public/private buckets.\n\nReturn valid YAML for a Rill connector.","type":"connector"}'
    - role: tool
      content_type: tool_response
      id: c2809355b0b942e6
      content: '{"response":"```yaml\ntype: connector\ndriver: s3\n\n# Credentials (from existing env vars)\naws_access_key_id: \"{{ .env.connector.s3.aws_access_key_id }}\"\naws_secret_access_key: \"{{ .env.connector.s3.aws_secret_access_key }}\"\n\n# Conservative default region\nregion: us-east-1\n```"}'
  response:
    - role: assistant
      content_type: tool_call
      id: call_EXYOF6GRWIpcLHYHkAB1WLm0
      tool_name: develop_file
      content: '{"path":"models/glob_test_csv.yaml","prompt":"Create a DuckDB model that loads CSV data from S3 path:\n`s3://integration-test.rilldata.com/glob_test/y=*/*.csv`\n\nConstraints / best practices:\n- Create as YAML (not .sql).\n- Materialize into default OLAP connector (duckdb) (readwrite).\n- Use DuckDB''s `read_csv_auto` (or `read_csv`) with a glob that supports the given wildcard.\n- Ensure the model reads from S3; configure it to use the S3 connector by setting `input` connector to `s3` if needed, but the query will execute in duckdb (output to duckdb). If Rill supports separate input/output connectors in the model, set input to s3 and output to duckdb.\n- Include `materialize: true`.\n- Add a few robust CSV options: `header=true`, `ignore_errors=true` (if supported), `all_varchar=true` (optional) to avoid type issues, and include a `_rill_loaded_at` column with `now()`.\n- Name the table/model `glob_test_csv`.\n\nReturn valid Rill model YAML.","type":"model"}'
- index: 9
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Models as SQL files:** Always create new models as `.yaml` files, not `.sql` files (which are harder to extend later).\n- **Not creating connector files:** When Rill has native support for a connector (like S3 or BigQuery), always create a dedicated connector resource file for it.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. This also includes models that load external data using a native SQL function, like `read_parquet(...)` or `s3(...)`. Non-materialized models become views, which re-execute on every query.\n- **Referencing non-existant environment variables:** Only reference environment variables that are present in `.env` (returned in `env` from `project_status`). If you need the user to add another environment variable, stop and ask them to do so.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: system
      content_type: text
      content: "# Instructions for developing a model in Rill\n\n## Introduction\n\nModels are resources that specify ETL or transformation logic, outputting a tabular dataset to one of the project's connectors. They are typically found near the root of the project's DAG, referencing only connectors and other models.\n\nBy default, models output data as a table with the same name as the model in the project's default OLAP connector. The core of a model is usually a `SELECT` SQL statement, which Rill executes as `CREATE TABLE <name> AS <SELECT statement>`. The SQL should be a plain SELECT query without a trailing semicolon.\n\nModels in Rill are similar to models in dbt, but support additional advanced features:\n- **Different input and output connectors:** Run a query in one database (e.g., BigQuery) and output results to another (e.g., DuckDB or ClickHouse).\n- **Stateful incremental ingestion:** Track state and load only new or changed data.\n- **Partition support:** Define explicit partitions (e.g., Hive-partitioned files in S3) for scalable, idempotent incremental runs.\n- **Scheduled refresh:** Use cron expressions to automatically refresh data on a schedule.\n\n### Model categories\n\nWhen reasoning about a model, consider these attributes:\n\n- **Source model**: References external data, typically reading from a SQL database or object store connector and writing to an OLAP connector.\n- **Derived model**: References other models, usually performing joins or formatting columns to prepare denormalized tables for metrics views and dashboards.\n- **Incremental model**: Contains logic for incrementally loading data, processing only new or changed records.\n- **Partitioned model**: Loads data in well-defined increments (e.g., daily partitions), enabling scalability and idempotent incremental runs.\n- **Materialized model**: Outputs a physical table rather than a SQL view.\n\n### Performance considerations\n\nModels are usually expensive resources that can take a long time to run. Create or edit them with caution.\n\n**Exception:** Non-materialized models with the same input and output connector are cheap because they are created as SQL views rather than physical tables.\n\n**Development tip:** Use a \"dev partition\" to limit data processed during development. This speeds up iteration and avoids unnecessary costs. See the partitions section below for details.\n\n### Generating synthetic data for prototyping\n\nWhen developing models for prototyping or demonstration purposes where external data sources are not yet available, generate a `SELECT` query that returns realistic synthetic data with these characteristics:\n- Use realistic column names and data types that match typical business scenarios\n- Always include a time/timestamp column for time-series analysis\n- Generate 6-12 months of historical data with approximately 10,000 rows to enable meaningful analysis\n- Space out timestamps realistically across the time period rather than clustering them\n- Use realistic data distributions (e.g., varying quantities, diverse categories, plausible geographic distributions)\n\nOnly generate synthetic data when the user explicitly requests mock data or when required external sources don't exist in the project. If real data sources are available, always prefer using them.\n\n## Materialization\n\nThe `materialize:` property controls whether a model creates a physical table or a SQL view:\n\n- `materialize: true`: Creates a physical table. Use this for source models, expensive transformations, or when downstream queries need fast access.\n- `materialize: false`: Creates a SQL view. The query re-executes on every access. Only suitable for lightweight transformations where input and output connectors are the same that never reference external data.\n\nIf `materialize` is omitted, it defaults to `true` for all cross-connector models and `false` for single-connector models (i.e. where the input and output connector is the same).\n\nIn model files with a `.sql` extension, you can materialize by putting this on the first line of the file:\n```sql\n-- @materialize: true\n```\n\n**Best practices:**\n- Always materialize models that reference external data sources.\n- Always materialize models that perform expensive joins or aggregations.\n- Use views only for simple transformations on top of already-materialized tables.\n\n## Incremental models\n\nIncremental models process only new or changed data instead of reprocessing the entire dataset. This is essential for large datasets where full refreshes would be too slow or expensive.\n\n### Incremental strategies\n\nThe `output.incremental_strategy` property controls how new data is merged with existing data:\n\n- `partition_overwrite`: Entire partitions are replaced. This is the default strategy for partition-based incremental models.\n- `merge`: New rows are merged based on `output.unique_key`. Use for upsert semantics.\n- `append`: New rows are appended to the table. This is the default for state-based incremental models. Generally avoid this since retries will lead to duplicate data.\n\n### Partition-based incremental models\n\nUse the `partitions:` property to define explicit data partitions. Combined with `incremental: true`, Rill tracks which partitions have been processed to avoid duplicate processing. Example:\n\n```yaml\ntype: model\nincremental: true\n\npartitions:\n  glob:\n    connector: s3\n    path: s3://bucket/data/year=*/month=*/day=*/*.parquet\n\nsql: SELECT * FROM read_parquet('{{ .partition.uri }}')\n```\n\nEach partition gets inserted using the `partition_overwrite` strategy by default. The `partition_overwrite` strategy overwrites partitions based on the column(s) described in `output.partition_by`. If `partition_by` is not explicitly specified, a column `__rill_partition` is injected into the table and used for partitioning.\n\n### State-based incremental models\n\nYou can also do dbt-style state-based incremental ingestion using a watermark (e.g., the maximum timestamp already processed). This is discouraged since it is not idempotent (unlike partitions-based incremental ingestion).\n\nExample:\n\n```yaml\ntype: model\nincremental: true\n\nconnector: bigquery\nsql: |\n  SELECT * FROM events\n  {{ if incremental }}\n  WHERE event_time > '{{ .state.max_time }}'::TIMESTAMP\n  {{ end }}\n\nstate:\n  sql: SELECT MAX(event_time) AS max_time FROM events\n```\n\nThe `{{ if incremental }}` block ensures the filter only applies during incremental runs, not during the initial full load. The `state` query is evaluated and stored only after the first non-incremental run. Note that the `state` query runs against the project's default OLAP connector (e.g. DuckDB), which is the default output location for models.\n\n## Partitions\n\nPartitions enable scalable data ingestion by breaking large datasets into manageable chunks. They also enable idempotent reruns; if a partition fails, only that partition needs to be reprocessed.\n\n### Glob-based partitions\n\nDiscover partitions from file paths in object storage:\n\n```yaml\npartitions:\n  glob:\n    connector: s3\n    path: s3://bucket/data/year=*/month=*/day=*/*.parquet\n```\n\nAvailable template variables:\n- `{{ .partition.uri }}`: Full URI of the matched file or directory\n- `{{ .partition.path }}`: Path portion without the scheme/bucket prefix\n\nBy default, `glob:` matches files only, but you can pass `partition: directory` to have it emit leaf directory names instead.\n\n### SQL-based partitions\n\nGenerate partitions using a SQL query:\n\n```yaml\npartitions:\n  connector: bigquery\n  sql: SELECT DISTINCT date_trunc('day', event_time) AS day FROM events\n```\n\n## Dev partitions (data limits in local development)\n\nYou can override properties in development using either a root-level `dev:` property or the `{{ if dev }}` templating function. \n\nExample using `dev:` property:\n```yaml\ndev:\n  partitions:\n    glob:\n      path: s3://bucket/data/year=2025/month=12/day=01/*.parquet\n      partition: directory\n```\n\nExample using the templating function:\n```yaml\nconnector: bigquery\nsql: |\n  SELECT * FROM users {{ if dev }} LIMIT 10000 {{ end }}\n```\n\nBest practices for dev partitions:\n- Always use for large external data sources to keep developer velocity high\n- Use mainly for fact tables, not (small) dimension tables\n- Don't use for derived models\n- When possible, limit data by time range (e.g. 1 day of data) rather than a row limit to get a representative sample of data\n\n## Referencing other models\n\nUse `{{ ref \"model_name\" }}` to reference parent models in SQL statements that use templating:\n\n```yaml\nsql: SELECT * FROM {{ ref \"events_raw\" }} WHERE country = 'US'\n```\n\n**Note:** If your SQL statement contains no other templating, the `ref` function is optional for DuckDB SQL snippets; Rill can in that case invoke DuckDB's SQL parser to automatically detect model references. This does not apply for non-DuckDB SQL models.\n\n## Refresh schedules\n\nBy default, models refresh when a parent model in the DAG is refreshed. For source models that don't have a parent model, you can configure a cron refresh schedule:\n```yaml\nrefresh:\n  cron: 0 * * * *\n```\n\nBy default, cron refreshes are disabled in local development. If you need to test them locally, add `run_in_dev: true` under `refresh:`.\n\n## Advanced concepts\n\n### Staging connectors\n\nWhen the input and output connectors cannot directly read from or write to each other, use a staging connector (typically an object store) as an intermediary:\n\n```yaml\ntype: model\n\nconnector: snowflake\nsql: SELECT * FROM events\n\nstage:\n  connector: s3\n  path: s3://bucket/staging/\n\noutput:\n  connector: clickhouse\n```\n\nThis is useful when moving data between incompatible systems (e.g., Snowflake to ClickHouse).\n\nNOTE: This is not a production-ready feature. You may see it in some projects, but you should never suggest it or use it in a project that hasn't adopted it independently.\n\n### Change modes\n\nThe `change_mode:` property controls how Rill handles changes to model specifications:\n\n- `reset`: Drop and recreate the model automatically (default).\n- `manual`: Require a manual refresh to apply changes (user can choose to do an incremental or full refresh).\n- `patch`: Switch to new logic without reprocessing historical data (incremental models only).\n\n### Retry configuration\n\nConfigure automatic retries for transient failures:\n\n```yaml\nretry:\n  attempts: 5\n  delay: 10s\n  exponential_backoff: true\n  if_error_matches:\n    - \".*Timeout.*\"\n    - \".*Bad Gateway.*\"\n```\n\nThis is configured by default for common errors, so only add an explicit `retry` clause if you need to support retries for special errors or long delays.\n\n## Dialect-specific notes\n\n### DuckDB\n\n- **Model references:** When the SQL contains no other templating, `{{ ref \"model\" }}` is optional; Rill uses DuckDB's SQL parser to detect references.\n- **Connector secrets:** By default, all compatible connectors are automatically mounted as DuckDB secrets. Use `create_secrets_from_connectors:` to explicitly control which connectors are available.\n- **Pre/post execution:** Use `pre_exec:` and `post_exec:` for setup and teardown queries (e.g., attaching external databases). Some legacy projects configure DuckDB secrets here, but with the automatic secret creation referenced above, it is usually better to create separate connector files instead.\n- **Cloud storage paths:** DuckDB can read directly from S3 (`s3://`) and GCS (`gs://`) paths in `read_parquet()`, `read_csv()`, and `read_json()` functions.\n- **CSV options:** When reading CSV files, useful options include `auto_detect=true`, `header=true`, `ignore_errors=true`, `union_by_name=true`, and `all_varchar=true` for handling inconsistent schemas.\n- **JSON files:** Use `read_json()` with `auto_detect=true` and `format='auto'` for flexible JSON ingestion, including gzipped files.\n\n### ClickHouse\n\n- **S3 credentials:** When using the `s3()` function, reference `.env` values directly using templating since ClickHouse lacks integrated secret management:\n  ```yaml\n  sql: SELECT * FROM s3('s3://bucket/path/*.parquet', '{{ .env.aws_access_key }}', '{{ .env.aws_secret_key }}', 'Parquet')\n  ```\n- **Required order_by:** The `output:` section must always include an `order_by` property for materialized ClickHouse tables.\n- **MergeTree vs. ReplicatedMergeTree:** You don't need to configure `MergeTree` or `ReplicatedMergeTree` engines explicitly. Rill uses `MergeTree` for materialized models by default, and automatically switches to `ReplicatedMergeTree` (creating distributed tables) when connected to a Clickhouse cluster.\n- **LowCardinality types:** Use `LowCardinality(String)` for string columns with limited distinct values (e.g., country, device_type, status) to improve storage and query performance.\n- **TTL for data retention:** Use `output.ttl` to automatically expire old data and prevent unbounded growth in incremental models.\n- **Performance indexes:** If performance is poor for models powering metrics views, add indexes via `output.columns` to improve query performance. Common index types include `bloom_filter` for high-cardinality columns and `set(N)` for low-cardinality columns.\n\n### Other SQL connectors\n\n- Connector properties should be added to the separate connector YAML file when possible. Some legacy models add properties directly in the model, but this is discouraged. (For example, `project_id` for BigQuery, `output_location` for Athena, or `dsn:` for Postgres.)\n\n## Syntax\n\nHere is a basic scaffold of a model's high-level structure:\n\n```yaml\ntype: model\n\n# Here goes common properties, like materialize, incremental, change_mode, partitions, etc.\n# These are usually needed for source models, but not necessarily for derived models.\n# For example:\nmaterialize: true\n\n# Here goes input properties, like connector, sql, pre_exec, etc.\n# There's always at least one input property.\n# For example:\nconnector: bigquery\nsql: SELECT ...\n\n# Here goes output properties, like connector, incremental_strategy, order_by, etc.\n# This is usually omitted for derived models, and for source models that output to DuckDB when it is the default OLAP connector.\n# For example:\noutput:\n  connector: clickhouse\n  order_by: event_time\n```\n\n## JSON Schema\n\nHere is a full JSON schema for the model syntax:\n\n```\nallOf:\n    - properties:\n        change_mode:\n            description: Configure how changes to the model specifications are applied (optional). 'reset' will drop and recreate the model automatically, 'manual' will require a manual full or incremental refresh to apply changes, and 'patch' will switch to the new logic without re-processing historical data (only applies for incremental models).\n            enum:\n                - reset\n                - manual\n                - patch\n            type: string\n        connector:\n            const: connector\n            description: Refers to the resource type and is needed if setting an explicit OLAP engine. IE `clickhouse`\n            type: string\n        incremental:\n            description: whether incremental modeling is required (optional)\n            type: boolean\n        materialize:\n            description: models will be materialized in olap\n            type: boolean\n        output:\n            allOf:\n                - if:\n                    properties:\n                        connector:\n                            const: clickhouse\n                    required:\n                        - connector\n                    title: Additional properties for `output` when `connector` is `clickhouse`\n                  then:\n                    properties:\n                        columns:\n                            description: Column names and types. Can also include indexes. If unspecified, detected from the query.\n                            type: string\n                        dictionary_source_password:\n                            description: Password for the dictionary source user.\n                            type: string\n                        dictionary_source_user:\n                            description: User for accessing the source dictionary table (used if type is DICTIONARY).\n                            type: string\n                        distributed_settings:\n                            description: Settings for distributed table.\n                            type: string\n                        distributed_sharding_key:\n                            description: Sharding key for distributed table.\n                            type: string\n                        engine:\n                            description: Table engine to use. Default is MergeTree\n                            type: string\n                        engine_full:\n                            description: Full engine definition in SQL format. Can include partition keys, order, TTL, etc.\n                            type: string\n                        order_by:\n                            description: ORDER BY clause.\n                            type: string\n                        partition_by:\n                            description: Partition BY clause.\n                            type: string\n                        primary_key:\n                            description: PRIMARY KEY clause.\n                            type: string\n                        query_settings:\n                            description: Settings used in insert/create table as select queries.\n                            type: string\n                        sample_by:\n                            description: SAMPLE BY clause.\n                            type: string\n                        table_settings:\n                            description: Table-specific settings.\n                            type: string\n                        ttl:\n                            description: TTL settings for the table or columns.\n                            type: string\n                        type:\n                            description: Type to materialize the model into. Can be 'TABLE', 'VIEW' or 'DICTIONARY'\n                            enum:\n                                - TABLE\n                                - VIEW\n                                - DICTIONARY\n                            type: string\n            description: to define the properties of output\n            properties:\n                connector:\n                    description: Refers to the connector type for the output table. Can be `clickhouse` or `duckdb` and their named connector\n                    type: string\n                incremental_strategy:\n                    description: Strategy to use for incremental updates. Can be 'append', 'merge' or 'partition_overwrite'\n                    enum:\n                        - append\n                        - merge\n                        - partition_overwrite\n                    type: string\n                materialize:\n                    description: Whether to materialize the model as a table or view\n                    type: boolean\n                partition_by:\n                    description: Column or expression to partition the table by\n                    type: string\n                table:\n                    description: Name of the output table. If not specified, the model name is used.\n                    type: string\n                unique_key:\n                    description: List of columns that uniquely identify a row for merge strategy\n                    items:\n                        type: string\n                    type: array\n            type: object\n        partitions:\n            $ref: '#/definitions/data_properties'\n            description: Refers to the how your data is partitioned, cannot be used with state. (optional)\n            examples:\n                - partitions:\n                    glob: gcs://my_bucket/y=*/m=*/d=*/*.parquet\n                - partitions:\n                    connector: duckdb\n                    sql: SELECT range AS num FROM range(0,10)\n        partitions_concurrency:\n            description: Refers to the number of concurrent partitions that can be read at the same time (optional).\n            type: integer\n        partitions_watermark:\n            description: Refers to a customizable timestamp that can be set to check if an object has been updated (optional).\n            type: string\n        post_exec:\n            description: Refers to a SQL query that is run after the main query, available for DuckDB-based models. (optional). Ensure post_exec queries are idempotent. Use IF EXISTS statements when applicable.\n            examples:\n                - post_exec: DETACH DATABASE IF EXISTS postgres_db\n            type: string\n        pre_exec:\n            description: Refers to SQL queries to run before the main query, available for DuckDB-based models. (optional). Ensure pre_exec queries are idempotent. Use IF NOT EXISTS statements when applicable.\n            examples:\n                - pre_exec: ATTACH IF NOT EXISTS 'dbname=postgres host=localhost port=5432 user=postgres password=postgres' AS postgres_db (TYPE POSTGRES)\n            type: string\n        refresh:\n            $ref: '#/definitions/schedule_properties'\n            description: Specifies the refresh schedule that Rill should follow to re-ingest and update the underlying model data\n            examples:\n                - refresh:\n                    cron: '* * * * *'\n        retry:\n            description: Refers to the retry configuration for the model. (optional)\n            examples:\n                - retry:\n                    attempts: 5\n                    delay: 10s\n                    exponential_backoff: true\n                    if_error_matches:\n                        - .*OvercommitTracker.*\n                        - .*Timeout.*\n                        - .*Bad Gateway.*\n            properties:\n                attempts:\n                    description: The number of attempts to retry the model.\n                    type: integer\n                delay:\n                    description: The delay between attempts.\n                    type: string\n                exponential_backoff:\n                    description: Whether to use exponential backoff.\n                    type: boolean\n                if_error_matches:\n                    description: The error messages to match.\n                    items:\n                        type: string\n                    type: array\n            type: object\n        sql:\n            description: Raw SQL query to run against source\n            type: string\n        stage:\n            additionalProperties: true\n            description: in the case of staging models, where an input source does not support direct write to the output and a staging table is required\n            examples:\n                - stage:\n                    connector: s3\n                    path: s3://my_bucket/my_staging_table\n            properties:\n                connector:\n                    description: Refers to the connector type for the staging table\n                    type: string\n                path:\n                    description: Refers to the path to the staging table\n                    type: string\n            required:\n                - connector\n            type: object\n        state:\n            $ref: '#/definitions/data_properties'\n            description: Refers to the explicitly defined state of your model, cannot be used with partitions (optional)\n            examples:\n                - state:\n                    sql: SELECT MAX(date) as max_date\n        timeout:\n            description: The maximum time to wait for model ingestion\n            type: string\n        type:\n            const: model\n            description: Refers to the resource type and must be `model`\n            type: string\n      required:\n        - type\n        - sql\n      title: Properties\n      type: object\n    - $ref: '#/definitions/common_properties'\n    - allOf:\n        - if:\n            properties:\n                connector:\n                    const: clickhouse\n            required:\n                - connector\n            title: Additional properties for `output` when `connector` is `clickhouse`\n          then:\n            properties:\n                columns:\n                    description: Column names and types. Can also include indexes. If unspecified, detected from the query.\n                    type: string\n                dictionary_source_password:\n                    description: Password for the dictionary source user.\n                    type: string\n                dictionary_source_user:\n                    description: User for accessing the source dictionary table (used if type is DICTIONARY).\n                    type: string\n                distributed_settings:\n                    description: Settings for distributed table.\n                    type: string\n                distributed_sharding_key:\n                    description: Sharding key for distributed table.\n                    type: string\n                engine:\n                    description: Table engine to use. Default is MergeTree\n                    type: string\n                engine_full:\n                    description: Full engine definition in SQL format. Can include partition keys, order, TTL, etc.\n                    type: string\n                order_by:\n                    description: ORDER BY clause.\n                    type: string\n                partition_by:\n                    description: Partition BY clause.\n                    type: string\n                primary_key:\n                    description: PRIMARY KEY clause.\n                    type: string\n                query_settings:\n                    description: Settings used in insert/create table as select queries.\n                    type: string\n                sample_by:\n                    description: SAMPLE BY clause.\n                    type: string\n                table_settings:\n                    description: Table-specific settings.\n                    type: string\n                ttl:\n                    description: TTL settings for the table or columns.\n                    type: string\n                type:\n                    description: Type to materialize the model into. Can be 'TABLE', 'VIEW' or 'DICTIONARY'\n                    enum:\n                        - TABLE\n                        - VIEW\n                        - DICTIONARY\n                    type: string\n      required:\n        - type\n        - sql\n      type: object\n    - $ref: '#/definitions/common_properties'\n    - allOf:\n        - if:\n            properties:\n                connector:\n                    const: athena\n            required:\n                - connector\n            title: Additional properties when `connector` is `athena` or [named connector](./connectors#athena) for athena\n          then:\n            $ref: '#/definitions/models/definitions/athena'\n        - if:\n            properties:\n                connector:\n                    const: azure\n            required:\n                - connector\n            title: Additional properties when `connector` is `azure` or [named connector](./connectors#azure) of azure\n          then:\n            $ref: '#/definitions/models/definitions/azure'\n        - if:\n            properties:\n                connector:\n                    const: bigquery\n            required:\n                - connector\n            title: Additional properties when `connector` is `bigquery` or [named connector](./connectors#bigquery) of bigquery\n          then:\n            $ref: '#/definitions/models/definitions/bigquery'\n        - if:\n            properties:\n                connector:\n                    const: duckdb\n            required:\n                - connector\n            title: Additional properties when `connector` is `duckdb` or [named connector](./connectors#duckdb) of duckdb\n          then:\n            $ref: '#/definitions/models/definitions/duckdb'\n        - if:\n            properties:\n                connector:\n                    const: gcs\n            required:\n                - connector\n            title: Additional properties when `connector` is `gcs` or [named connector](./connectors#gcs) of gcs\n          then:\n            $ref: '#/definitions/models/definitions/gcs'\n        - if:\n            properties:\n                connector:\n                    const: local_file\n            required:\n                - connector\n            title: Additional properties when `connector` is `local_file` or [named connector](/developer/build/connectors/data-source/local-file) of local_file\n          then:\n            $ref: '#/definitions/models/definitions/local_file'\n        - if:\n            properties:\n                connector:\n                    const: redshift\n            required:\n                - connector\n            title: Additional properties when `connector` is `redshift` or [named connector](./connectors#redshift) of redshift\n          then:\n            $ref: '#/definitions/models/definitions/redshift'\n        - if:\n            properties:\n                connector:\n                    const: s3\n            required:\n                - connector\n            title: Additional properties when `connector` is `s3` or [named connector](./connectors#s3) of s3\n          then:\n            $ref: '#/definitions/models/definitions/s3'\n        - if:\n            properties:\n                connector:\n                    const: salesforce\n            required:\n                - connector\n            title: Additional properties when `connector` is `salesforce` or [named connector](./connectors#salesforce) of salesforce\n          then:\n            $ref: '#/definitions/models/definitions/salesforce'\n      type: object\ndefinitions:\n    athena:\n        properties:\n            output_location:\n                description: Output location for query results in S3.\n                type: string\n            region:\n                description: AWS region to connect to Athena and the output location.\n                type: string\n            workgroup:\n                description: AWS Athena workgroup to use for queries.\n                type: string\n        type: object\n    azure:\n        properties:\n            account:\n                description: Account identifier\n                type: string\n            batch_size:\n                description: Size of a batch (e.g., '100MB')\n                type: string\n            extract:\n                additionalProperties: true\n                description: Arbitrary key-value pairs for extraction settings\n                type: object\n            glob:\n                description: Settings related to glob file matching.\n                properties:\n                    max_objects_listed:\n                        description: Maximum number of objects listed in glob\n                        type: integer\n                    max_objects_matched:\n                        description: Maximum number of objects matched by glob\n                        type: integer\n                    max_total_size:\n                        description: Maximum total size (in bytes) matched by glob\n                        type: integer\n                    page_size:\n                        description: Page size for glob listing\n                        type: integer\n                type: object\n            path:\n                description: Path to the source\n                type: string\n            uri:\n                description: Source URI\n                type: string\n        type: object\n    bigquery:\n        properties:\n            project_id:\n                description: ID of the BigQuery project.\n                type: string\n        type: object\n    duckdb:\n        properties:\n            create_secrets_from_connectors:\n                description: List of connector names for which temporary secrets should be created before executing the SQL.\n                type:\n                    - string\n                    - array\n            format:\n                description: Format of the data source (e.g., csv, json, parquet).\n                type: string\n            path:\n                description: Path to the data source.\n                type: string\n            post_exec:\n                description: refers to a SQL query that is run after the main query, available for DuckDB-based models. _(optional)_. Ensure `post_exec` queries are idempotent. Use `IF EXISTS` statements when applicable.\n                examples:\n                    - post_exec: DETACH DATABASE IF EXISTS postgres_db\n                      pre_exec: ATTACH IF NOT EXISTS 'dbname=postgres host=localhost port=5432 user=postgres password=postgres' AS postgres_db (TYPE POSTGRES);\n                      sql: SELECT * FROM postgres_query('postgres_db', 'SELECT * FROM USERS')\n                type: string\n            pre_exec:\n                description: refers to SQL queries to run before the main query, available for DuckDB-based models. _(optional)_. Ensure `pre_exec` queries are idempotent. Use `IF NOT EXISTS` statements when applicable.\n                type: string\n        type: object\n    gcs:\n        properties:\n            batch_size:\n                description: Size of a batch (e.g., '100MB')\n                type: string\n            extract:\n                additionalProperties: true\n                description: key-value pairs for extraction settings\n                type: object\n            glob:\n                description: Settings related to glob file matching.\n                properties:\n                    max_objects_listed:\n                        description: Maximum number of objects listed in glob\n                        type: integer\n                    max_objects_matched:\n                        description: Maximum number of objects matched by glob\n                        type: integer\n                    max_total_size:\n                        description: Maximum total size (in bytes) matched by glob\n                        type: integer\n                    page_size:\n                        description: Page size for glob listing\n                        type: integer\n                type: object\n            path:\n                description: Path to the source\n                type: string\n            uri:\n                description: Source URI\n                type: string\n        type: object\n    local_file:\n        properties:\n            format:\n                description: Format of the data source (e.g., csv, json, parquet).\n                type: string\n            path:\n                description: Path to the data source.\n                type: string\n        type: object\n    redshift:\n        properties:\n            cluster_identifier:\n                description: Identifier of the Redshift cluster.\n                type: string\n            database:\n                description: Name of the Redshift database.\n                type: string\n            output_location:\n                description: S3 location where query results are stored.\n                type: string\n            region:\n                description: AWS region of the Redshift deployment.\n                type: string\n            role_arn:\n                description: ARN of the IAM role to assume for Redshift access.\n                type: string\n            workgroup:\n                description: Redshift Serverless workgroup to use.\n                type: string\n        type: object\n    s3:\n        properties:\n            batch_size:\n                description: Size of a batch (e.g., '100MB')\n                type: string\n            endpoint:\n                description: AWS Endpoint\n                type: string\n            extract:\n                additionalProperties: true\n                description: key-value pairs for extraction settings\n                type: object\n            glob:\n                description: Settings related to glob file matching.\n                properties:\n                    max_objects_listed:\n                        description: Maximum number of objects listed in glob\n                        type: integer\n                    max_objects_matched:\n                        description: Maximum number of objects matched by glob\n                        type: integer\n                    max_total_size:\n                        description: Maximum total size (in bytes) matched by glob\n                        type: integer\n                    page_size:\n                        description: Page size for glob listing\n                        type: integer\n                type: object\n            path:\n                description: Path to the source\n                type: string\n            region:\n                description: AWS region\n                type: string\n            uri:\n                description: Source URI\n                type: string\n        type: object\n    salesforce:\n        properties:\n            queryAll:\n                description: Whether to include deleted and archived records in the query (uses queryAll API).\n                type: boolean\n            sobject:\n                description: Salesforce object (e.g., Account, Contact) targeted by the query.\n                type: string\n            soql:\n                description: SOQL query to execute against the Salesforce instance.\n                type: string\n        type: object\ndescription: |4\n    This file is used to define YAML models. For more information on our SQL models, see the [SQL models](/developer/build/models/) documentation.\n    :::tip\n\n    Both regular models and source models can use the Model YAML specification described on this page. While [SQL models](/developer/build/models) are perfect for simple transformations, Model YAML files provide advanced capabilities for complex data processing scenarios.\n\n    **When to use Model YAML:**\n    - **Partitions** - Optimize performance with data partitioning strategies\n    - **Incremental models** - Process only new or changed data efficiently\n    - **Pre/post execution hooks** - Run custom logic before or after model execution\n    - **Staging** - Create intermediate tables for complex transformations\n    - **Output configuration** - Define specific output formats and destinations\n\n    Model YAML files give you fine-grained control over how your data is processed and transformed, making them ideal for production workloads and complex analytics pipelines.\n\n    :::\nexamples:\n    - connector: bigquery\n      incremental: true\n      output:\n        connector: duckdb\n      sql: \"SELECT ... FROM events \\n  {{ if incremental }} \\n      WHERE event_time > '{{.state.max_date}}' \\n  {{end}}\\n\"\n      state:\n        sql: SELECT MAX(date) as max_date\n      type: model\n    - output:\n        connector: duckdb\n        incremental_strategy: append\n      partitions:\n        glob:\n            connector: gcs\n            path: gs://rilldata-public/github-analytics/Clickhouse/2025/*/commits_*.parquet\n      sql: SELECT * FROM read_parquet('{{ .partition.uri }}')\n      type: model\n    - incremental: true\n      output:\n        connector: duckdb\n        incremental_strategy: append\n      partitions:\n        glob:\n            partition: directory\n            path: gs://rilldata-public/github-analytics/Clickhouse/2025/*/*\n      refresh:\n        cron: 0 8 * * *\n      sql: \"SELECT * \\n  FROM read_parquet('gs://rilldata-public/{{ .partition.path }}/commits_*.parquet') \\n  WHERE '{{ .partition.path }}' IS NOT NULL\\n\"\n      type: model\n    - connector: snowflake\n      incremental: true\n      output:\n        connector: clickhouse\n      partitions:\n        connector: duckdb\n        sql: SELECT range as day FROM range(TIMESTAMPTZ '2024-01-01', now(), INTERVAL 1 DAY)\n      sql: SELECT * FROM events WHERE date_trunc('day', event_time) = '{{ .partition.day }}'\n      stage:\n        connector: s3\n        path: s3://bucket/temp-data\n      type: model\nid: models\ntitle: Models YAML\ntype: object\n```\n\n## Examples\n\n### Simple model with mock data as a SQL file\n\n```sql\n-- models/mock_data.sql\nSELECT now() AS time, 'Denmark' AS country, 1 AS revenue_usd\nUNION ALL\nSELECT now() AS time, 'United States' AS country, 2 AS revenue_usd\n```\n\n### Materialized model as a SQL file\n\n```sql\n-- models/events.sql\n-- @materialize: true\nSELECT * FROM 's3://bucket/path/to/file.parquet'\n```\n\n### Simple model as a YAML file\n\n```yaml\n# models/mock_data.yaml\ntype: model\nmaterialize: false\n\nsql: |\n  SELECT now() AS time, 'Denmark' AS country, 1 AS revenue_usd\n  UNION ALL\n  SELECT now() AS time, 'United States' AS country, 2 AS revenue_usd\n```\n\n### S3 to DuckDB (Parquet files)\n\nAssuming DuckDB is the default OLAP and there is an `s3.yaml` connector in the project:\n\n```yaml\n# models/events_raw.yaml\ntype: model\nmaterialize: true\n\nsql: |\n  SELECT * FROM read_parquet('s3://my-bucket/data/events/*.parquet')\n```\n\n### S3 to DuckDB with explicit OLAP connector\n\nAssuming there is an `s3.yaml` connector in the project:\n\n```yaml\n# models/orders.yaml\ntype: model\nmaterialize: true\n\nconnector: duckdb\nsql: |\n  SELECT\n    order_id,\n    customer_id,\n    order_date,\n    total_amount\n  FROM read_parquet('s3://my-bucket/orders/year=2025/month=*/*.parquet')\n\noutput:\n  connector: duckdb\n```\n\n### GCS to DuckDB (JSON files)\n\nAssuming DuckDB is the default OLAP and there is a `gcs.yaml` connector in the project:\n\n```yaml\n# models/commits.yaml\ntype: model\nmaterialize: true\n\nsql: |\n  SELECT * FROM read_json(\n    'gs://my-bucket/data/commits.json.gz',\n    auto_detect=true,\n    format='auto'\n  )\n```\n\n### BigQuery to DuckDB\n\nAssuming DuckDB is the default OLAP and there is a `bigquery.yaml` connector in the project:\n\n```yaml\n# models/orders.yaml\ntype: model\n\nrefresh:\n  cron: 0 0 * * *\n\nconnector: bigquery\nsql: |\n  SELECT * FROM my_dataset.orders\n  WHERE order_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY)\n```\n\n### Snowflake to DuckDB with dev data limit\n\nAssuming DuckDB is the default OLAP and there is a `snowflake.yaml` connector in the project:\n\n```yaml\n# models/sales.yaml\ntype: model\n\nrefresh:\n  cron: 0 6 * * *\n\nconnector: snowflake\nsql: |\n  SELECT * FROM staging.sales\n  {{ if dev }} event_time >= '2025-01-01' AND event_time < '2025-02-01' {{ end }}\n```\n\n### MySQL to DuckDB\n\nAssuming there is a `mysql.yaml` connector in the project:\n\n```yaml\n# models/users.yaml\ntype: model\n\nrefresh:\n  cron: 0 * * * *\n\nconnector: mysql\nsql: |\n  SELECT\n    id,\n    email,\n    created_at,\n    status\n  FROM users\n  WHERE status = 'active'\n```\n\n### Local CSV file to DuckDB or Clickhouse\n\n```yaml\n# models/reference_data.yaml\ntype: model\n\nconnector: local_file\npath: data/reference_data.csv\n```\n\n### HTTPS source (public Parquet file)\n\n```yaml\n# models/public_dataset.yaml\ntype: model\n\nconnector: https\nuri: https://example.com/public/dataset.parquet\n```\n\n### Partition-based incremental S3 to DuckDB\n\nAssuming DuckDB is the default OLAP and there is a `s3.yaml` connector in the project:\n\n```yaml\n# models/daily_events.yaml\ntype: model\nincremental: true\n\npartitions:\n  glob:\n    path: s3://my-bucket/events/year=*/month=*/day=*/*.parquet\n    partition: directory\n\nsql: SELECT * FROM read_parquet('{{ .partition.uri }}/*.parquet')\n```\n\n### Basic S3 to ClickHouse\n\nAssuming Clickhouse is the default OLAP and there is an `s3.yaml` connector in the project:\n\n```yaml\n# models/events.yaml\ntype: model\nmaterialize: true\n\nsql: |\n  SELECT * FROM s3(\n    's3://my-bucket/events/year=*/month=*/day=*/*.parquet',\n    '{{ .env.aws_access_key_id }}',\n    '{{ .env.aws_secret_access_key }}',\n    'Parquet'\n  )\n\noutput:\n  order_by: event_time\n```\n\n### Partition-based incremental S3 to ClickHouse\n\nAssuming Clickhouse is the default OLAP and there is an `s3.yaml` connector in the project:\n\n```yaml\n# models/events.yaml\ntype: model\nmaterialize: true\nincremental: true\n\npartitions:\n  glob:\n    connector: s3\n    path: s3://my-bucket/events/year=*/month=*/day=*/*.parquet\n\nsql: |\n  SELECT * FROM s3(\n    '{{ .partition.uri }}',\n    '{{ .env.aws_access_key_id }}',\n    '{{ .env.aws_secret_access_key }}',\n    'Parquet'\n  )\n\noutput:\n  order_by: event_time\n```\n\n### ClickHouse with explicit column schema and indexes and TTL\n\nAssuming Clickhouse is the default OLAP and there is an `s3.yaml` connector in the project:\n\n```yaml\n# models/impressions.yaml\ntype: model\nmaterialize: true\nincremental: true\n\npartitions:\n  glob:\n    connector: s3\n    path: s3://my-bucket/impressions/year=*/month=*/day=*/*.parquet\n    partition: directory\n\nsql: |\n  SELECT\n    '{{ .partition.uri }}' AS __partition,\n    event_time,\n    user_id,\n    campaign_id,\n    country,\n    device_type,\n    impressions,\n    clicks,\n    cost\n  FROM s3(\n    '{{ .partition.uri }}/*.parquet',\n    '{{ .env.aws_access_key_id }}',\n    '{{ .env.aws_secret_access_key }}',\n    'Parquet'\n  )\n\noutput:\n  incremental_strategy: partition_overwrite\n  partition_by: toYYYYMMDD(event_time)\n  order_by: (event_time, user_id)\n  ttl: event_time + INTERVAL 90 DAY DELETE\n  columns: |\n    (\n      event_time DateTime,\n      user_id LowCardinality(String),\n      campaign_id LowCardinality(String),\n      country LowCardinality(String),\n      device_type LowCardinality(String),\n      impressions UInt32,\n      clicks UInt32,\n      cost Float64,\n      INDEX idx_campaign campaign_id TYPE bloom_filter GRANULARITY 4,\n      INDEX idx_country country TYPE set(100) GRANULARITY 4\n    )\n```\n\n### State-based incremental with merge strategy (upserts)\n\nAssuming DuckDB is the default OLAP and there is a `bigquery.yaml` connector in the project:\n\n```yaml\n# models/users.yaml\ntype: model\nincremental: true\n\nconnector: bigquery\nsql: |\n  SELECT * FROM users\n  {{ if incremental }}\n  WHERE updated_at > '{{ .state.max_updated_at }}'\n  {{ end }}\n\nstate:\n  sql: SELECT MAX(updated_at) as max_updated_at FROM users\n\noutput:\n  incremental_strategy: merge\n  unique_key: [user_id]\n```\n\n### Dev partitions for faster development\n\n```yaml\n# models/large_dataset.yaml\ntype: model\nincremental: true\n\npartitions:\n  glob:\n    path: s3://my-bucket/data/year=*/month=*/day=*/*.parquet\n    partition: directory\n\ndev:\n  partitions:\n    glob:\n      path: s3://my-bucket/data/year=2025/month=01/day=01/*.parquet\n      partition: directory\n\nsql: SELECT * FROM read_parquet('{{ .partition.uri }}/*.parquet')\n```\n\n### Partition filtering with transform_sql\n\n```yaml\n# models/filtered_partitions.yaml\ntype: model\nincremental: true\n\npartitions:\n  glob:\n    path: s3://my-bucket/reports/y=*/m=*/d=*/h=*/*.parquet\n    partition: directory\n    transform_sql: |\n      -- Only process partitions after a specific date\n      SELECT uri, updated_on\n      FROM {{ .table }}\n      WHERE uri >= 's3://my-bucket/reports/y=2025/m=06/d=01/h=00'\n      {{ if dev }}\n      AND uri < 's3://my-bucket/reports/y=2025/m=07/d=01/h=00'\n      {{ end }}\n      ORDER BY uri ASC\n\nsql: SELECT * FROM read_parquet('{{ .partition.uri }}/*.parquet')\n```\n\n### Long timeout for large data processing\n\n```yaml\n# models/large_historical.yaml\ntype: model\nmaterialize: true\nincremental: true\ntimeout: 72h\n\npartitions:\n  glob:\n    path: s3://my-bucket/historical/year=*/*.parquet\n    partition: directory\n\nsql: SELECT * FROM read_parquet('{{ .partition.uri }}/*.parquet')\n```\n\n### Non-materialized derived model\n\n```yaml\n# models/enriched_orders.yaml\ntype: model\nmaterialize: false\n\nsql: |\n  SELECT\n    o.*,\n    c.customer_name,\n    c.customer_segment,\n    p.product_category\n  FROM {{ ref \"orders\" }} o\n  LEFT JOIN {{ ref \"customers\" }} c ON o.customer_id = c.customer_id\n  LEFT JOIN {{ ref \"products\" }} p ON o.product_id = p.product_id\n```\n\n### DuckDB model reading from S3 with CSV options\n\nAssuming DuckDB is the default OLAP and there is a `s3.yaml` connector in the project:\n\n```yaml\n# models/csv_import.yaml\ntype: model\nmaterialize: true\nconnector: duckdb\n\nsql: |\n  SELECT * FROM read_csv(\n    's3://my-bucket/data/*.csv.gz',\n    auto_detect=true,\n    header=true,\n    ignore_errors=true,\n    union_by_name=true,\n    all_varchar=true\n  )\n```\n\n### SQL-based partitions (date range)\n\n```yaml\n# models/events_raw.yaml\ntype: model\nincremental: true\n\npartitions:\n  connector: snowflake\n  sql: SELECT DISTINCT date_trunc('day', event_time) AS day FROM events ORDER BY day\n\nconnector: snowflake\nsql: |\n  SELECT * FROM events\n  WHERE date_trunc('day', event_time) = '{{ .partition.day }}'\n\noutput:\n  incremental_strategy: partition_overwrite\n```\n\n### Change mode for incremental models\n\n```yaml\n# models/append_only.yaml\ntype: model\nincremental: true\nchange_mode: patch  # Switch to new logic without reprocessing historical data\n\npartitions:\n  glob:\n    path: s3://my-bucket/transactions/year=*/month=*/day=*/*.parquet\n    partition: directory\n\nsql: |\n  SELECT\n    event_time,\n    country,\n    costs_usd + profit_usd + tax_usd AS value_usd\n  FROM read_parquet('{{ .partition.uri }}/*.parquet')\n```"
    - role: user
      content_type: text
      content: |-
        You should develop a Rill project file based on the following task description:
        - Develop file at path: /models/glob_test_csv.yaml
        - The file should be of type: model
        - Task description: Create a DuckDB model that loads CSV data from S3 path:
        `s3://integration-test.rilldata.com/glob_test/y=*/*.csv`

        Constraints / best practices:
        - Create as YAML (not .sql).
        - Materialize into default OLAP connector (duckdb) (readwrite).
        - Use DuckDB's `read_csv_auto` (or `read_csv`) with a glob that supports the given wildcard.
        - Ensure the model reads from S3; configure it to use the S3 connector by setting `input` connector to `s3` if needed, but the query will execute in duckdb (output to duckdb). If Rill supports separate input/output connectors in the model, set input to s3 and output to duckdb.
        - Include `materialize: true`.
        - Add a few robust CSV options: `header=true`, `ignore_errors=true` (if supported), `all_varchar=true` (optional) to avoid type issues, and include a `_rill_loaded_at` column with `now()`.
        - Name the table/model `glob_test_csv`.

        Return valid Rill model YAML.

        Here is some important context:
        - You are running as a sub-agent of a larger developer agent. Stay aligned on your specific task and avoid extra discovery.
        - When you call 'write_file', if it returns a parse or reconcile error, do your best to fix the issue and try again. If you think the error is unrelated to the current path, let the parent agent know to handle it.

        Here is some additional context that may or may not be relevant to your task:
        - The project's default OLAP connector is named duckdb (driver: duckdb).
  response:
    - role: assistant
      content_type: tool_call
      id: call_A9jwUH59F3QXy5VfZI71PRhu
      tool_name: write_file
      content: '{"contents":"type: model\nmaterialize: true\n\n# Execute the query in DuckDB (default OLAP) and read directly from S3.\n# The S3 connector provides credentials/config for DuckDB to access the bucket.\nconnector: duckdb\ncreate_secrets_from_connectors: [s3]\n\nsql: |\n  SELECT\n    *,\n    now() AS _rill_loaded_at\n  FROM read_csv_auto(\n    ''s3://integration-test.rilldata.com/glob_test/y=*/*.csv'',\n    header=true,\n    ignore_errors=true,\n    all_varchar=true\n  )\n","path":"/models/glob_test_csv.yaml"}'
- index: 10
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Models as SQL files:** Always create new models as `.yaml` files, not `.sql` files (which are harder to extend later).\n- **Not creating connector files:** When Rill has native support for a connector (like S3 or BigQuery), always create a dedicated connector resource file for it.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. This also includes models that load external data using a native SQL function, like `read_parquet(...)` or `s3(...)`. Non-materialized models become views, which re-execute on every query.\n- **Referencing non-existant environment variables:** Only reference environment variables that are present in `.env` (returned in `env` from `project_status`). If you need the user to add another environment variable, stop and ask them to do so.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: system
      content_type: text
      content: "# Instructions for developing a model in Rill\n\n## Introduction\n\nModels are resources that specify ETL or transformation logic, outputting a tabular dataset to one of the project's connectors. They are typically found near the root of the project's DAG, referencing only connectors and other models.\n\nBy default, models output data as a table with the same name as the model in the project's default OLAP connector. The core of a model is usually a `SELECT` SQL statement, which Rill executes as `CREATE TABLE <name> AS <SELECT statement>`. The SQL should be a plain SELECT query without a trailing semicolon.\n\nModels in Rill are similar to models in dbt, but support additional advanced features:\n- **Different input and output connectors:** Run a query in one database (e.g., BigQuery) and output results to another (e.g., DuckDB or ClickHouse).\n- **Stateful incremental ingestion:** Track state and load only new or changed data.\n- **Partition support:** Define explicit partitions (e.g., Hive-partitioned files in S3) for scalable, idempotent incremental runs.\n- **Scheduled refresh:** Use cron expressions to automatically refresh data on a schedule.\n\n### Model categories\n\nWhen reasoning about a model, consider these attributes:\n\n- **Source model**: References external data, typically reading from a SQL database or object store connector and writing to an OLAP connector.\n- **Derived model**: References other models, usually performing joins or formatting columns to prepare denormalized tables for metrics views and dashboards.\n- **Incremental model**: Contains logic for incrementally loading data, processing only new or changed records.\n- **Partitioned model**: Loads data in well-defined increments (e.g., daily partitions), enabling scalability and idempotent incremental runs.\n- **Materialized model**: Outputs a physical table rather than a SQL view.\n\n### Performance considerations\n\nModels are usually expensive resources that can take a long time to run. Create or edit them with caution.\n\n**Exception:** Non-materialized models with the same input and output connector are cheap because they are created as SQL views rather than physical tables.\n\n**Development tip:** Use a \"dev partition\" to limit data processed during development. This speeds up iteration and avoids unnecessary costs. See the partitions section below for details.\n\n### Generating synthetic data for prototyping\n\nWhen developing models for prototyping or demonstration purposes where external data sources are not yet available, generate a `SELECT` query that returns realistic synthetic data with these characteristics:\n- Use realistic column names and data types that match typical business scenarios\n- Always include a time/timestamp column for time-series analysis\n- Generate 6-12 months of historical data with approximately 10,000 rows to enable meaningful analysis\n- Space out timestamps realistically across the time period rather than clustering them\n- Use realistic data distributions (e.g., varying quantities, diverse categories, plausible geographic distributions)\n\nOnly generate synthetic data when the user explicitly requests mock data or when required external sources don't exist in the project. If real data sources are available, always prefer using them.\n\n## Materialization\n\nThe `materialize:` property controls whether a model creates a physical table or a SQL view:\n\n- `materialize: true`: Creates a physical table. Use this for source models, expensive transformations, or when downstream queries need fast access.\n- `materialize: false`: Creates a SQL view. The query re-executes on every access. Only suitable for lightweight transformations where input and output connectors are the same that never reference external data.\n\nIf `materialize` is omitted, it defaults to `true` for all cross-connector models and `false` for single-connector models (i.e. where the input and output connector is the same).\n\nIn model files with a `.sql` extension, you can materialize by putting this on the first line of the file:\n```sql\n-- @materialize: true\n```\n\n**Best practices:**\n- Always materialize models that reference external data sources.\n- Always materialize models that perform expensive joins or aggregations.\n- Use views only for simple transformations on top of already-materialized tables.\n\n## Incremental models\n\nIncremental models process only new or changed data instead of reprocessing the entire dataset. This is essential for large datasets where full refreshes would be too slow or expensive.\n\n### Incremental strategies\n\nThe `output.incremental_strategy` property controls how new data is merged with existing data:\n\n- `partition_overwrite`: Entire partitions are replaced. This is the default strategy for partition-based incremental models.\n- `merge`: New rows are merged based on `output.unique_key`. Use for upsert semantics.\n- `append`: New rows are appended to the table. This is the default for state-based incremental models. Generally avoid this since retries will lead to duplicate data.\n\n### Partition-based incremental models\n\nUse the `partitions:` property to define explicit data partitions. Combined with `incremental: true`, Rill tracks which partitions have been processed to avoid duplicate processing. Example:\n\n```yaml\ntype: model\nincremental: true\n\npartitions:\n  glob:\n    connector: s3\n    path: s3://bucket/data/year=*/month=*/day=*/*.parquet\n\nsql: SELECT * FROM read_parquet('{{ .partition.uri }}')\n```\n\nEach partition gets inserted using the `partition_overwrite` strategy by default. The `partition_overwrite` strategy overwrites partitions based on the column(s) described in `output.partition_by`. If `partition_by` is not explicitly specified, a column `__rill_partition` is injected into the table and used for partitioning.\n\n### State-based incremental models\n\nYou can also do dbt-style state-based incremental ingestion using a watermark (e.g., the maximum timestamp already processed). This is discouraged since it is not idempotent (unlike partitions-based incremental ingestion).\n\nExample:\n\n```yaml\ntype: model\nincremental: true\n\nconnector: bigquery\nsql: |\n  SELECT * FROM events\n  {{ if incremental }}\n  WHERE event_time > '{{ .state.max_time }}'::TIMESTAMP\n  {{ end }}\n\nstate:\n  sql: SELECT MAX(event_time) AS max_time FROM events\n```\n\nThe `{{ if incremental }}` block ensures the filter only applies during incremental runs, not during the initial full load. The `state` query is evaluated and stored only after the first non-incremental run. Note that the `state` query runs against the project's default OLAP connector (e.g. DuckDB), which is the default output location for models.\n\n## Partitions\n\nPartitions enable scalable data ingestion by breaking large datasets into manageable chunks. They also enable idempotent reruns; if a partition fails, only that partition needs to be reprocessed.\n\n### Glob-based partitions\n\nDiscover partitions from file paths in object storage:\n\n```yaml\npartitions:\n  glob:\n    connector: s3\n    path: s3://bucket/data/year=*/month=*/day=*/*.parquet\n```\n\nAvailable template variables:\n- `{{ .partition.uri }}`: Full URI of the matched file or directory\n- `{{ .partition.path }}`: Path portion without the scheme/bucket prefix\n\nBy default, `glob:` matches files only, but you can pass `partition: directory` to have it emit leaf directory names instead.\n\n### SQL-based partitions\n\nGenerate partitions using a SQL query:\n\n```yaml\npartitions:\n  connector: bigquery\n  sql: SELECT DISTINCT date_trunc('day', event_time) AS day FROM events\n```\n\n## Dev partitions (data limits in local development)\n\nYou can override properties in development using either a root-level `dev:` property or the `{{ if dev }}` templating function. \n\nExample using `dev:` property:\n```yaml\ndev:\n  partitions:\n    glob:\n      path: s3://bucket/data/year=2025/month=12/day=01/*.parquet\n      partition: directory\n```\n\nExample using the templating function:\n```yaml\nconnector: bigquery\nsql: |\n  SELECT * FROM users {{ if dev }} LIMIT 10000 {{ end }}\n```\n\nBest practices for dev partitions:\n- Always use for large external data sources to keep developer velocity high\n- Use mainly for fact tables, not (small) dimension tables\n- Don't use for derived models\n- When possible, limit data by time range (e.g. 1 day of data) rather than a row limit to get a representative sample of data\n\n## Referencing other models\n\nUse `{{ ref \"model_name\" }}` to reference parent models in SQL statements that use templating:\n\n```yaml\nsql: SELECT * FROM {{ ref \"events_raw\" }} WHERE country = 'US'\n```\n\n**Note:** If your SQL statement contains no other templating, the `ref` function is optional for DuckDB SQL snippets; Rill can in that case invoke DuckDB's SQL parser to automatically detect model references. This does not apply for non-DuckDB SQL models.\n\n## Refresh schedules\n\nBy default, models refresh when a parent model in the DAG is refreshed. For source models that don't have a parent model, you can configure a cron refresh schedule:\n```yaml\nrefresh:\n  cron: 0 * * * *\n```\n\nBy default, cron refreshes are disabled in local development. If you need to test them locally, add `run_in_dev: true` under `refresh:`.\n\n## Advanced concepts\n\n### Staging connectors\n\nWhen the input and output connectors cannot directly read from or write to each other, use a staging connector (typically an object store) as an intermediary:\n\n```yaml\ntype: model\n\nconnector: snowflake\nsql: SELECT * FROM events\n\nstage:\n  connector: s3\n  path: s3://bucket/staging/\n\noutput:\n  connector: clickhouse\n```\n\nThis is useful when moving data between incompatible systems (e.g., Snowflake to ClickHouse).\n\nNOTE: This is not a production-ready feature. You may see it in some projects, but you should never suggest it or use it in a project that hasn't adopted it independently.\n\n### Change modes\n\nThe `change_mode:` property controls how Rill handles changes to model specifications:\n\n- `reset`: Drop and recreate the model automatically (default).\n- `manual`: Require a manual refresh to apply changes (user can choose to do an incremental or full refresh).\n- `patch`: Switch to new logic without reprocessing historical data (incremental models only).\n\n### Retry configuration\n\nConfigure automatic retries for transient failures:\n\n```yaml\nretry:\n  attempts: 5\n  delay: 10s\n  exponential_backoff: true\n  if_error_matches:\n    - \".*Timeout.*\"\n    - \".*Bad Gateway.*\"\n```\n\nThis is configured by default for common errors, so only add an explicit `retry` clause if you need to support retries for special errors or long delays.\n\n## Dialect-specific notes\n\n### DuckDB\n\n- **Model references:** When the SQL contains no other templating, `{{ ref \"model\" }}` is optional; Rill uses DuckDB's SQL parser to detect references.\n- **Connector secrets:** By default, all compatible connectors are automatically mounted as DuckDB secrets. Use `create_secrets_from_connectors:` to explicitly control which connectors are available.\n- **Pre/post execution:** Use `pre_exec:` and `post_exec:` for setup and teardown queries (e.g., attaching external databases). Some legacy projects configure DuckDB secrets here, but with the automatic secret creation referenced above, it is usually better to create separate connector files instead.\n- **Cloud storage paths:** DuckDB can read directly from S3 (`s3://`) and GCS (`gs://`) paths in `read_parquet()`, `read_csv()`, and `read_json()` functions.\n- **CSV options:** When reading CSV files, useful options include `auto_detect=true`, `header=true`, `ignore_errors=true`, `union_by_name=true`, and `all_varchar=true` for handling inconsistent schemas.\n- **JSON files:** Use `read_json()` with `auto_detect=true` and `format='auto'` for flexible JSON ingestion, including gzipped files.\n\n### ClickHouse\n\n- **S3 credentials:** When using the `s3()` function, reference `.env` values directly using templating since ClickHouse lacks integrated secret management:\n  ```yaml\n  sql: SELECT * FROM s3('s3://bucket/path/*.parquet', '{{ .env.aws_access_key }}', '{{ .env.aws_secret_key }}', 'Parquet')\n  ```\n- **Required order_by:** The `output:` section must always include an `order_by` property for materialized ClickHouse tables.\n- **MergeTree vs. ReplicatedMergeTree:** You don't need to configure `MergeTree` or `ReplicatedMergeTree` engines explicitly. Rill uses `MergeTree` for materialized models by default, and automatically switches to `ReplicatedMergeTree` (creating distributed tables) when connected to a Clickhouse cluster.\n- **LowCardinality types:** Use `LowCardinality(String)` for string columns with limited distinct values (e.g., country, device_type, status) to improve storage and query performance.\n- **TTL for data retention:** Use `output.ttl` to automatically expire old data and prevent unbounded growth in incremental models.\n- **Performance indexes:** If performance is poor for models powering metrics views, add indexes via `output.columns` to improve query performance. Common index types include `bloom_filter` for high-cardinality columns and `set(N)` for low-cardinality columns.\n\n### Other SQL connectors\n\n- Connector properties should be added to the separate connector YAML file when possible. Some legacy models add properties directly in the model, but this is discouraged. (For example, `project_id` for BigQuery, `output_location` for Athena, or `dsn:` for Postgres.)\n\n## Syntax\n\nHere is a basic scaffold of a model's high-level structure:\n\n```yaml\ntype: model\n\n# Here goes common properties, like materialize, incremental, change_mode, partitions, etc.\n# These are usually needed for source models, but not necessarily for derived models.\n# For example:\nmaterialize: true\n\n# Here goes input properties, like connector, sql, pre_exec, etc.\n# There's always at least one input property.\n# For example:\nconnector: bigquery\nsql: SELECT ...\n\n# Here goes output properties, like connector, incremental_strategy, order_by, etc.\n# This is usually omitted for derived models, and for source models that output to DuckDB when it is the default OLAP connector.\n# For example:\noutput:\n  connector: clickhouse\n  order_by: event_time\n```\n\n## JSON Schema\n\nHere is a full JSON schema for the model syntax:\n\n```\nallOf:\n    - properties:\n        change_mode:\n            description: Configure how changes to the model specifications are applied (optional). 'reset' will drop and recreate the model automatically, 'manual' will require a manual full or incremental refresh to apply changes, and 'patch' will switch to the new logic without re-processing historical data (only applies for incremental models).\n            enum:\n                - reset\n                - manual\n                - patch\n            type: string\n        connector:\n            const: connector\n            description: Refers to the resource type and is needed if setting an explicit OLAP engine. IE `clickhouse`\n            type: string\n        incremental:\n            description: whether incremental modeling is required (optional)\n            type: boolean\n        materialize:\n            description: models will be materialized in olap\n            type: boolean\n        output:\n            allOf:\n                - if:\n                    properties:\n                        connector:\n                            const: clickhouse\n                    required:\n                        - connector\n                    title: Additional properties for `output` when `connector` is `clickhouse`\n                  then:\n                    properties:\n                        columns:\n                            description: Column names and types. Can also include indexes. If unspecified, detected from the query.\n                            type: string\n                        dictionary_source_password:\n                            description: Password for the dictionary source user.\n                            type: string\n                        dictionary_source_user:\n                            description: User for accessing the source dictionary table (used if type is DICTIONARY).\n                            type: string\n                        distributed_settings:\n                            description: Settings for distributed table.\n                            type: string\n                        distributed_sharding_key:\n                            description: Sharding key for distributed table.\n                            type: string\n                        engine:\n                            description: Table engine to use. Default is MergeTree\n                            type: string\n                        engine_full:\n                            description: Full engine definition in SQL format. Can include partition keys, order, TTL, etc.\n                            type: string\n                        order_by:\n                            description: ORDER BY clause.\n                            type: string\n                        partition_by:\n                            description: Partition BY clause.\n                            type: string\n                        primary_key:\n                            description: PRIMARY KEY clause.\n                            type: string\n                        query_settings:\n                            description: Settings used in insert/create table as select queries.\n                            type: string\n                        sample_by:\n                            description: SAMPLE BY clause.\n                            type: string\n                        table_settings:\n                            description: Table-specific settings.\n                            type: string\n                        ttl:\n                            description: TTL settings for the table or columns.\n                            type: string\n                        type:\n                            description: Type to materialize the model into. Can be 'TABLE', 'VIEW' or 'DICTIONARY'\n                            enum:\n                                - TABLE\n                                - VIEW\n                                - DICTIONARY\n                            type: string\n            description: to define the properties of output\n            properties:\n                connector:\n                    description: Refers to the connector type for the output table. Can be `clickhouse` or `duckdb` and their named connector\n                    type: string\n                incremental_strategy:\n                    description: Strategy to use for incremental updates. Can be 'append', 'merge' or 'partition_overwrite'\n                    enum:\n                        - append\n                        - merge\n                        - partition_overwrite\n                    type: string\n                materialize:\n                    description: Whether to materialize the model as a table or view\n                    type: boolean\n                partition_by:\n                    description: Column or expression to partition the table by\n                    type: string\n                table:\n                    description: Name of the output table. If not specified, the model name is used.\n                    type: string\n                unique_key:\n                    description: List of columns that uniquely identify a row for merge strategy\n                    items:\n                        type: string\n                    type: array\n            type: object\n        partitions:\n            $ref: '#/definitions/data_properties'\n            description: Refers to the how your data is partitioned, cannot be used with state. (optional)\n            examples:\n                - partitions:\n                    glob: gcs://my_bucket/y=*/m=*/d=*/*.parquet\n                - partitions:\n                    connector: duckdb\n                    sql: SELECT range AS num FROM range(0,10)\n        partitions_concurrency:\n            description: Refers to the number of concurrent partitions that can be read at the same time (optional).\n            type: integer\n        partitions_watermark:\n            description: Refers to a customizable timestamp that can be set to check if an object has been updated (optional).\n            type: string\n        post_exec:\n            description: Refers to a SQL query that is run after the main query, available for DuckDB-based models. (optional). Ensure post_exec queries are idempotent. Use IF EXISTS statements when applicable.\n            examples:\n                - post_exec: DETACH DATABASE IF EXISTS postgres_db\n            type: string\n        pre_exec:\n            description: Refers to SQL queries to run before the main query, available for DuckDB-based models. (optional). Ensure pre_exec queries are idempotent. Use IF NOT EXISTS statements when applicable.\n            examples:\n                - pre_exec: ATTACH IF NOT EXISTS 'dbname=postgres host=localhost port=5432 user=postgres password=postgres' AS postgres_db (TYPE POSTGRES)\n            type: string\n        refresh:\n            $ref: '#/definitions/schedule_properties'\n            description: Specifies the refresh schedule that Rill should follow to re-ingest and update the underlying model data\n            examples:\n                - refresh:\n                    cron: '* * * * *'\n        retry:\n            description: Refers to the retry configuration for the model. (optional)\n            examples:\n                - retry:\n                    attempts: 5\n                    delay: 10s\n                    exponential_backoff: true\n                    if_error_matches:\n                        - .*OvercommitTracker.*\n                        - .*Timeout.*\n                        - .*Bad Gateway.*\n            properties:\n                attempts:\n                    description: The number of attempts to retry the model.\n                    type: integer\n                delay:\n                    description: The delay between attempts.\n                    type: string\n                exponential_backoff:\n                    description: Whether to use exponential backoff.\n                    type: boolean\n                if_error_matches:\n                    description: The error messages to match.\n                    items:\n                        type: string\n                    type: array\n            type: object\n        sql:\n            description: Raw SQL query to run against source\n            type: string\n        stage:\n            additionalProperties: true\n            description: in the case of staging models, where an input source does not support direct write to the output and a staging table is required\n            examples:\n                - stage:\n                    connector: s3\n                    path: s3://my_bucket/my_staging_table\n            properties:\n                connector:\n                    description: Refers to the connector type for the staging table\n                    type: string\n                path:\n                    description: Refers to the path to the staging table\n                    type: string\n            required:\n                - connector\n            type: object\n        state:\n            $ref: '#/definitions/data_properties'\n            description: Refers to the explicitly defined state of your model, cannot be used with partitions (optional)\n            examples:\n                - state:\n                    sql: SELECT MAX(date) as max_date\n        timeout:\n            description: The maximum time to wait for model ingestion\n            type: string\n        type:\n            const: model\n            description: Refers to the resource type and must be `model`\n            type: string\n      required:\n        - type\n        - sql\n      title: Properties\n      type: object\n    - $ref: '#/definitions/common_properties'\n    - allOf:\n        - if:\n            properties:\n                connector:\n                    const: clickhouse\n            required:\n                - connector\n            title: Additional properties for `output` when `connector` is `clickhouse`\n          then:\n            properties:\n                columns:\n                    description: Column names and types. Can also include indexes. If unspecified, detected from the query.\n                    type: string\n                dictionary_source_password:\n                    description: Password for the dictionary source user.\n                    type: string\n                dictionary_source_user:\n                    description: User for accessing the source dictionary table (used if type is DICTIONARY).\n                    type: string\n                distributed_settings:\n                    description: Settings for distributed table.\n                    type: string\n                distributed_sharding_key:\n                    description: Sharding key for distributed table.\n                    type: string\n                engine:\n                    description: Table engine to use. Default is MergeTree\n                    type: string\n                engine_full:\n                    description: Full engine definition in SQL format. Can include partition keys, order, TTL, etc.\n                    type: string\n                order_by:\n                    description: ORDER BY clause.\n                    type: string\n                partition_by:\n                    description: Partition BY clause.\n                    type: string\n                primary_key:\n                    description: PRIMARY KEY clause.\n                    type: string\n                query_settings:\n                    description: Settings used in insert/create table as select queries.\n                    type: string\n                sample_by:\n                    description: SAMPLE BY clause.\n                    type: string\n                table_settings:\n                    description: Table-specific settings.\n                    type: string\n                ttl:\n                    description: TTL settings for the table or columns.\n                    type: string\n                type:\n                    description: Type to materialize the model into. Can be 'TABLE', 'VIEW' or 'DICTIONARY'\n                    enum:\n                        - TABLE\n                        - VIEW\n                        - DICTIONARY\n                    type: string\n      required:\n        - type\n        - sql\n      type: object\n    - $ref: '#/definitions/common_properties'\n    - allOf:\n        - if:\n            properties:\n                connector:\n                    const: athena\n            required:\n                - connector\n            title: Additional properties when `connector` is `athena` or [named connector](./connectors#athena) for athena\n          then:\n            $ref: '#/definitions/models/definitions/athena'\n        - if:\n            properties:\n                connector:\n                    const: azure\n            required:\n                - connector\n            title: Additional properties when `connector` is `azure` or [named connector](./connectors#azure) of azure\n          then:\n            $ref: '#/definitions/models/definitions/azure'\n        - if:\n            properties:\n                connector:\n                    const: bigquery\n            required:\n                - connector\n            title: Additional properties when `connector` is `bigquery` or [named connector](./connectors#bigquery) of bigquery\n          then:\n            $ref: '#/definitions/models/definitions/bigquery'\n        - if:\n            properties:\n                connector:\n                    const: duckdb\n            required:\n                - connector\n            title: Additional properties when `connector` is `duckdb` or [named connector](./connectors#duckdb) of duckdb\n          then:\n            $ref: '#/definitions/models/definitions/duckdb'\n        - if:\n            properties:\n                connector:\n                    const: gcs\n            required:\n                - connector\n            title: Additional properties when `connector` is `gcs` or [named connector](./connectors#gcs) of gcs\n          then:\n            $ref: '#/definitions/models/definitions/gcs'\n        - if:\n            properties:\n                connector:\n                    const: local_file\n            required:\n                - connector\n            title: Additional properties when `connector` is `local_file` or [named connector](/developer/build/connectors/data-source/local-file) of local_file\n          then:\n            $ref: '#/definitions/models/definitions/local_file'\n        - if:\n            properties:\n                connector:\n                    const: redshift\n            required:\n                - connector\n            title: Additional properties when `connector` is `redshift` or [named connector](./connectors#redshift) of redshift\n          then:\n            $ref: '#/definitions/models/definitions/redshift'\n        - if:\n            properties:\n                connector:\n                    const: s3\n            required:\n                - connector\n            title: Additional properties when `connector` is `s3` or [named connector](./connectors#s3) of s3\n          then:\n            $ref: '#/definitions/models/definitions/s3'\n        - if:\n            properties:\n                connector:\n                    const: salesforce\n            required:\n                - connector\n            title: Additional properties when `connector` is `salesforce` or [named connector](./connectors#salesforce) of salesforce\n          then:\n            $ref: '#/definitions/models/definitions/salesforce'\n      type: object\ndefinitions:\n    athena:\n        properties:\n            output_location:\n                description: Output location for query results in S3.\n                type: string\n            region:\n                description: AWS region to connect to Athena and the output location.\n                type: string\n            workgroup:\n                description: AWS Athena workgroup to use for queries.\n                type: string\n        type: object\n    azure:\n        properties:\n            account:\n                description: Account identifier\n                type: string\n            batch_size:\n                description: Size of a batch (e.g., '100MB')\n                type: string\n            extract:\n                additionalProperties: true\n                description: Arbitrary key-value pairs for extraction settings\n                type: object\n            glob:\n                description: Settings related to glob file matching.\n                properties:\n                    max_objects_listed:\n                        description: Maximum number of objects listed in glob\n                        type: integer\n                    max_objects_matched:\n                        description: Maximum number of objects matched by glob\n                        type: integer\n                    max_total_size:\n                        description: Maximum total size (in bytes) matched by glob\n                        type: integer\n                    page_size:\n                        description: Page size for glob listing\n                        type: integer\n                type: object\n            path:\n                description: Path to the source\n                type: string\n            uri:\n                description: Source URI\n                type: string\n        type: object\n    bigquery:\n        properties:\n            project_id:\n                description: ID of the BigQuery project.\n                type: string\n        type: object\n    duckdb:\n        properties:\n            create_secrets_from_connectors:\n                description: List of connector names for which temporary secrets should be created before executing the SQL.\n                type:\n                    - string\n                    - array\n            format:\n                description: Format of the data source (e.g., csv, json, parquet).\n                type: string\n            path:\n                description: Path to the data source.\n                type: string\n            post_exec:\n                description: refers to a SQL query that is run after the main query, available for DuckDB-based models. _(optional)_. Ensure `post_exec` queries are idempotent. Use `IF EXISTS` statements when applicable.\n                examples:\n                    - post_exec: DETACH DATABASE IF EXISTS postgres_db\n                      pre_exec: ATTACH IF NOT EXISTS 'dbname=postgres host=localhost port=5432 user=postgres password=postgres' AS postgres_db (TYPE POSTGRES);\n                      sql: SELECT * FROM postgres_query('postgres_db', 'SELECT * FROM USERS')\n                type: string\n            pre_exec:\n                description: refers to SQL queries to run before the main query, available for DuckDB-based models. _(optional)_. Ensure `pre_exec` queries are idempotent. Use `IF NOT EXISTS` statements when applicable.\n                type: string\n        type: object\n    gcs:\n        properties:\n            batch_size:\n                description: Size of a batch (e.g., '100MB')\n                type: string\n            extract:\n                additionalProperties: true\n                description: key-value pairs for extraction settings\n                type: object\n            glob:\n                description: Settings related to glob file matching.\n                properties:\n                    max_objects_listed:\n                        description: Maximum number of objects listed in glob\n                        type: integer\n                    max_objects_matched:\n                        description: Maximum number of objects matched by glob\n                        type: integer\n                    max_total_size:\n                        description: Maximum total size (in bytes) matched by glob\n                        type: integer\n                    page_size:\n                        description: Page size for glob listing\n                        type: integer\n                type: object\n            path:\n                description: Path to the source\n                type: string\n            uri:\n                description: Source URI\n                type: string\n        type: object\n    local_file:\n        properties:\n            format:\n                description: Format of the data source (e.g., csv, json, parquet).\n                type: string\n            path:\n                description: Path to the data source.\n                type: string\n        type: object\n    redshift:\n        properties:\n            cluster_identifier:\n                description: Identifier of the Redshift cluster.\n                type: string\n            database:\n                description: Name of the Redshift database.\n                type: string\n            output_location:\n                description: S3 location where query results are stored.\n                type: string\n            region:\n                description: AWS region of the Redshift deployment.\n                type: string\n            role_arn:\n                description: ARN of the IAM role to assume for Redshift access.\n                type: string\n            workgroup:\n                description: Redshift Serverless workgroup to use.\n                type: string\n        type: object\n    s3:\n        properties:\n            batch_size:\n                description: Size of a batch (e.g., '100MB')\n                type: string\n            endpoint:\n                description: AWS Endpoint\n                type: string\n            extract:\n                additionalProperties: true\n                description: key-value pairs for extraction settings\n                type: object\n            glob:\n                description: Settings related to glob file matching.\n                properties:\n                    max_objects_listed:\n                        description: Maximum number of objects listed in glob\n                        type: integer\n                    max_objects_matched:\n                        description: Maximum number of objects matched by glob\n                        type: integer\n                    max_total_size:\n                        description: Maximum total size (in bytes) matched by glob\n                        type: integer\n                    page_size:\n                        description: Page size for glob listing\n                        type: integer\n                type: object\n            path:\n                description: Path to the source\n                type: string\n            region:\n                description: AWS region\n                type: string\n            uri:\n                description: Source URI\n                type: string\n        type: object\n    salesforce:\n        properties:\n            queryAll:\n                description: Whether to include deleted and archived records in the query (uses queryAll API).\n                type: boolean\n            sobject:\n                description: Salesforce object (e.g., Account, Contact) targeted by the query.\n                type: string\n            soql:\n                description: SOQL query to execute against the Salesforce instance.\n                type: string\n        type: object\ndescription: |4\n    This file is used to define YAML models. For more information on our SQL models, see the [SQL models](/developer/build/models/) documentation.\n    :::tip\n\n    Both regular models and source models can use the Model YAML specification described on this page. While [SQL models](/developer/build/models) are perfect for simple transformations, Model YAML files provide advanced capabilities for complex data processing scenarios.\n\n    **When to use Model YAML:**\n    - **Partitions** - Optimize performance with data partitioning strategies\n    - **Incremental models** - Process only new or changed data efficiently\n    - **Pre/post execution hooks** - Run custom logic before or after model execution\n    - **Staging** - Create intermediate tables for complex transformations\n    - **Output configuration** - Define specific output formats and destinations\n\n    Model YAML files give you fine-grained control over how your data is processed and transformed, making them ideal for production workloads and complex analytics pipelines.\n\n    :::\nexamples:\n    - connector: bigquery\n      incremental: true\n      output:\n        connector: duckdb\n      sql: \"SELECT ... FROM events \\n  {{ if incremental }} \\n      WHERE event_time > '{{.state.max_date}}' \\n  {{end}}\\n\"\n      state:\n        sql: SELECT MAX(date) as max_date\n      type: model\n    - output:\n        connector: duckdb\n        incremental_strategy: append\n      partitions:\n        glob:\n            connector: gcs\n            path: gs://rilldata-public/github-analytics/Clickhouse/2025/*/commits_*.parquet\n      sql: SELECT * FROM read_parquet('{{ .partition.uri }}')\n      type: model\n    - incremental: true\n      output:\n        connector: duckdb\n        incremental_strategy: append\n      partitions:\n        glob:\n            partition: directory\n            path: gs://rilldata-public/github-analytics/Clickhouse/2025/*/*\n      refresh:\n        cron: 0 8 * * *\n      sql: \"SELECT * \\n  FROM read_parquet('gs://rilldata-public/{{ .partition.path }}/commits_*.parquet') \\n  WHERE '{{ .partition.path }}' IS NOT NULL\\n\"\n      type: model\n    - connector: snowflake\n      incremental: true\n      output:\n        connector: clickhouse\n      partitions:\n        connector: duckdb\n        sql: SELECT range as day FROM range(TIMESTAMPTZ '2024-01-01', now(), INTERVAL 1 DAY)\n      sql: SELECT * FROM events WHERE date_trunc('day', event_time) = '{{ .partition.day }}'\n      stage:\n        connector: s3\n        path: s3://bucket/temp-data\n      type: model\nid: models\ntitle: Models YAML\ntype: object\n```\n\n## Examples\n\n### Simple model with mock data as a SQL file\n\n```sql\n-- models/mock_data.sql\nSELECT now() AS time, 'Denmark' AS country, 1 AS revenue_usd\nUNION ALL\nSELECT now() AS time, 'United States' AS country, 2 AS revenue_usd\n```\n\n### Materialized model as a SQL file\n\n```sql\n-- models/events.sql\n-- @materialize: true\nSELECT * FROM 's3://bucket/path/to/file.parquet'\n```\n\n### Simple model as a YAML file\n\n```yaml\n# models/mock_data.yaml\ntype: model\nmaterialize: false\n\nsql: |\n  SELECT now() AS time, 'Denmark' AS country, 1 AS revenue_usd\n  UNION ALL\n  SELECT now() AS time, 'United States' AS country, 2 AS revenue_usd\n```\n\n### S3 to DuckDB (Parquet files)\n\nAssuming DuckDB is the default OLAP and there is an `s3.yaml` connector in the project:\n\n```yaml\n# models/events_raw.yaml\ntype: model\nmaterialize: true\n\nsql: |\n  SELECT * FROM read_parquet('s3://my-bucket/data/events/*.parquet')\n```\n\n### S3 to DuckDB with explicit OLAP connector\n\nAssuming there is an `s3.yaml` connector in the project:\n\n```yaml\n# models/orders.yaml\ntype: model\nmaterialize: true\n\nconnector: duckdb\nsql: |\n  SELECT\n    order_id,\n    customer_id,\n    order_date,\n    total_amount\n  FROM read_parquet('s3://my-bucket/orders/year=2025/month=*/*.parquet')\n\noutput:\n  connector: duckdb\n```\n\n### GCS to DuckDB (JSON files)\n\nAssuming DuckDB is the default OLAP and there is a `gcs.yaml` connector in the project:\n\n```yaml\n# models/commits.yaml\ntype: model\nmaterialize: true\n\nsql: |\n  SELECT * FROM read_json(\n    'gs://my-bucket/data/commits.json.gz',\n    auto_detect=true,\n    format='auto'\n  )\n```\n\n### BigQuery to DuckDB\n\nAssuming DuckDB is the default OLAP and there is a `bigquery.yaml` connector in the project:\n\n```yaml\n# models/orders.yaml\ntype: model\n\nrefresh:\n  cron: 0 0 * * *\n\nconnector: bigquery\nsql: |\n  SELECT * FROM my_dataset.orders\n  WHERE order_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY)\n```\n\n### Snowflake to DuckDB with dev data limit\n\nAssuming DuckDB is the default OLAP and there is a `snowflake.yaml` connector in the project:\n\n```yaml\n# models/sales.yaml\ntype: model\n\nrefresh:\n  cron: 0 6 * * *\n\nconnector: snowflake\nsql: |\n  SELECT * FROM staging.sales\n  {{ if dev }} event_time >= '2025-01-01' AND event_time < '2025-02-01' {{ end }}\n```\n\n### MySQL to DuckDB\n\nAssuming there is a `mysql.yaml` connector in the project:\n\n```yaml\n# models/users.yaml\ntype: model\n\nrefresh:\n  cron: 0 * * * *\n\nconnector: mysql\nsql: |\n  SELECT\n    id,\n    email,\n    created_at,\n    status\n  FROM users\n  WHERE status = 'active'\n```\n\n### Local CSV file to DuckDB or Clickhouse\n\n```yaml\n# models/reference_data.yaml\ntype: model\n\nconnector: local_file\npath: data/reference_data.csv\n```\n\n### HTTPS source (public Parquet file)\n\n```yaml\n# models/public_dataset.yaml\ntype: model\n\nconnector: https\nuri: https://example.com/public/dataset.parquet\n```\n\n### Partition-based incremental S3 to DuckDB\n\nAssuming DuckDB is the default OLAP and there is a `s3.yaml` connector in the project:\n\n```yaml\n# models/daily_events.yaml\ntype: model\nincremental: true\n\npartitions:\n  glob:\n    path: s3://my-bucket/events/year=*/month=*/day=*/*.parquet\n    partition: directory\n\nsql: SELECT * FROM read_parquet('{{ .partition.uri }}/*.parquet')\n```\n\n### Basic S3 to ClickHouse\n\nAssuming Clickhouse is the default OLAP and there is an `s3.yaml` connector in the project:\n\n```yaml\n# models/events.yaml\ntype: model\nmaterialize: true\n\nsql: |\n  SELECT * FROM s3(\n    's3://my-bucket/events/year=*/month=*/day=*/*.parquet',\n    '{{ .env.aws_access_key_id }}',\n    '{{ .env.aws_secret_access_key }}',\n    'Parquet'\n  )\n\noutput:\n  order_by: event_time\n```\n\n### Partition-based incremental S3 to ClickHouse\n\nAssuming Clickhouse is the default OLAP and there is an `s3.yaml` connector in the project:\n\n```yaml\n# models/events.yaml\ntype: model\nmaterialize: true\nincremental: true\n\npartitions:\n  glob:\n    connector: s3\n    path: s3://my-bucket/events/year=*/month=*/day=*/*.parquet\n\nsql: |\n  SELECT * FROM s3(\n    '{{ .partition.uri }}',\n    '{{ .env.aws_access_key_id }}',\n    '{{ .env.aws_secret_access_key }}',\n    'Parquet'\n  )\n\noutput:\n  order_by: event_time\n```\n\n### ClickHouse with explicit column schema and indexes and TTL\n\nAssuming Clickhouse is the default OLAP and there is an `s3.yaml` connector in the project:\n\n```yaml\n# models/impressions.yaml\ntype: model\nmaterialize: true\nincremental: true\n\npartitions:\n  glob:\n    connector: s3\n    path: s3://my-bucket/impressions/year=*/month=*/day=*/*.parquet\n    partition: directory\n\nsql: |\n  SELECT\n    '{{ .partition.uri }}' AS __partition,\n    event_time,\n    user_id,\n    campaign_id,\n    country,\n    device_type,\n    impressions,\n    clicks,\n    cost\n  FROM s3(\n    '{{ .partition.uri }}/*.parquet',\n    '{{ .env.aws_access_key_id }}',\n    '{{ .env.aws_secret_access_key }}',\n    'Parquet'\n  )\n\noutput:\n  incremental_strategy: partition_overwrite\n  partition_by: toYYYYMMDD(event_time)\n  order_by: (event_time, user_id)\n  ttl: event_time + INTERVAL 90 DAY DELETE\n  columns: |\n    (\n      event_time DateTime,\n      user_id LowCardinality(String),\n      campaign_id LowCardinality(String),\n      country LowCardinality(String),\n      device_type LowCardinality(String),\n      impressions UInt32,\n      clicks UInt32,\n      cost Float64,\n      INDEX idx_campaign campaign_id TYPE bloom_filter GRANULARITY 4,\n      INDEX idx_country country TYPE set(100) GRANULARITY 4\n    )\n```\n\n### State-based incremental with merge strategy (upserts)\n\nAssuming DuckDB is the default OLAP and there is a `bigquery.yaml` connector in the project:\n\n```yaml\n# models/users.yaml\ntype: model\nincremental: true\n\nconnector: bigquery\nsql: |\n  SELECT * FROM users\n  {{ if incremental }}\n  WHERE updated_at > '{{ .state.max_updated_at }}'\n  {{ end }}\n\nstate:\n  sql: SELECT MAX(updated_at) as max_updated_at FROM users\n\noutput:\n  incremental_strategy: merge\n  unique_key: [user_id]\n```\n\n### Dev partitions for faster development\n\n```yaml\n# models/large_dataset.yaml\ntype: model\nincremental: true\n\npartitions:\n  glob:\n    path: s3://my-bucket/data/year=*/month=*/day=*/*.parquet\n    partition: directory\n\ndev:\n  partitions:\n    glob:\n      path: s3://my-bucket/data/year=2025/month=01/day=01/*.parquet\n      partition: directory\n\nsql: SELECT * FROM read_parquet('{{ .partition.uri }}/*.parquet')\n```\n\n### Partition filtering with transform_sql\n\n```yaml\n# models/filtered_partitions.yaml\ntype: model\nincremental: true\n\npartitions:\n  glob:\n    path: s3://my-bucket/reports/y=*/m=*/d=*/h=*/*.parquet\n    partition: directory\n    transform_sql: |\n      -- Only process partitions after a specific date\n      SELECT uri, updated_on\n      FROM {{ .table }}\n      WHERE uri >= 's3://my-bucket/reports/y=2025/m=06/d=01/h=00'\n      {{ if dev }}\n      AND uri < 's3://my-bucket/reports/y=2025/m=07/d=01/h=00'\n      {{ end }}\n      ORDER BY uri ASC\n\nsql: SELECT * FROM read_parquet('{{ .partition.uri }}/*.parquet')\n```\n\n### Long timeout for large data processing\n\n```yaml\n# models/large_historical.yaml\ntype: model\nmaterialize: true\nincremental: true\ntimeout: 72h\n\npartitions:\n  glob:\n    path: s3://my-bucket/historical/year=*/*.parquet\n    partition: directory\n\nsql: SELECT * FROM read_parquet('{{ .partition.uri }}/*.parquet')\n```\n\n### Non-materialized derived model\n\n```yaml\n# models/enriched_orders.yaml\ntype: model\nmaterialize: false\n\nsql: |\n  SELECT\n    o.*,\n    c.customer_name,\n    c.customer_segment,\n    p.product_category\n  FROM {{ ref \"orders\" }} o\n  LEFT JOIN {{ ref \"customers\" }} c ON o.customer_id = c.customer_id\n  LEFT JOIN {{ ref \"products\" }} p ON o.product_id = p.product_id\n```\n\n### DuckDB model reading from S3 with CSV options\n\nAssuming DuckDB is the default OLAP and there is a `s3.yaml` connector in the project:\n\n```yaml\n# models/csv_import.yaml\ntype: model\nmaterialize: true\nconnector: duckdb\n\nsql: |\n  SELECT * FROM read_csv(\n    's3://my-bucket/data/*.csv.gz',\n    auto_detect=true,\n    header=true,\n    ignore_errors=true,\n    union_by_name=true,\n    all_varchar=true\n  )\n```\n\n### SQL-based partitions (date range)\n\n```yaml\n# models/events_raw.yaml\ntype: model\nincremental: true\n\npartitions:\n  connector: snowflake\n  sql: SELECT DISTINCT date_trunc('day', event_time) AS day FROM events ORDER BY day\n\nconnector: snowflake\nsql: |\n  SELECT * FROM events\n  WHERE date_trunc('day', event_time) = '{{ .partition.day }}'\n\noutput:\n  incremental_strategy: partition_overwrite\n```\n\n### Change mode for incremental models\n\n```yaml\n# models/append_only.yaml\ntype: model\nincremental: true\nchange_mode: patch  # Switch to new logic without reprocessing historical data\n\npartitions:\n  glob:\n    path: s3://my-bucket/transactions/year=*/month=*/day=*/*.parquet\n    partition: directory\n\nsql: |\n  SELECT\n    event_time,\n    country,\n    costs_usd + profit_usd + tax_usd AS value_usd\n  FROM read_parquet('{{ .partition.uri }}/*.parquet')\n```"
    - role: user
      content_type: text
      content: |-
        You should develop a Rill project file based on the following task description:
        - Develop file at path: /models/glob_test_csv.yaml
        - The file should be of type: model
        - Task description: Create a DuckDB model that loads CSV data from S3 path:
        `s3://integration-test.rilldata.com/glob_test/y=*/*.csv`

        Constraints / best practices:
        - Create as YAML (not .sql).
        - Materialize into default OLAP connector (duckdb) (readwrite).
        - Use DuckDB's `read_csv_auto` (or `read_csv`) with a glob that supports the given wildcard.
        - Ensure the model reads from S3; configure it to use the S3 connector by setting `input` connector to `s3` if needed, but the query will execute in duckdb (output to duckdb). If Rill supports separate input/output connectors in the model, set input to s3 and output to duckdb.
        - Include `materialize: true`.
        - Add a few robust CSV options: `header=true`, `ignore_errors=true` (if supported), `all_varchar=true` (optional) to avoid type issues, and include a `_rill_loaded_at` column with `now()`.
        - Name the table/model `glob_test_csv`.

        Return valid Rill model YAML.

        Here is some important context:
        - You are running as a sub-agent of a larger developer agent. Stay aligned on your specific task and avoid extra discovery.
        - When you call 'write_file', if it returns a parse or reconcile error, do your best to fix the issue and try again. If you think the error is unrelated to the current path, let the parent agent know to handle it.

        Here is some additional context that may or may not be relevant to your task:
        - The project's default OLAP connector is named duckdb (driver: duckdb).
    - role: assistant
      content_type: tool_call
      id: d75159e234fe452c
      tool_name: write_file
      content: '{"contents":"type: model\nmaterialize: true\n\n# Execute the query in DuckDB (default OLAP) and read directly from S3.\n# The S3 connector provides credentials/config for DuckDB to access the bucket.\nconnector: duckdb\ncreate_secrets_from_connectors: [s3]\n\nsql: |\n  SELECT\n    *,\n    now() AS _rill_loaded_at\n  FROM read_csv_auto(\n    ''s3://integration-test.rilldata.com/glob_test/y=*/*.csv'',\n    header=true,\n    ignore_errors=true,\n    all_varchar=true\n  )\n","path":"/models/glob_test_csv.yaml"}'
    - role: tool
      content_type: tool_response
      id: d75159e234fe452c
      content: '{"diff":"--- /models/glob_test_csv.yaml\n+++ /models/glob_test_csv.yaml\n@@ -1 +1,19 @@\n+type: model\n+materialize: true\n \n+# Execute the query in DuckDB (default OLAP) and read directly from S3.\n+# The S3 connector provides credentials/config for DuckDB to access the bucket.\n+connector: duckdb\n+create_secrets_from_connectors: [s3]\n+\n+sql: |\n+  SELECT\n+    *,\n+    now() AS _rill_loaded_at\n+  FROM read_csv_auto(\n+    ''s3://integration-test.rilldata.com/glob_test/y=*/*.csv'',\n+    header=true,\n+    ignore_errors=true,\n+    all_varchar=true\n+  )\n+\n","is_new_file":true,"resources":[{"kind":"rill.runtime.v1.Model","name":"glob_test_csv","reconcile_error":"","reconcile_status":"RECONCILE_STATUS_IDLE"}]}'
  response:
    - role: assistant
      content_type: text
      content: |-
        ```yaml
        type: model
        materialize: true

        # Execute the query in DuckDB (default OLAP) and read directly from S3.
        # The S3 connector provides credentials/config for DuckDB to access the bucket.
        connector: duckdb
        create_secrets_from_connectors: [s3]

        sql: |
          SELECT
            *,
            now() AS _rill_loaded_at
          FROM read_csv_auto(
            's3://integration-test.rilldata.com/glob_test/y=*/*.csv',
            header=true,
            ignore_errors=true,
            all_varchar=true
          )
        ```
- index: 11
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Models as SQL files:** Always create new models as `.yaml` files, not `.sql` files (which are harder to extend later).\n- **Not creating connector files:** When Rill has native support for a connector (like S3 or BigQuery), always create a dedicated connector resource file for it.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. This also includes models that load external data using a native SQL function, like `read_parquet(...)` or `s3(...)`. Non-materialized models become views, which re-execute on every query.\n- **Referencing non-existant environment variables:** Only reference environment variables that are present in `.env` (returned in `env` from `project_status`). If you need the user to add another environment variable, stop and ask them to do so.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: user
      content_type: text
      content: |-
        This may not relate to the user's task, but for context, the project's default OLAP connector is named duckdb (driver: duckdb).
         The default OLAP is in readwrite mode, so you can use it in models if you want.


        Task: I have some CSV files in S3. Can you create a connector for S3 and a DuckDB model that loads the data at s3://integration-test.rilldata.com/glob_test/y=*/*.csv? I've already added environment variables for S3 access. Please proceed without asking clarifying questions.
    - role: assistant
      content_type: tool_call
      id: 96eedb8a328f4b8c
      tool_name: list_files
      content: '{}'
    - role: tool
      content_type: tool_response
      id: 96eedb8a328f4b8c
      content: '{"files":[{"path":"/connectors/duckdb.yaml","resources":[{"kind":"rill.runtime.v1.Connector","name":"duckdb","reconcile_error":"","reconcile_status":"RECONCILE_STATUS_IDLE"}]},{"path":"/rill.yaml"}]}'
    - role: assistant
      content_type: tool_call
      id: 12392148d788479a
      tool_name: project_status
      content: '{}'
    - role: tool
      content_type: tool_response
      id: 12392148d788479a
      content: '{"default_olap_connector":"duckdb","env":["connector.openai.api_key","connector.s3.aws_access_key_id","connector.s3.aws_secret_access_key"],"resources":[{"kind":"rill.runtime.v1.ProjectParser","name":"parser","path":"","reconcile_error":"","reconcile_status":"RECONCILE_STATUS_IDLE","refs":[]},{"kind":"rill.runtime.v1.Connector","name":"duckdb","path":"/connectors/duckdb.yaml","reconcile_error":"","reconcile_status":"RECONCILE_STATUS_IDLE","refs":[]}],"parse_errors":[]}'
    - role: assistant
      content_type: tool_call
      id: e4a7881f2a234928
      tool_name: search_files
      content: '{"glob_pattern":"**/*.yaml","pattern":"type:\\s*connector|driver:\\s*s3|s3"}'
    - role: tool
      content_type: tool_response
      id: e4a7881f2a234928
      content: '{"matches":[{"path":"/connectors/duckdb.yaml","lines":[2],"snippets":["  1: \n\u003e 2: type: connector\n  3: driver: duckdb\n  4: managed: true"]}]}'
    - role: assistant
      content_type: tool_call
      id: c2809355b0b942e6
      tool_name: develop_file
      content: '{"path":"connectors/s3.yaml","prompt":"Create a new S3 connector resource.\n\nRequirements:\n- File path: connectors/s3.yaml\n- Resource name should be `s3` (derived from filename).\n- Use driver: s3.\n- Use env vars already present in the project status: `connector.s3.aws_access_key_id` and `connector.s3.aws_secret_access_key`.\n- Configure region conservatively (us-east-1) but allow override via env var if present (do not introduce new required env vars). If you include optional env var references, guard them with defaults if supported.\n- Keep minimal properties needed for Rill to access public/private buckets.\n\nReturn valid YAML for a Rill connector.","type":"connector"}'
    - role: tool
      content_type: tool_response
      id: c2809355b0b942e6
      content: '{"response":"```yaml\ntype: connector\ndriver: s3\n\n# Credentials (from existing env vars)\naws_access_key_id: \"{{ .env.connector.s3.aws_access_key_id }}\"\naws_secret_access_key: \"{{ .env.connector.s3.aws_secret_access_key }}\"\n\n# Conservative default region\nregion: us-east-1\n```"}'
    - role: assistant
      content_type: tool_call
      id: cc73a202640542c1
      tool_name: develop_file
      content: '{"path":"models/glob_test_csv.yaml","prompt":"Create a DuckDB model that loads CSV data from S3 path:\n`s3://integration-test.rilldata.com/glob_test/y=*/*.csv`\n\nConstraints / best practices:\n- Create as YAML (not .sql).\n- Materialize into default OLAP connector (duckdb) (readwrite).\n- Use DuckDB''s `read_csv_auto` (or `read_csv`) with a glob that supports the given wildcard.\n- Ensure the model reads from S3; configure it to use the S3 connector by setting `input` connector to `s3` if needed, but the query will execute in duckdb (output to duckdb). If Rill supports separate input/output connectors in the model, set input to s3 and output to duckdb.\n- Include `materialize: true`.\n- Add a few robust CSV options: `header=true`, `ignore_errors=true` (if supported), `all_varchar=true` (optional) to avoid type issues, and include a `_rill_loaded_at` column with `now()`.\n- Name the table/model `glob_test_csv`.\n\nReturn valid Rill model YAML.","type":"model"}'
    - role: tool
      content_type: tool_response
      id: cc73a202640542c1
      content: '{"response":"```yaml\ntype: model\nmaterialize: true\n\n# Execute the query in DuckDB (default OLAP) and read directly from S3.\n# The S3 connector provides credentials/config for DuckDB to access the bucket.\nconnector: duckdb\ncreate_secrets_from_connectors: [s3]\n\nsql: |\n  SELECT\n    *,\n    now() AS _rill_loaded_at\n  FROM read_csv_auto(\n    ''s3://integration-test.rilldata.com/glob_test/y=*/*.csv'',\n    header=true,\n    ignore_errors=true,\n    all_varchar=true\n  )\n```"}'
  response:
    - role: assistant
      content_type: tool_call
      id: call_lpcTJbMCOfsuPx1QNeRnMwug
      tool_name: project_status
      content: '{"where_error":true}'
- index: 12
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Models as SQL files:** Always create new models as `.yaml` files, not `.sql` files (which are harder to extend later).\n- **Not creating connector files:** When Rill has native support for a connector (like S3 or BigQuery), always create a dedicated connector resource file for it.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. This also includes models that load external data using a native SQL function, like `read_parquet(...)` or `s3(...)`. Non-materialized models become views, which re-execute on every query.\n- **Referencing non-existant environment variables:** Only reference environment variables that are present in `.env` (returned in `env` from `project_status`). If you need the user to add another environment variable, stop and ask them to do so.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: user
      content_type: text
      content: |-
        This may not relate to the user's task, but for context, the project's default OLAP connector is named duckdb (driver: duckdb).
         The default OLAP is in readwrite mode, so you can use it in models if you want.


        Task: I have some CSV files in S3. Can you create a connector for S3 and a DuckDB model that loads the data at s3://integration-test.rilldata.com/glob_test/y=*/*.csv? I've already added environment variables for S3 access. Please proceed without asking clarifying questions.
    - role: assistant
      content_type: tool_call
      id: 96eedb8a328f4b8c
      tool_name: list_files
      content: '{}'
    - role: tool
      content_type: tool_response
      id: 96eedb8a328f4b8c
      content: '{"files":[{"path":"/connectors/duckdb.yaml","resources":[{"kind":"rill.runtime.v1.Connector","name":"duckdb","reconcile_error":"","reconcile_status":"RECONCILE_STATUS_IDLE"}]},{"path":"/rill.yaml"}]}'
    - role: assistant
      content_type: tool_call
      id: 12392148d788479a
      tool_name: project_status
      content: '{}'
    - role: tool
      content_type: tool_response
      id: 12392148d788479a
      content: '{"default_olap_connector":"duckdb","env":["connector.openai.api_key","connector.s3.aws_access_key_id","connector.s3.aws_secret_access_key"],"resources":[{"kind":"rill.runtime.v1.ProjectParser","name":"parser","path":"","reconcile_error":"","reconcile_status":"RECONCILE_STATUS_IDLE","refs":[]},{"kind":"rill.runtime.v1.Connector","name":"duckdb","path":"/connectors/duckdb.yaml","reconcile_error":"","reconcile_status":"RECONCILE_STATUS_IDLE","refs":[]}],"parse_errors":[]}'
    - role: assistant
      content_type: tool_call
      id: e4a7881f2a234928
      tool_name: search_files
      content: '{"glob_pattern":"**/*.yaml","pattern":"type:\\s*connector|driver:\\s*s3|s3"}'
    - role: tool
      content_type: tool_response
      id: e4a7881f2a234928
      content: '{"matches":[{"path":"/connectors/duckdb.yaml","lines":[2],"snippets":["  1: \n\u003e 2: type: connector\n  3: driver: duckdb\n  4: managed: true"]}]}'
    - role: assistant
      content_type: tool_call
      id: c2809355b0b942e6
      tool_name: develop_file
      content: '{"path":"connectors/s3.yaml","prompt":"Create a new S3 connector resource.\n\nRequirements:\n- File path: connectors/s3.yaml\n- Resource name should be `s3` (derived from filename).\n- Use driver: s3.\n- Use env vars already present in the project status: `connector.s3.aws_access_key_id` and `connector.s3.aws_secret_access_key`.\n- Configure region conservatively (us-east-1) but allow override via env var if present (do not introduce new required env vars). If you include optional env var references, guard them with defaults if supported.\n- Keep minimal properties needed for Rill to access public/private buckets.\n\nReturn valid YAML for a Rill connector.","type":"connector"}'
    - role: tool
      content_type: tool_response
      id: c2809355b0b942e6
      content: '{"response":"```yaml\ntype: connector\ndriver: s3\n\n# Credentials (from existing env vars)\naws_access_key_id: \"{{ .env.connector.s3.aws_access_key_id }}\"\naws_secret_access_key: \"{{ .env.connector.s3.aws_secret_access_key }}\"\n\n# Conservative default region\nregion: us-east-1\n```"}'
    - role: assistant
      content_type: tool_call
      id: cc73a202640542c1
      tool_name: develop_file
      content: '{"path":"models/glob_test_csv.yaml","prompt":"Create a DuckDB model that loads CSV data from S3 path:\n`s3://integration-test.rilldata.com/glob_test/y=*/*.csv`\n\nConstraints / best practices:\n- Create as YAML (not .sql).\n- Materialize into default OLAP connector (duckdb) (readwrite).\n- Use DuckDB''s `read_csv_auto` (or `read_csv`) with a glob that supports the given wildcard.\n- Ensure the model reads from S3; configure it to use the S3 connector by setting `input` connector to `s3` if needed, but the query will execute in duckdb (output to duckdb). If Rill supports separate input/output connectors in the model, set input to s3 and output to duckdb.\n- Include `materialize: true`.\n- Add a few robust CSV options: `header=true`, `ignore_errors=true` (if supported), `all_varchar=true` (optional) to avoid type issues, and include a `_rill_loaded_at` column with `now()`.\n- Name the table/model `glob_test_csv`.\n\nReturn valid Rill model YAML.","type":"model"}'
    - role: tool
      content_type: tool_response
      id: cc73a202640542c1
      content: '{"response":"```yaml\ntype: model\nmaterialize: true\n\n# Execute the query in DuckDB (default OLAP) and read directly from S3.\n# The S3 connector provides credentials/config for DuckDB to access the bucket.\nconnector: duckdb\ncreate_secrets_from_connectors: [s3]\n\nsql: |\n  SELECT\n    *,\n    now() AS _rill_loaded_at\n  FROM read_csv_auto(\n    ''s3://integration-test.rilldata.com/glob_test/y=*/*.csv'',\n    header=true,\n    ignore_errors=true,\n    all_varchar=true\n  )\n```"}'
    - role: assistant
      content_type: tool_call
      id: d49208acbcdd4934
      tool_name: project_status
      content: '{"where_error":true}'
    - role: tool
      content_type: tool_response
      id: d49208acbcdd4934
      content: '{"default_olap_connector":"duckdb","env":["connector.s3.aws_access_key_id","connector.s3.aws_secret_access_key","connector.openai.api_key"],"resources":[],"parse_errors":[]}'
  response:
    - role: assistant
      content_type: text
      content: |-
        Created two new resources:

        1) **S3 connector**: `connectors/s3.yaml`
        ```yaml
        type: connector
        driver: s3

        # Credentials (from existing env vars)
        aws_access_key_id: "{{ .env.connector.s3.aws_access_key_id }}"
        aws_secret_access_key: "{{ .env.connector.s3.aws_secret_access_key }}"

        # Conservative default region
        region: us-east-1
        ```

        2) **DuckDB model that loads the CSVs from S3**: `models/glob_test_csv.yaml`
        ```yaml
        type: model
        materialize: true

        # Execute the query in DuckDB (default OLAP) and read directly from S3.
        # The S3 connector provides credentials/config for DuckDB to access the bucket.
        connector: duckdb
        create_secrets_from_connectors: [s3]

        sql: |
          SELECT
            *,
            now() AS _rill_loaded_at
          FROM read_csv_auto(
            's3://integration-test.rilldata.com/glob_test/y=*/*.csv',
            header=true,
            ignore_errors=true,
            all_varchar=true
          )
        ```

        These should result in a materialized DuckDB table named `glob_test_csv` populated from `s3://integration-test.rilldata.com/glob_test/y=*/*.csv`.
