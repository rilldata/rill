- index: 1
  input:
    - role: system
      content_type: text
      content: |-
        You are a routing agent that determines which specialized agent should handle a user's request.
        You operate in the context of a business intelligence tool that supports data modeling and data exploration, and more.
        Your input includes the user's previous messages and responses, as well as the user's latest message, which you are responsible for routing.
        Routing guidelines:
        - If the user's question relates to developing or permanently changing the data model, metrics, or dashboards, you should route to the developer.
        - If the user's question relates to exploring dashboards or retrieving specific business metrics, you should route to the analyst.
        - If the user asks a general question, you should route to the analyst.
        - If the user specifically requests an agent by name, you must route to that agent if it's available.
        You must answer with a single agent choice and no further explanation. Pick only from this list of available agents (description in parentheses):
        - analyst_agent (Agent that assists with data analysis tasks.)
        - developer_agent (Agent that assists with development tasks.)
    - role: user
      content_type: text
      content: Clickhouse has a public playground database on play.clickhouse.com:9440 (username `play` and no password). Can you set it up as the OLAP connector and develop a dashboard based on one of its tables? Please proceed without asking clarifying questions.
  response:
    - role: assistant
      content_type: text
      content: '{"agent":"developer_agent"}'
- index: 2
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. Non-materialized models become views, which re-execute on every query.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: user
      content_type: text
      content: |-
        This may not relate to the user's task, but for context, the project's default OLAP connector is named duckdb (driver: duckdb).
         The default OLAP is in readwrite mode, so you can use it in models if you want.


        Task: Clickhouse has a public playground database on play.clickhouse.com:9440 (username `play` and no password). Can you set it up as the OLAP connector and develop a dashboard based on one of its tables? Please proceed without asking clarifying questions.
  response:
    - role: assistant
      content_type: tool_call
      id: call_zJ84M8XvuEiJ9Wv8odUeTprT
      tool_name: develop_file
      content: '{"path":"connectors/clickhouse.yaml","prompt":"Create a connector for the public ClickHouse playground database at play.clickhouse.com:9440. Use the ''clickhouse'' driver. Set the username to ''play'' and leave the password blank. This connector will be used as an OLAP connector for dashboards. Set ''mode'' to ''read'' to prevent writes. Name the connector ''clickhouse''.","type":"connector"}'
- index: 3
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. Non-materialized models become views, which re-execute on every query.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: system
      content_type: text
      content: |-
        # Instructions for developing a connector in Rill

        ## Introduction

        Connectors are resources that contain credentials and settings for connecting to external systems. They are typically found at the root of the project's DAG, providing access to data sources and services that power downstream resources like models and metrics views.

        Connectors are usually lightweight resources. When reconciled, they validate the connection to the external system but do not move or process data. The main exception is managed OLAP connectors (with `managed: true`), which trigger database provisioning.

        ### Driver capabilities

        Each connector uses a **driver** that implements one or more capabilities:

        - **OLAP database**: Can power metrics views and dashboards (e.g., `duckdb`, `clickhouse`)
        - **SQL database**: Can run SQL queries as model inputs (e.g., `postgres`, `bigquery`, `snowflake`)
        - **Information schema**: Can list tables and their schemas (e.g., `duckdb`, `bigquery`)
        - **Object store**: Can list, read, and write flat files (e.g., `s3`, `gcs`)
        - **Notifier**: Can send notifications and alerts (e.g., `slack`)
        - **AI**: Can generate embeddings or responses (e.g., `openai`)

        ## Core Concepts

        ### Naming conventions

        Connectors are typically named after their driver (e.g., a file `connectors/duckdb.yaml` creates a connector named `duckdb`). Use descriptive names when you have multiple connectors of the same type:
        - `connectors/s3_data.yaml` and `connectors/s3_staging.yaml`
        - `connectors/clickhouse_prod.yaml` and `connectors/clickhouse_dev.yaml`

        ### Secrets management

        Always store sensitive credentials in `.env` and reference them using template syntax:

        ```yaml
        type: connector
        driver: s3
        aws_access_key_id: "{{ .env.aws_access_key_id }}"
        aws_secret_access_key: "{{ .env.aws_secret_access_key }}"
        ```

        NOTE: Some legacy projects use the deprecated `.vars` instead of `.env`.

        ### Managed connectors

        OLAP connectors can be provisioned automatically by Rill using `managed: true`. This is supported for `duckdb` and `clickhouse` drivers:

        ```yaml
        type: connector
        driver: duckdb
        managed: true
        ```

        When a managed connector is reconciled, Rill provisions the database infrastructure. The user is billed for the CPU, memory, and disk usage of the provisioned database.

        ### Access modes

        Control read/write access using the `mode` property:

        - `mode: read`: Prevents Rill models from writing to this connector
        - `mode: readwrite`: Allows both reading and writing (default for managed connectors)

        Use `mode: read` when connecting to external databases with pre-existing tables to prevent unintended modifications.

        ### Dev/Prod configuration

        Use `dev:` and `prod:` blocks for environment-specific settings:

        ```yaml
        type: connector
        driver: clickhouse
        mode: readwrite

        # Use a local database in development to prevent overwriting data in the production cluster when iterating on changes.
        dev:
          managed: true

        prod:
          host: "{{ .env.clickhouse_host }}"
          port: 9440
          username: "{{ .env.clickhouse_user }}"
          password: "{{ .env.clickhouse_password }}"
          ssl: true
        ```

        ## Driver-Specific Notes

        ### DuckDB

        DuckDB is Rill's default embedded OLAP database. Key properties:

        - `managed: true`: Rill provisions and manages the database
        - `init_sql`: SQL to run at startup (install extensions, create secrets, attach databases)

        For MotherDuck (cloud DuckDB), use the `path` property with `md:` prefix:

        ```yaml
        type: connector
        driver: duckdb
        path: "md:my_database"
        token: "{{ .env.motherduck_token }}"
        ```

        ### ClickHouse

        ClickHouse can be user-managed or Rill-managed. Key properties:

        - `managed: true`: Rill provisions and manages an empty Clickhouse cluster. If set, don't set any other connector properties.
        - `host`, `port`, `username`, `password`: Connection credentials
        - `database`: Target database name
        - `ssl: true`: Required for ClickHouse Cloud
        - `cluster`: Cluster name for multi-node Clickhouse clusters
        - `dsn`: Alternative connection string format (format: `clickhouse://host:port?username=<username>&...`)

        Common ports:
        - `8443`: HTTPS native protocol (ClickHouse Cloud)
        - `9440`: Secure native protocol
        - `9000`: Native protocol (non-SSL)

        ### S3

        AWS S3 and S3-compatible storage. Key properties:

        - `aws_access_key_id`, `aws_secret_access_key`: AWS credentials
        - `region`: AWS region
        - `endpoint`: Custom endpoint for S3-compatible services (R2, MinIO, GCS interop)
        - `path_prefixes`: A list of bucket paths that the connector can access, e.g. `[s3://my-bucket]`; useful for improving bucket introspection

        ### GCS

        Google Cloud Storage. Key properties:

        - `google_application_credentials`: Service account JSON (must be a literal JSON string value)
        - `key_id`: HMAC key ID to use instead of a service account JSON; required for direct use with DuckDB and Clickhouse through S3 compatibility
        - `secret`: HMAC secret to use instead of a service account JSON; required for direct use with DuckDB and Clickhouse through S3 compatibility

        ### BigQuery

        Google BigQuery. Key properties:

        - `project_id`: GCP project ID
        - `google_application_credentials`: Service account JSON

        ### Snowflake

        Snowflake data warehouse. Key properties:

        - `account`, `user`, `privateKey`, `database`, `schema`, `warehouse`, `role`: Connection parameters
        - `dsn`: Connection string to use instead of separate connection parameters

        ### Postgres

        PostgreSQL database. Key properties:

        - `host`, `port`, `user`, `password`, `dbname`: Connection parameters
        - `sslmode`: SSL mode (`disable`, `require`, `verify-full`, etc.)

        ### Druid

        Apache Druid. Can be configured via host/port or DSN:

        - `host`, `port`, `username`, `password`, `ssl`: Direct connection
        - `dsn`: Full connection string

        ### Redshift

        Amazon Redshift. Key properties:

        - `aws_access_key_id`, `aws_secret_access_key`: AWS credentials
        - `workgroup`: Redshift Serverless workgroup name
        - `region`: AWS region
        - `database`: Database name

        ### Athena

        Amazon Athena. Key properties:

        - `aws_access_key_id`, `aws_secret_access_key`: AWS credentials
        - `workgroup`: Redshift Serverless workgroup name
        - `region`: AWS region
        - `output_location`: S3 path in format `s3://bucket/path` to store temporary query results in (Athena only)

        ### Other drivers

        - **Slack**: Use `bot_token` for alert notifications
        - **OpenAI**: Use `api_key` for AI-powered features
        - **HTTPS**: Simple connector for public HTTP sources
        - **Pinot**: Use `broker_host`, `controller_host`, `username`, `password`

        ## JSON Schema

        Here is a full JSON schema for the connector syntax:

        ```
        allOf:
            - properties:
                type:
                    const: connector
                    description: Refers to the resource type and must be `connector`
                    type: string
              required:
                - type
              title: Properties
              type: object
            - $ref: '#/definitions/common_properties'
            - oneOf:
                - examples:
                    - allow_host_access: true
                      aws_access_key_id: myawsaccesskey
                      aws_access_token: mytemporarytoken
                      aws_region: us-east-1
                      aws_secret_access_key: myawssecretkey
                      driver: athena
                      external_id: MyExternalID
                      output_location: s3://my-bucket/athena-output/
                      role_arn: arn:aws:iam::123456789012:role/MyRole
                      role_session_name: MySession
                      type: connector
                      workgroup: primary
                  properties:
                    allow_host_access:
                        description: Allow the Athena client to access host environment configurations such as environment variables or local AWS credential files. Defaults to true, enabling use of credentials and settings from the host environment unless explicitly disabled.
                        type: boolean
                    aws_access_key_id:
                        description: AWS Access Key ID used for authentication. Required when using static credentials directly or as base credentials for assuming a role.
                        type: string
                    aws_access_token:
                        description: AWS session token used with temporary credentials. Required only if the Access Key and Secret Key are part of a temporary session credentials.
                        type: string
                    aws_region:
                        description: AWS region where Athena and the result S3 bucket are located (e.g., us-east-1). Defaults to 'us-east-1' if not specified.
                        type: string
                    aws_secret_access_key:
                        description: AWS Secret Access Key paired with the Access Key ID. Required when using static credentials directly or as base credentials for assuming a role.
                        type: string
                    driver:
                        const: athena
                        description: Refers to the driver type and must be driver `athena`
                        type: string
                    external_id:
                        description: External ID required by some roles when assuming them, typically for cross-account access. Used only if 'role_arn' is specified and the role's trust policy requires it.
                        type: string
                    output_location:
                        description: S3 URI where Athena query results should be stored (e.g., s3://your-bucket/athena/results/). Optional if the selected workgroup has a default result configuration.
                        type: string
                    role_arn:
                        description: ARN of the IAM role to assume. When specified, the SDK uses the base credentials to call STS AssumeRole and obtain temporary credentials scoped to this role.
                        type: string
                    role_session_name:
                        description: Session name to associate with the STS AssumeRole session. Used only if 'role_arn' is specified. Useful for identifying and auditing the session.
                        type: string
                    workgroup:
                        description: Athena workgroup to use for query execution. Defaults to 'primary' if not specified.
                        type: string
                  required:
                    - driver
                  title: Athena
                  type: object
                - examples:
                    - azure_storage_account: mystorageaccount
                      azure_storage_key: credentialstring
                      driver: azure
                      type: connector
                  properties:
                    allow_host_access:
                        description: Allow access to host environment configuration
                        type: boolean
                    azure_storage_account:
                        description: Azure storage account name
                        type: string
                    azure_storage_connection_string:
                        description: Optional azure connection string for storage account
                        type: string
                    azure_storage_key:
                        description: Azure storage access key
                        type: string
                    azure_storage_sas_token:
                        description: Optional azure SAS token for authentication
                        type: string
                    driver:
                        const: azure
                        description: Refers to the driver type and must be driver `azure`
                        type: string
                    path_prefixes:
                        description: |
                            A list of container or virtual directory prefixes that this connector is allowed to access.
                            Useful when different containers or paths use different credentials, allowing the system
                            to route access through the appropriate connector based on the blob path.
                            Example: `azure://my-bucket/`, ` azure://my-bucket/path/` ,`azure://my-bucket/path/prefix`
                        type:
                            - string
                            - array
                  required:
                    - driver
                    - azure_storage_account
                    - azure_storage_key
                  title: Azure
                  type: object
                - examples:
                    - allow_host_access: true
                      driver: bigquery
                      google_application_credentials: credentialjsonstring
                      project_id: my-project-id
                      type: connector
                  properties:
                    allow_host_access:
                        description: Enable the BigQuery client to use credentials from the host environment when no service account JSON is provided. This includes Application Default Credentials from environment variables, local credential files, or the Google Compute Engine metadata server. Defaults to true, allowing seamless authentication in GCP environments.
                        type: boolean
                    dataset_id:
                        description: BigQuery dataset ID
                        type: string
                    driver:
                        const: bigquery
                        description: Refers to the driver type and must be driver `bigquery`
                        type: string
                    google_application_credentials:
                        description: Raw contents of the Google Cloud service account key (in JSON format) used for authentication.
                        type: string
                    location:
                        description: BigQuery dataset location
                        type: string
                    project_id:
                        description: Google Cloud project ID
                        type: string
                  required:
                    - driver
                  title: BigQuery
                  type: object
                - examples:
                    - cluster: mycluster
                      database: mydatabase
                      driver: clickhouse
                      host: localhost
                      managed: false
                      mode: readwrite
                      password: mypassword
                      port: 9000
                      ssl: true
                      type: connector
                      username: myusername
                  properties:
                    can_scale_to_zero:
                        description: Indicates if the database can scale to zero
                        type: boolean
                    cluster:
                        description: Cluster name, required for running distributed queries
                        type: string
                    conn_max_lifetime:
                        description: Maximum time a connection may be reused
                        type: string
                    database:
                        description: Name of the ClickHouse database within the cluster
                        type: string
                    dial_timeout:
                        description: Timeout for dialing the ClickHouse server
                        type: string
                    driver:
                        const: clickhouse
                        description: Refers to the driver type and must be driver `clickhouse`
                        type: string
                    dsn:
                        description: DSN(Data Source Name) for the ClickHouse connection
                        type: string
                    embed_port:
                        description: Port to run ClickHouse locally (0 for random port)
                        type: integer
                    host:
                        description: Host where the ClickHouse instance is running
                        type: string
                    log_queries:
                        description: Controls whether to log raw SQL queries
                        type: boolean
                    managed:
                        description: '`true` means Rill will provision the connector using the default provisioner. `false` disables automatic provisioning.'
                        type: boolean
                    max_idle_conns:
                        description: Maximum number of idle connections in the pool
                        type: integer
                    max_open_conns:
                        description: Maximum number of open connections to the database
                        type: integer
                    mode:
                        description: '`read` - Controls the operation mode for the ClickHouse connection. Defaults to ''read'' for safe operation with external databases. Set to ''readwrite'' to enable model creation and table mutations. Note: When ''managed: true'', this is automatically set to ''readwrite''.'
                        type: string
                    password:
                        description: Password for authentication
                        type: string
                    port:
                        description: Port where the ClickHouse instance is accessible
                        type: integer
                    query_settings:
                        description: query settings to be set on dashboard queries. `query_settings_override` takes precedence over these settings and if set these are ignored. Each setting must be separated by a comma. Example `max_threads = 8, max_memory_usage = 10000000000`
                        type: string
                    query_settings_override:
                        description: override the default settings used in queries. Changing the default settings can lead to incorrect query results and is generally not recommended. If you need to add settings, use `query_settings`
                        type: string
                    read_timeout:
                        description: Maximum time for a connection to read data
                        type: string
                    ssl:
                        description: Indicates whether a secured SSL connection is required
                        type: boolean
                    username:
                        description: Username for authentication
                        type: string
                  required:
                    - driver
                  title: ClickHouse
                  type: object
                - examples:
                    - driver: druid
                      host: localhost
                      password: mypassword
                      port: 8082
                      ssl: true
                      type: connector
                      username: myusername
                  properties:
                    driver:
                        const: druid
                        description: Refers to the driver type and must be driver `druid`
                        type: string
                    dsn:
                        description: Data Source Name (DSN) for connecting to Druid
                        type: string
                    host:
                        description: Hostname of the Druid coordinator or broker
                        type: string
                    log_queries:
                        description: Log raw SQL queries sent to Druid
                        type: boolean
                    max_open_conns:
                        description: Maximum number of open database connections (0 = default, -1 = unlimited)
                        type: integer
                    password:
                        description: Password for authenticating with Druid
                        type: string
                    port:
                        description: Port number of the Druid service
                        type: integer
                    skip_version_check:
                        description: Skip checking Druid version compatibility
                        type: boolean
                    ssl:
                        description: Enable SSL for secure connection
                        type: boolean
                    username:
                        description: Username for authenticating with Druid
                        type: string
                  required:
                    - driver
                    - dsn
                  title: Druid
                  type: object
                - examples:
                    - allow_host_access: true
                      cpu: 4
                      driver: duckdb
                      init_sql: INSTALL httpfs; LOAD httpfs;
                      log_queries: true
                      memory_limit_gb: 16
                      mode: readwrite
                      pool_size: 5
                      read_write_ratio: 0.7
                      type: connector
                  properties:
                    allow_host_access:
                        description: Whether access to local environment and file system is allowed
                        type: boolean
                    attach:
                        description: Full ATTACH statement to attach a DuckDB database
                        type: string
                    boot_queries:
                        description: Deprecated - Use init_sql instead
                        type: string
                    conn_init_sql:
                        description: SQL executed when a new connection is initialized
                        type: string
                    cpu:
                        description: Number of CPU cores available to the database
                        minimum: 1
                        type: integer
                    create_secrets_from_connectors:
                        description: List of connector names for which temporary secrets should be created before executing the SQL.
                        type:
                            - string
                            - array
                    database_name:
                        description: Name of the attached DuckDB database (auto-detected if not set)
                        type: string
                    driver:
                        const: duckdb
                        description: Must be "duckdb"
                        type: string
                    init_sql:
                        description: SQL executed during database initialization
                        type: string
                    log_queries:
                        description: Whether to log raw SQL queries executed through OLAP
                        type: boolean
                    memory_limit_gb:
                        description: Amount of memory in GB available to the database
                        minimum: 1
                        type: integer
                    mode:
                        default: read
                        description: Set the mode for the DuckDB connection.
                        enum:
                            - read
                            - readwrite
                        type: string
                    path:
                        description: Path to external DuckDB database
                        type: string
                    pool_size:
                        description: Number of concurrent connections and queries allowed
                        minimum: 1
                        type: integer
                    read_write_ratio:
                        default: 0.5
                        description: Ratio of resources allocated to read vs write operations
                        maximum: 1
                        minimum: 0
                        type: number
                    schema_name:
                        description: Default schema used by the DuckDB database
                        type: string
                  required:
                    - driver
                  title: DuckDB
                  type: object
                - examples:
                    - driver: duckdb
                      mode: read
                      path: /path/to/my-duckdb-database.db
                      type: connector
                  properties:
                    driver:
                        const: duckdb
                        description: Refers to the driver type and must be driver `duckdb`
                        type: string
                    mode:
                        default: read
                        description: Set the mode for the DuckDB connection.
                        enum:
                            - read
                            - readwrite
                        type: string
                    path:
                        description: Path to the DuckDB database
                        type: string
                  required:
                    - driver
                    - db
                  title: External DuckDB
                  type: object
                - examples:
                    - driver: gcs
                      google_application_credentials: credentialjsonstring
                      type: connector
                  properties:
                    allow_host_access:
                        description: Allow access to host environment configuration
                        type: boolean
                    driver:
                        const: gcs
                        description: Refers to the driver type and must be driver `gcs`
                        type: string
                    google_application_credentials:
                        description: Google Cloud credentials JSON string
                        type: string
                    key_id:
                        description: Optional S3-compatible Key ID when used in compatibility mode
                        type: string
                    path_prefixes:
                        description: "A list of bucket path prefixes that this connector is allowed to access. \nUseful when different buckets or bucket prefixes use different credentials, \nallowing the system to select the appropriate connector based on the bucket path.\nExample: `gs://my-bucket/`, ` gs://my-bucket/path/` ,`gs://my-bucket/path/prefix`\n"
                        type:
                            - string
                            - array
                    secret:
                        description: Optional S3-compatible Secret when used in compatibility mode
                        type: string
                  required:
                    - driver
                    - bucket
                  title: GCS
                  type: object
                - examples:
                    - driver: https
                      headers:
                        Authorization: Bearer my-token
                      type: connector
                  properties:
                    driver:
                        const: https
                        description: Refers to the driver type and must be driver `https`
                        type: string
                    headers:
                        additionalProperties:
                            type: string
                        description: HTTP headers to include in the request
                        type: object
                    path_prefixes:
                        description: |
                            A list of HTTP/HTTPS URL prefixes that this connector is allowed to access.
                            Useful when different URL namespaces use different credentials, enabling the
                            system to choose the appropriate connector based on the URL path.
                            Example: `https://example.com/`, ` https://example.com/path/` ,`https://example.com/path/prefix`
                        type:
                            - string
                            - array
                  required:
                    - driver
                    - path
                  title: HTTPS
                  type: object
                - examples:
                    - driver: duckdb
                      path: md:my_database
                      schema_name: my_schema
                      token: '{{ .env.connector.motherduck.token }}'
                      type: connector
                  properties:
                    create_secrets_from_connectors:
                        description: List of connector names for which temporary secrets should be created before executing the SQL.
                        type:
                            - string
                            - array
                    driver:
                        const: duckdb
                        description: Refers to the driver type and must be driver `duckdb`.
                        type: string
                    init_sql:
                        description: SQL executed during database initialization.
                        type: string
                    mode:
                        default: read
                        description: Set the mode for the MotherDuck connection. By default, it is set to 'read' which allows only read operations. Set to 'readwrite' to enable model creation and table mutations.
                        enum:
                            - read
                            - readwrite
                        type: string
                    path:
                        description: Path to your MD database
                        type: string
                    schema_name:
                        description: Define your schema if not main, uses main by default
                        type: string
                    token:
                        description: MotherDuck token
                        type: string
                  required:
                    - driver
                    - path
                    - token
                  title: MotherDuck
                  type: object
                - examples:
                    - database: mydb
                      driver: mysql
                      host: localhost
                      password: p@ss
                      port: 3306
                      ssl-mode: preferred
                      type: connector
                      user: user
                    - driver: mysql
                      dsn: mysql://user:p%40ss@localhost:3306/mydb?ssl-mode=preferred
                      type: connector
                  properties:
                    database:
                        description: Name of the MySQL database
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `mysql`
                        type: string
                    dsn:
                        description: |
                            **Data Source Name (DSN)** for the MySQL connection, provided in [MySQL URI format](https://dev.mysql.com/doc/refman/8.4/en/connecting-using-uri-or-key-value-pairs.html#connecting-using-uri).
                            The DSN must follow the standard MySQL URI scheme:
                            ```text
                            mysql://user:password@host:3306/my-db
                            ```
                            Rules for special characters in password:
                            - The following characters are allowed [unescaped in the URI](https://datatracker.ietf.org/doc/html/rfc3986#section-2.3): `~` `.` `_` `-`
                            - All other special characters must be percent-encoded (`%XX` format).
                            ```text
                            mysql://user:pa%40ss@localhost:3306/my-db   # password contains '@'
                            mysql://user:pa%3Ass@localhost:3306/my-db   # password contains ':'
                            ```
                        type: string
                    host:
                        description: Hostname of the MySQL server
                        type: string
                    password:
                        description: Password for authentication
                        type: string
                    port:
                        description: Port number for the MySQL server
                        type: integer
                    ssl-mode:
                        description: 'ssl mode options: `disabled`, `preferred`, or `required`.'
                        type: string
                    user:
                        description: Username for authentication
                        type: string
                  required:
                    - driver
                  title: MySQL
                  type: object
                - examples:
                    - api_key: my-api-key
                      api_type: openai
                      api_version: "2023-05-15"
                      base_url: https://api.openai.com/v1
                      driver: openai
                      model: gpt-4o
                      type: connector
                  properties:
                    api_key:
                        description: API key for connecting to OpenAI
                        type: string
                    api_type:
                        description: The type of OpenAI API to use
                        type: string
                    api_version:
                        description: The version of the OpenAI API to use (e.g., '2023-05-15'). Required when API Type is AZURE or AZURE_AD
                        type: string
                    base_url:
                        description: The base URL for the OpenAI API (e.g., 'https://api.openai.com/v1')
                        type: string
                    driver:
                        description: The driver type, must be set to "openai"
                        type: string
                    model:
                        description: The OpenAI model to use (e.g., 'gpt-4o')
                        type: string
                  required:
                    - api_key
                  title: OpenAI
                  type: object
                - examples:
                    - broker_host: localhost
                      broker_port: 9000
                      controller_host: localhost
                      controller_port: 9000
                      driver: pinot
                      log_queries: true
                      max_open_conns: 100
                      password: mypassword
                      ssl: true
                      type: connector
                      username: myusername
                  properties:
                    broker_host:
                        description: Hostname of the Pinot broker
                        type: string
                    broker_port:
                        description: Port number for the Pinot broker
                        type: integer
                    controller_host:
                        description: Hostname of the Pinot controller
                        type: string
                    controller_port:
                        description: Port number for the Pinot controller
                        type: integer
                    driver:
                        description: Refers to the driver type and must be driver `pinot`
                        type: string
                    dsn:
                        description: DSN(Data Source Name) for the Pinot connection
                        type: string
                    log_queries:
                        description: Log raw SQL queries executed through Pinot
                        type: boolean
                    max_open_conns:
                        description: Maximum number of open connections to the Pinot database
                        type: integer
                    password:
                        description: Password for authenticating with Pinot
                        type: string
                    ssl:
                        description: Enable SSL connection to Pinot
                        type: boolean
                    username:
                        description: Username for authenticating with Pinot
                        type: string
                  required:
                    - driver
                    - dsn
                    - broker_host
                    - controller_host
                  title: Pinot
                  type: object
                - examples:
                    - dbname: mydatabase
                      driver: postgres
                      host: localhost
                      password: mypassword
                      port: 5432
                      sslmode: prefer
                      type: connector
                      user: myusername
                    - driver: postgres
                      dsn: user=myusername password='my pass\'word' host=localhost port=5432 dbname=mydatabase sslmode=prefer
                      type: connector
                    - driver: postgres
                      dsn: postgres://myusername:p%40ss@localhost:5432/mydatabase?sslmode=prefer
                      type: connector
                  properties:
                    dbname:
                        description: Name of the Postgres database
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `postgres`
                        type: string
                    dsn:
                        description: |
                            **Data Source Name (DSN)** for the PostgreSQL connection, provided in
                            [PostgreSQL connection string format](https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING).
                            PostgreSQL supports both **key=value format** and **URI format**.

                            key=value format example:
                            ```text
                            user=user password=password host=host port=5432 dbname=mydb
                            ```
                            Rules for key=value format for special characters:
                            - To write an empty value, or a value containing spaces, `=`, single quotes, or backslashes, surround it with single quotes.
                            - Single quotes and backslashes inside a value must be escaped with a backslash (`\'` and `\\`).

                            URI format example:
                            ```text
                            postgres://user:password@host:5432/mydb
                            ```

                            Rules for URI format:
                            - The following characters are allowed [unescaped in the URI](https://datatracker.ietf.org/doc/html/rfc3986#section-2.3): `~` `.` `_` `-`
                            - All other special characters must be percent-encoded (`%XX` format).

                            Examples (URI format with encoded characters):
                            ```text
                            postgres://user:pa%40ss@localhost:5432/my-db   # '@' is encoded as %40
                            postgres://user:pa%3Ass@localhost:5432/my-db   # ':' is encoded as %3A
                            ```
                        type: string
                    host:
                        description: Hostname of the Postgres server
                        type: string
                    password:
                        description: Password for authentication
                        type: string
                    port:
                        description: Port number for the Postgres server
                        type: string
                    sslmode:
                        description: 'ssl mode options: `disable`, `allow`, `prefer` or `require`.'
                        type: string
                    user:
                        description: Username for authentication
                        type: string
                  required:
                    - driver
                  title: Postgres
                  type: object
                - examples:
                    - aws_access_key_id: my-access-key-id
                      aws_access_token: my-access-token
                      aws_secret_access_key: my-secret-access-key
                      cluster_identifier: my-cluster-identifier
                      database: mydatabase
                      driver: redshift
                      region: us-east-1
                      type: connector
                      workgroup: my-workgroup
                  properties:
                    aws_access_key_id:
                        description: AWS Access Key ID used for authenticating with Redshift.
                        type: string
                    aws_access_token:
                        description: AWS Session Token for temporary credentials (optional).
                        type: string
                    aws_secret_access_key:
                        description: AWS Secret Access Key used for authenticating with Redshift.
                        type: string
                    cluster_identifier:
                        description: Cluster identifier for provisioned Redshift clusters, in case of Redshift Serverless use 'workgroup' .
                        type: string
                    database:
                        description: Name of the Redshift database to query.
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `redshift`
                        type: string
                    region:
                        description: AWS region where the Redshift cluster or workgroup is hosted (e.g., 'us-east-1').
                        type: string
                    workgroup:
                        description: Workgroup name for Redshift Serverless, in case of provisioned Redshift clusters use 'cluster_identifier'.
                        type: string
                  required:
                    - driver
                    - aws_access_key_id
                    - aws_secret_access_key
                    - database
                  title: Redshift
                  type: object
                - examples:
                    - aws_access_key_id: my-access-key-id
                      aws_access_token: my-access-token
                      aws_secret_access_key: my-secret-access-key
                      driver: s3
                      endpoint: https://my-s3-endpoint.com
                      region: us-east-1
                      type: connector
                  properties:
                    allow_host_access:
                        description: Allow access to host environment configuration
                        type: boolean
                    aws_access_key_id:
                        description: AWS Access Key ID used for authentication
                        type: string
                    aws_access_token:
                        description: Optional AWS session token for temporary credentials
                        type: string
                    aws_secret_access_key:
                        description: AWS Secret Access Key used for authentication
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `s3`
                        type: string
                    endpoint:
                        description: Optional custom endpoint URL for S3-compatible storage
                        type: string
                    path_prefixes:
                        description: |
                            A list of bucket path prefixes that this connector is allowed to access.
                            Useful when different buckets or bucket prefixes use different credentials,
                            allowing the system to select the appropriate connector based on the bucket path.
                            Example: `s3://my-bucket/`, ` s3://my-bucket/path/` ,`s3://my-bucket/path/prefix`
                        type:
                            - string
                            - array
                    region:
                        description: AWS region of the S3 bucket
                        type: string
                  required:
                    - driver
                    - bucket
                  title: S3
                  type: object
                - examples:
                    - client_id: my-client-id
                      driver: salesforce
                      endpoint: https://login.salesforce.com
                      password: mypassword
                      type: connector
                      username: myusername
                  properties:
                    client_id:
                        description: Client ID used for Salesforce OAuth authentication
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `salesforce`
                        type: string
                    endpoint:
                        description: Salesforce API endpoint URL
                        type: string
                    key:
                        description: Authentication key for Salesforce (secret)
                        type: string
                    password:
                        description: Salesforce account password (secret)
                        type: string
                    username:
                        description: Salesforce account username
                        type: string
                  required:
                    - driver
                    - username
                    - endpoint
                    - client_id
                  title: Salesforce
                  type: object
                - examples:
                    - bot_token: xoxb-my-bot-token
                      driver: slack
                      type: connector
                  properties:
                    bot_token:
                        description: Bot token used for authenticating Slack API requests
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `slack`
                        type: string
                  required:
                    - driver
                    - bot_token
                  title: Slack
                  type: object
                - examples:
                    - account: my_account_identifier
                      database: my_db
                      driver: snowflake
                      parallel_fetch_limit: 2
                      privateKey: '{{ .env.SNOWFLAKE_PRIVATE_KEY }}'
                      role: my_role
                      schema: my_schema
                      type: connector
                      user: my_user
                      warehouse: my_wh
                    - driver: snowflake
                      dsn: '{{ .env.SNOWFLAKE_DSN }}'
                      parallel_fetch_limit: 2
                      type: connector
                  properties:
                    account:
                        description: Snowflake account identifier. To find your Snowflake account identifier, look at your Snowflake account URL. The account identifier is everything before .snowflakecomputing.com
                        type: string
                    authenticator:
                        description: Optional authenticator type (e.g., SNOWFLAKE_JWT).
                        type: string
                    database:
                        description: Name of the Snowflake database.
                        type: string
                    driver:
                        const: snowflake
                        description: Refers to the driver type and must be driver `snowflake`
                        type: string
                    dsn:
                        description: "DSN (Data Source Name) for the Snowflake connection.\n\nThis is intended for **advanced configuration** where you want to specify\nproperties that are not explicitly defined above.  \nIt can only be used when the other connection fields (account, user, password,\ndatabase, schema, warehouse, role, authenticator, privateKey) are **not used**.\n\nFor details on private key generation and encoding, see the `privateKey` property.\n"
                        type: string
                    parallel_fetch_limit:
                        description: Maximum number of concurrent fetches during query execution.
                        type: integer
                    password:
                        description: Password for the Snowflake connection. _(deprecated, use privateKey instead)_
                        type: string
                    privateKey:
                        description: |
                            Private key for JWT authentication.
                            :::tip
                            Private key must be generated as a **PKCS#8 (nocrypt) key**, since the Snowflake Go driver
                            only supports unencrypted private keys. After generating, it must be **base64 URL encoded**.

                            Example commands to generate and encode:

                            ```bash
                            # Generate a 2048-bit unencrypted PKCS#8 private key
                            openssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out rsa_key.p8 -nocrypt

                            # Convert URL safe format for Snowflake
                            cat rsa_key.p8 | grep -v "\----" | tr -d '\n' | tr '+/' '-_'
                            ```
                            See: https://docs.snowflake.com/en/guide/key-pair-auth
                            :::
                        type: string
                    role:
                        description: Snowflake role to use.
                        type: string
                    schema:
                        description: Schema within the database to use.
                        type: string
                    user:
                        description: Username for the Snowflake connection.
                        type: string
                    warehouse:
                        description: Compute warehouse to use for queries.
                        type: string
                  required:
                    - type
                    - driver
                  title: Snowflake
                  type: object
                - examples:
                    - driver: sqlite
                      dsn: file:mydatabase.db
                      type: connector
                  properties:
                    driver:
                        description: Refers to the driver type and must be driver `sqlite`
                        type: string
                    dsn:
                        description: DSN(Data Source Name) for the sqlite connection
                        type: string
                  required:
                    - driver
                    - dsn
                  title: SQLite
                  type: object
        description: |
            Connector YAML files define how Rill connects to external data sources and OLAP engines. Each connector specifies a driver type and its required connection parameters.

            ## Available Connector Types

            ### _OLAP Engines_
            - [**ClickHouse**](#clickhouse) - ClickHouse analytical database
            - [**Druid**](#druid) - Apache Druid
            - [**DuckDB**](#duckdb) - Embedded DuckDB engine (default)
            - [**MotherDuck**](#motherduck) - MotherDuck cloud database
            - [**Pinot**](#pinot) - Apache Pinot

            ### _Data Warehouses_
            - [**Athena**](#athena) - Amazon Athena
            - [**BigQuery**](#bigquery) - Google BigQuery
            - [**Redshift**](#redshift) - Amazon Redshift
            - [**Snowflake**](#snowflake) - Snowflake data warehouse

            ### _Databases_
            - [**MySQL**](#mysql) - MySQL databases
            - [**PostgreSQL**](#postgres) - PostgreSQL databases
            - [**SQLite**](#sqlite) - SQLite databases

            ### _Cloud Storage_
            - [**Azure**](#azure) - Azure Blob Storage
            - [**GCS**](#gcs) - Google Cloud Storage
            - [**S3**](#s3) - Amazon S3 storage

            ### _Other_
            - [**Extenral DuckDB**](#external-duckdb) - External DuckDB database
            - [**HTTPS**](#https) - Public files via HTTP/HTTPS
            - [**OpenAI**](#openai) - OpenAI data
            - [**Salesforce**](#salesforce) - Salesforce data
            - [**Slack**](#slack) - Slack data

            :::warning Security Recommendation
            For all credential parameters (passwords, tokens, keys), use environment variables with the syntax `{{.env.connector.<connector_driver>.<parameter_name>}}`. This keeps sensitive data out of your YAML files and version control. See our [credentials documentation](/developers/build/connectors/credentials/) for complete setup instructions.
            :::
        id: connectors
        title: Connector YAML
        type: object
        ```

        ## Examples

        ### DuckDB: Managed

        Explicit:

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb
        managed: true
        ```

        or relying on defaults:

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb
        ```

        ### DuckDB: With init_sql for S3 secrets

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb

        init_sql: |
          CREATE SECRET IF NOT EXISTS s3 (
            TYPE S3,
            KEY_ID '{{ .env.aws_access_key_id }}',
            SECRET '{{ .env.aws_secret_access_key }}',
            REGION 'us-east-1'
          )
        ```

        This is now deprecated in favor of creating a dedicated `s3.yaml` connector file, which Rill will automatically load and create as a secret in DuckDB.

        ### DuckDB: With extensions

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb

        init_sql: |
          INSTALL spatial;
          LOAD spatial;
        ```

        ### DuckDB: MotherDuck database with existing tables

        ```yaml
        # connectors/motherduck.yaml
        type: connector
        driver: duckdb
        path: "md:my_database"
        token: "{{ .env.motherduck_token }}"
        schema_name: main
        mode: read
        ```

        ### ClickHouse: Cloud with SSL

        ```yaml
        # connectors/clickhouse.yaml
        type: connector
        driver: clickhouse
        host: "abc123.us-east-1.aws.clickhouse.cloud"
        port: 8443
        username: "default"
        password: "{{ .env.clickhouse_password }}"
        database: "default"
        ssl: true
        ```

        ### ClickHouse: Readwrite with cluster

        ```yaml
        # connectors/clickhouse.yaml
        type: connector
        driver: clickhouse
        mode: readwrite

        host: "{{ .env.clickhouse_host }}"
        port: 9440
        username: "{{ .env.clickhouse_user }}"
        password: "{{ .env.clickhouse_password }}"
        database: "default"
        cluster: "my_cluster"
        ssl: true
        ```

        ### ClickHouse: Dev/prod configuration

        ```yaml
        # connectors/clickhouse.yaml
        type: connector
        driver: clickhouse
        mode: readwrite

        dev:
          managed: true

        prod:
          host: "{{ .env.clickhouse_host }}"
          port: 9440
          username: "{{ .env.clickhouse_user }}"
          password: "{{ .env.clickhouse_password }}"
          database: "default"
          cluster: "{{ .env.clickhouse_cluster }}"
          ssl: true
        ```

        ### S3: Basic with credentials and region

        ```yaml
        # connectors/s3.yaml
        type: connector
        driver: s3
        aws_access_key_id: "{{ .env.aws_access_key_id }}"
        aws_secret_access_key: "{{ .env.aws_secret_access_key }}"
        region: us-west-2
        ```

        ### S3: Cloudflare R2 (S3-compatible)

        ```yaml
        # connectors/r2.yaml
        type: connector
        driver: s3
        aws_access_key_id: "{{ .env.r2_access_key_id }}"
        aws_secret_access_key: "{{ .env.r2_secret_access_key }}"
        endpoint: "https://{{ .env.r2_account_id }}.r2.cloudflarestorage.com"
        region: auto
        ```

        ### GCS: Minimal (uses default credentials)

        ```yaml
        # connectors/gcs.yaml
        type: connector
        driver: gcs
        ```

        ### GCS: With explicit credentials

        ```yaml
        # connectors/gcs.yaml
        type: connector
        driver: gcs
        google_application_credentials: "{{ .env.gcs_service_account_json }}"
        ```

        ### BigQuery

        ```yaml
        # connectors/bigquery.yaml
        type: connector
        driver: bigquery
        project_id: "my-gcp-project"
        google_application_credentials: "{{ .env.bigquery_service_account_json }}"
        ```

        ### Snowflake: Basic with DSN

        ```yaml
        # connectors/snowflake.yaml
        type: connector
        driver: snowflake
        dsn: "{{ .env.snowflake_dsn }}"
        ```

        ### Postgres

        ```yaml
        # connectors/postgres.yaml
        type: connector
        driver: postgres
        host: "{{ .env.postgres_host }}"
        port: 5432
        user: "{{ .env.postgres_user }}"
        password: "{{ .env.postgres_password }}"
        dbname: "analytics"
        sslmode: require
        ```

        ### Druid: Host-based

        ```yaml
        # connectors/druid.yaml
        type: connector
        driver: druid
        host: "{{ .env.druid_host }}"
        port: 8888
        username: "{{ .env.druid_user }}"
        password: "{{ .env.druid_password }}"
        ssl: true
        ```

        ### Redshift: Serverless

        ```yaml
        # connectors/redshift.yaml
        type: connector
        driver: redshift
        aws_access_key_id: "{{ .env.aws_access_key_id }}"
        aws_secret_access_key: "{{ .env.aws_secret_access_key }}"
        workgroup: "my-workgroup"
        region: us-east-1
        database: "analytics"
        ```

        ### OpenAI

        ```yaml
        # connectors/openai.yaml
        type: connector
        driver: openai
        api_key: "{{ .env.openai_api_key }}"
        ```

        ### Slack

        ```yaml
        # connectors/slack.yaml
        type: connector
        driver: slack
        bot_token: "{{ .env.slack_bot_token }}"
        ```

        ### Pinot

        ```yaml
        # connectors/pinot.yaml
        type: connector
        driver: pinot
        broker_host: "{{ .env.pinot_broker_host }}"
        controller_host: "{{ .env.pinot_controller_host }}"
        username: "{{ .env.pinot_user }}"
        password: "{{ .env.pinot_password }}"
        ssl: true
        ```
    - role: user
      content_type: text
      content: |-
        You should develop a Rill project file based on the following task description: Create a connector for the public ClickHouse playground database at play.clickhouse.com:9440. Use the 'clickhouse' driver. Set the username to 'play' and leave the password blank. This connector will be used as an OLAP connector for dashboards. Set 'mode' to 'read' to prevent writes. Name the connector 'clickhouse'.

        Here is some important context:
        - You should develop the file at path: /connectors/clickhouse.yaml
        - The file should be of type: connector
        - You are running as a sub-agent of a larger developer agent. Stay aligned on your specific task, avoid extra discovery, and report back your success or failure to the parent agent.

        Here is some additional context that may or may not be relevant to your task:
        - The project's default OLAP connector is named duckdb (driver: duckdb).
  response:
    - role: assistant
      content_type: tool_call
      id: call_k8hETAwauS6f5nohiQQTZEw1
      tool_name: write_file
      content: '{"contents":"type: connector\ndriver: clickhouse\nhost: play.clickhouse.com\nport: 9440\nusername: play\npassword: \"\"\nmode: read\n","path":"/connectors/clickhouse.yaml"}'
- index: 4
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. Non-materialized models become views, which re-execute on every query.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: system
      content_type: text
      content: |-
        # Instructions for developing a connector in Rill

        ## Introduction

        Connectors are resources that contain credentials and settings for connecting to external systems. They are typically found at the root of the project's DAG, providing access to data sources and services that power downstream resources like models and metrics views.

        Connectors are usually lightweight resources. When reconciled, they validate the connection to the external system but do not move or process data. The main exception is managed OLAP connectors (with `managed: true`), which trigger database provisioning.

        ### Driver capabilities

        Each connector uses a **driver** that implements one or more capabilities:

        - **OLAP database**: Can power metrics views and dashboards (e.g., `duckdb`, `clickhouse`)
        - **SQL database**: Can run SQL queries as model inputs (e.g., `postgres`, `bigquery`, `snowflake`)
        - **Information schema**: Can list tables and their schemas (e.g., `duckdb`, `bigquery`)
        - **Object store**: Can list, read, and write flat files (e.g., `s3`, `gcs`)
        - **Notifier**: Can send notifications and alerts (e.g., `slack`)
        - **AI**: Can generate embeddings or responses (e.g., `openai`)

        ## Core Concepts

        ### Naming conventions

        Connectors are typically named after their driver (e.g., a file `connectors/duckdb.yaml` creates a connector named `duckdb`). Use descriptive names when you have multiple connectors of the same type:
        - `connectors/s3_data.yaml` and `connectors/s3_staging.yaml`
        - `connectors/clickhouse_prod.yaml` and `connectors/clickhouse_dev.yaml`

        ### Secrets management

        Always store sensitive credentials in `.env` and reference them using template syntax:

        ```yaml
        type: connector
        driver: s3
        aws_access_key_id: "{{ .env.aws_access_key_id }}"
        aws_secret_access_key: "{{ .env.aws_secret_access_key }}"
        ```

        NOTE: Some legacy projects use the deprecated `.vars` instead of `.env`.

        ### Managed connectors

        OLAP connectors can be provisioned automatically by Rill using `managed: true`. This is supported for `duckdb` and `clickhouse` drivers:

        ```yaml
        type: connector
        driver: duckdb
        managed: true
        ```

        When a managed connector is reconciled, Rill provisions the database infrastructure. The user is billed for the CPU, memory, and disk usage of the provisioned database.

        ### Access modes

        Control read/write access using the `mode` property:

        - `mode: read`: Prevents Rill models from writing to this connector
        - `mode: readwrite`: Allows both reading and writing (default for managed connectors)

        Use `mode: read` when connecting to external databases with pre-existing tables to prevent unintended modifications.

        ### Dev/Prod configuration

        Use `dev:` and `prod:` blocks for environment-specific settings:

        ```yaml
        type: connector
        driver: clickhouse
        mode: readwrite

        # Use a local database in development to prevent overwriting data in the production cluster when iterating on changes.
        dev:
          managed: true

        prod:
          host: "{{ .env.clickhouse_host }}"
          port: 9440
          username: "{{ .env.clickhouse_user }}"
          password: "{{ .env.clickhouse_password }}"
          ssl: true
        ```

        ## Driver-Specific Notes

        ### DuckDB

        DuckDB is Rill's default embedded OLAP database. Key properties:

        - `managed: true`: Rill provisions and manages the database
        - `init_sql`: SQL to run at startup (install extensions, create secrets, attach databases)

        For MotherDuck (cloud DuckDB), use the `path` property with `md:` prefix:

        ```yaml
        type: connector
        driver: duckdb
        path: "md:my_database"
        token: "{{ .env.motherduck_token }}"
        ```

        ### ClickHouse

        ClickHouse can be user-managed or Rill-managed. Key properties:

        - `managed: true`: Rill provisions and manages an empty Clickhouse cluster. If set, don't set any other connector properties.
        - `host`, `port`, `username`, `password`: Connection credentials
        - `database`: Target database name
        - `ssl: true`: Required for ClickHouse Cloud
        - `cluster`: Cluster name for multi-node Clickhouse clusters
        - `dsn`: Alternative connection string format (format: `clickhouse://host:port?username=<username>&...`)

        Common ports:
        - `8443`: HTTPS native protocol (ClickHouse Cloud)
        - `9440`: Secure native protocol
        - `9000`: Native protocol (non-SSL)

        ### S3

        AWS S3 and S3-compatible storage. Key properties:

        - `aws_access_key_id`, `aws_secret_access_key`: AWS credentials
        - `region`: AWS region
        - `endpoint`: Custom endpoint for S3-compatible services (R2, MinIO, GCS interop)
        - `path_prefixes`: A list of bucket paths that the connector can access, e.g. `[s3://my-bucket]`; useful for improving bucket introspection

        ### GCS

        Google Cloud Storage. Key properties:

        - `google_application_credentials`: Service account JSON (must be a literal JSON string value)
        - `key_id`: HMAC key ID to use instead of a service account JSON; required for direct use with DuckDB and Clickhouse through S3 compatibility
        - `secret`: HMAC secret to use instead of a service account JSON; required for direct use with DuckDB and Clickhouse through S3 compatibility

        ### BigQuery

        Google BigQuery. Key properties:

        - `project_id`: GCP project ID
        - `google_application_credentials`: Service account JSON

        ### Snowflake

        Snowflake data warehouse. Key properties:

        - `account`, `user`, `privateKey`, `database`, `schema`, `warehouse`, `role`: Connection parameters
        - `dsn`: Connection string to use instead of separate connection parameters

        ### Postgres

        PostgreSQL database. Key properties:

        - `host`, `port`, `user`, `password`, `dbname`: Connection parameters
        - `sslmode`: SSL mode (`disable`, `require`, `verify-full`, etc.)

        ### Druid

        Apache Druid. Can be configured via host/port or DSN:

        - `host`, `port`, `username`, `password`, `ssl`: Direct connection
        - `dsn`: Full connection string

        ### Redshift

        Amazon Redshift. Key properties:

        - `aws_access_key_id`, `aws_secret_access_key`: AWS credentials
        - `workgroup`: Redshift Serverless workgroup name
        - `region`: AWS region
        - `database`: Database name

        ### Athena

        Amazon Athena. Key properties:

        - `aws_access_key_id`, `aws_secret_access_key`: AWS credentials
        - `workgroup`: Redshift Serverless workgroup name
        - `region`: AWS region
        - `output_location`: S3 path in format `s3://bucket/path` to store temporary query results in (Athena only)

        ### Other drivers

        - **Slack**: Use `bot_token` for alert notifications
        - **OpenAI**: Use `api_key` for AI-powered features
        - **HTTPS**: Simple connector for public HTTP sources
        - **Pinot**: Use `broker_host`, `controller_host`, `username`, `password`

        ## JSON Schema

        Here is a full JSON schema for the connector syntax:

        ```
        allOf:
            - properties:
                type:
                    const: connector
                    description: Refers to the resource type and must be `connector`
                    type: string
              required:
                - type
              title: Properties
              type: object
            - $ref: '#/definitions/common_properties'
            - oneOf:
                - examples:
                    - allow_host_access: true
                      aws_access_key_id: myawsaccesskey
                      aws_access_token: mytemporarytoken
                      aws_region: us-east-1
                      aws_secret_access_key: myawssecretkey
                      driver: athena
                      external_id: MyExternalID
                      output_location: s3://my-bucket/athena-output/
                      role_arn: arn:aws:iam::123456789012:role/MyRole
                      role_session_name: MySession
                      type: connector
                      workgroup: primary
                  properties:
                    allow_host_access:
                        description: Allow the Athena client to access host environment configurations such as environment variables or local AWS credential files. Defaults to true, enabling use of credentials and settings from the host environment unless explicitly disabled.
                        type: boolean
                    aws_access_key_id:
                        description: AWS Access Key ID used for authentication. Required when using static credentials directly or as base credentials for assuming a role.
                        type: string
                    aws_access_token:
                        description: AWS session token used with temporary credentials. Required only if the Access Key and Secret Key are part of a temporary session credentials.
                        type: string
                    aws_region:
                        description: AWS region where Athena and the result S3 bucket are located (e.g., us-east-1). Defaults to 'us-east-1' if not specified.
                        type: string
                    aws_secret_access_key:
                        description: AWS Secret Access Key paired with the Access Key ID. Required when using static credentials directly or as base credentials for assuming a role.
                        type: string
                    driver:
                        const: athena
                        description: Refers to the driver type and must be driver `athena`
                        type: string
                    external_id:
                        description: External ID required by some roles when assuming them, typically for cross-account access. Used only if 'role_arn' is specified and the role's trust policy requires it.
                        type: string
                    output_location:
                        description: S3 URI where Athena query results should be stored (e.g., s3://your-bucket/athena/results/). Optional if the selected workgroup has a default result configuration.
                        type: string
                    role_arn:
                        description: ARN of the IAM role to assume. When specified, the SDK uses the base credentials to call STS AssumeRole and obtain temporary credentials scoped to this role.
                        type: string
                    role_session_name:
                        description: Session name to associate with the STS AssumeRole session. Used only if 'role_arn' is specified. Useful for identifying and auditing the session.
                        type: string
                    workgroup:
                        description: Athena workgroup to use for query execution. Defaults to 'primary' if not specified.
                        type: string
                  required:
                    - driver
                  title: Athena
                  type: object
                - examples:
                    - azure_storage_account: mystorageaccount
                      azure_storage_key: credentialstring
                      driver: azure
                      type: connector
                  properties:
                    allow_host_access:
                        description: Allow access to host environment configuration
                        type: boolean
                    azure_storage_account:
                        description: Azure storage account name
                        type: string
                    azure_storage_connection_string:
                        description: Optional azure connection string for storage account
                        type: string
                    azure_storage_key:
                        description: Azure storage access key
                        type: string
                    azure_storage_sas_token:
                        description: Optional azure SAS token for authentication
                        type: string
                    driver:
                        const: azure
                        description: Refers to the driver type and must be driver `azure`
                        type: string
                    path_prefixes:
                        description: |
                            A list of container or virtual directory prefixes that this connector is allowed to access.
                            Useful when different containers or paths use different credentials, allowing the system
                            to route access through the appropriate connector based on the blob path.
                            Example: `azure://my-bucket/`, ` azure://my-bucket/path/` ,`azure://my-bucket/path/prefix`
                        type:
                            - string
                            - array
                  required:
                    - driver
                    - azure_storage_account
                    - azure_storage_key
                  title: Azure
                  type: object
                - examples:
                    - allow_host_access: true
                      driver: bigquery
                      google_application_credentials: credentialjsonstring
                      project_id: my-project-id
                      type: connector
                  properties:
                    allow_host_access:
                        description: Enable the BigQuery client to use credentials from the host environment when no service account JSON is provided. This includes Application Default Credentials from environment variables, local credential files, or the Google Compute Engine metadata server. Defaults to true, allowing seamless authentication in GCP environments.
                        type: boolean
                    dataset_id:
                        description: BigQuery dataset ID
                        type: string
                    driver:
                        const: bigquery
                        description: Refers to the driver type and must be driver `bigquery`
                        type: string
                    google_application_credentials:
                        description: Raw contents of the Google Cloud service account key (in JSON format) used for authentication.
                        type: string
                    location:
                        description: BigQuery dataset location
                        type: string
                    project_id:
                        description: Google Cloud project ID
                        type: string
                  required:
                    - driver
                  title: BigQuery
                  type: object
                - examples:
                    - cluster: mycluster
                      database: mydatabase
                      driver: clickhouse
                      host: localhost
                      managed: false
                      mode: readwrite
                      password: mypassword
                      port: 9000
                      ssl: true
                      type: connector
                      username: myusername
                  properties:
                    can_scale_to_zero:
                        description: Indicates if the database can scale to zero
                        type: boolean
                    cluster:
                        description: Cluster name, required for running distributed queries
                        type: string
                    conn_max_lifetime:
                        description: Maximum time a connection may be reused
                        type: string
                    database:
                        description: Name of the ClickHouse database within the cluster
                        type: string
                    dial_timeout:
                        description: Timeout for dialing the ClickHouse server
                        type: string
                    driver:
                        const: clickhouse
                        description: Refers to the driver type and must be driver `clickhouse`
                        type: string
                    dsn:
                        description: DSN(Data Source Name) for the ClickHouse connection
                        type: string
                    embed_port:
                        description: Port to run ClickHouse locally (0 for random port)
                        type: integer
                    host:
                        description: Host where the ClickHouse instance is running
                        type: string
                    log_queries:
                        description: Controls whether to log raw SQL queries
                        type: boolean
                    managed:
                        description: '`true` means Rill will provision the connector using the default provisioner. `false` disables automatic provisioning.'
                        type: boolean
                    max_idle_conns:
                        description: Maximum number of idle connections in the pool
                        type: integer
                    max_open_conns:
                        description: Maximum number of open connections to the database
                        type: integer
                    mode:
                        description: '`read` - Controls the operation mode for the ClickHouse connection. Defaults to ''read'' for safe operation with external databases. Set to ''readwrite'' to enable model creation and table mutations. Note: When ''managed: true'', this is automatically set to ''readwrite''.'
                        type: string
                    password:
                        description: Password for authentication
                        type: string
                    port:
                        description: Port where the ClickHouse instance is accessible
                        type: integer
                    query_settings:
                        description: query settings to be set on dashboard queries. `query_settings_override` takes precedence over these settings and if set these are ignored. Each setting must be separated by a comma. Example `max_threads = 8, max_memory_usage = 10000000000`
                        type: string
                    query_settings_override:
                        description: override the default settings used in queries. Changing the default settings can lead to incorrect query results and is generally not recommended. If you need to add settings, use `query_settings`
                        type: string
                    read_timeout:
                        description: Maximum time for a connection to read data
                        type: string
                    ssl:
                        description: Indicates whether a secured SSL connection is required
                        type: boolean
                    username:
                        description: Username for authentication
                        type: string
                  required:
                    - driver
                  title: ClickHouse
                  type: object
                - examples:
                    - driver: druid
                      host: localhost
                      password: mypassword
                      port: 8082
                      ssl: true
                      type: connector
                      username: myusername
                  properties:
                    driver:
                        const: druid
                        description: Refers to the driver type and must be driver `druid`
                        type: string
                    dsn:
                        description: Data Source Name (DSN) for connecting to Druid
                        type: string
                    host:
                        description: Hostname of the Druid coordinator or broker
                        type: string
                    log_queries:
                        description: Log raw SQL queries sent to Druid
                        type: boolean
                    max_open_conns:
                        description: Maximum number of open database connections (0 = default, -1 = unlimited)
                        type: integer
                    password:
                        description: Password for authenticating with Druid
                        type: string
                    port:
                        description: Port number of the Druid service
                        type: integer
                    skip_version_check:
                        description: Skip checking Druid version compatibility
                        type: boolean
                    ssl:
                        description: Enable SSL for secure connection
                        type: boolean
                    username:
                        description: Username for authenticating with Druid
                        type: string
                  required:
                    - driver
                    - dsn
                  title: Druid
                  type: object
                - examples:
                    - allow_host_access: true
                      cpu: 4
                      driver: duckdb
                      init_sql: INSTALL httpfs; LOAD httpfs;
                      log_queries: true
                      memory_limit_gb: 16
                      mode: readwrite
                      pool_size: 5
                      read_write_ratio: 0.7
                      type: connector
                  properties:
                    allow_host_access:
                        description: Whether access to local environment and file system is allowed
                        type: boolean
                    attach:
                        description: Full ATTACH statement to attach a DuckDB database
                        type: string
                    boot_queries:
                        description: Deprecated - Use init_sql instead
                        type: string
                    conn_init_sql:
                        description: SQL executed when a new connection is initialized
                        type: string
                    cpu:
                        description: Number of CPU cores available to the database
                        minimum: 1
                        type: integer
                    create_secrets_from_connectors:
                        description: List of connector names for which temporary secrets should be created before executing the SQL.
                        type:
                            - string
                            - array
                    database_name:
                        description: Name of the attached DuckDB database (auto-detected if not set)
                        type: string
                    driver:
                        const: duckdb
                        description: Must be "duckdb"
                        type: string
                    init_sql:
                        description: SQL executed during database initialization
                        type: string
                    log_queries:
                        description: Whether to log raw SQL queries executed through OLAP
                        type: boolean
                    memory_limit_gb:
                        description: Amount of memory in GB available to the database
                        minimum: 1
                        type: integer
                    mode:
                        default: read
                        description: Set the mode for the DuckDB connection.
                        enum:
                            - read
                            - readwrite
                        type: string
                    path:
                        description: Path to external DuckDB database
                        type: string
                    pool_size:
                        description: Number of concurrent connections and queries allowed
                        minimum: 1
                        type: integer
                    read_write_ratio:
                        default: 0.5
                        description: Ratio of resources allocated to read vs write operations
                        maximum: 1
                        minimum: 0
                        type: number
                    schema_name:
                        description: Default schema used by the DuckDB database
                        type: string
                  required:
                    - driver
                  title: DuckDB
                  type: object
                - examples:
                    - driver: duckdb
                      mode: read
                      path: /path/to/my-duckdb-database.db
                      type: connector
                  properties:
                    driver:
                        const: duckdb
                        description: Refers to the driver type and must be driver `duckdb`
                        type: string
                    mode:
                        default: read
                        description: Set the mode for the DuckDB connection.
                        enum:
                            - read
                            - readwrite
                        type: string
                    path:
                        description: Path to the DuckDB database
                        type: string
                  required:
                    - driver
                    - db
                  title: External DuckDB
                  type: object
                - examples:
                    - driver: gcs
                      google_application_credentials: credentialjsonstring
                      type: connector
                  properties:
                    allow_host_access:
                        description: Allow access to host environment configuration
                        type: boolean
                    driver:
                        const: gcs
                        description: Refers to the driver type and must be driver `gcs`
                        type: string
                    google_application_credentials:
                        description: Google Cloud credentials JSON string
                        type: string
                    key_id:
                        description: Optional S3-compatible Key ID when used in compatibility mode
                        type: string
                    path_prefixes:
                        description: "A list of bucket path prefixes that this connector is allowed to access. \nUseful when different buckets or bucket prefixes use different credentials, \nallowing the system to select the appropriate connector based on the bucket path.\nExample: `gs://my-bucket/`, ` gs://my-bucket/path/` ,`gs://my-bucket/path/prefix`\n"
                        type:
                            - string
                            - array
                    secret:
                        description: Optional S3-compatible Secret when used in compatibility mode
                        type: string
                  required:
                    - driver
                    - bucket
                  title: GCS
                  type: object
                - examples:
                    - driver: https
                      headers:
                        Authorization: Bearer my-token
                      type: connector
                  properties:
                    driver:
                        const: https
                        description: Refers to the driver type and must be driver `https`
                        type: string
                    headers:
                        additionalProperties:
                            type: string
                        description: HTTP headers to include in the request
                        type: object
                    path_prefixes:
                        description: |
                            A list of HTTP/HTTPS URL prefixes that this connector is allowed to access.
                            Useful when different URL namespaces use different credentials, enabling the
                            system to choose the appropriate connector based on the URL path.
                            Example: `https://example.com/`, ` https://example.com/path/` ,`https://example.com/path/prefix`
                        type:
                            - string
                            - array
                  required:
                    - driver
                    - path
                  title: HTTPS
                  type: object
                - examples:
                    - driver: duckdb
                      path: md:my_database
                      schema_name: my_schema
                      token: '{{ .env.connector.motherduck.token }}'
                      type: connector
                  properties:
                    create_secrets_from_connectors:
                        description: List of connector names for which temporary secrets should be created before executing the SQL.
                        type:
                            - string
                            - array
                    driver:
                        const: duckdb
                        description: Refers to the driver type and must be driver `duckdb`.
                        type: string
                    init_sql:
                        description: SQL executed during database initialization.
                        type: string
                    mode:
                        default: read
                        description: Set the mode for the MotherDuck connection. By default, it is set to 'read' which allows only read operations. Set to 'readwrite' to enable model creation and table mutations.
                        enum:
                            - read
                            - readwrite
                        type: string
                    path:
                        description: Path to your MD database
                        type: string
                    schema_name:
                        description: Define your schema if not main, uses main by default
                        type: string
                    token:
                        description: MotherDuck token
                        type: string
                  required:
                    - driver
                    - path
                    - token
                  title: MotherDuck
                  type: object
                - examples:
                    - database: mydb
                      driver: mysql
                      host: localhost
                      password: p@ss
                      port: 3306
                      ssl-mode: preferred
                      type: connector
                      user: user
                    - driver: mysql
                      dsn: mysql://user:p%40ss@localhost:3306/mydb?ssl-mode=preferred
                      type: connector
                  properties:
                    database:
                        description: Name of the MySQL database
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `mysql`
                        type: string
                    dsn:
                        description: |
                            **Data Source Name (DSN)** for the MySQL connection, provided in [MySQL URI format](https://dev.mysql.com/doc/refman/8.4/en/connecting-using-uri-or-key-value-pairs.html#connecting-using-uri).
                            The DSN must follow the standard MySQL URI scheme:
                            ```text
                            mysql://user:password@host:3306/my-db
                            ```
                            Rules for special characters in password:
                            - The following characters are allowed [unescaped in the URI](https://datatracker.ietf.org/doc/html/rfc3986#section-2.3): `~` `.` `_` `-`
                            - All other special characters must be percent-encoded (`%XX` format).
                            ```text
                            mysql://user:pa%40ss@localhost:3306/my-db   # password contains '@'
                            mysql://user:pa%3Ass@localhost:3306/my-db   # password contains ':'
                            ```
                        type: string
                    host:
                        description: Hostname of the MySQL server
                        type: string
                    password:
                        description: Password for authentication
                        type: string
                    port:
                        description: Port number for the MySQL server
                        type: integer
                    ssl-mode:
                        description: 'ssl mode options: `disabled`, `preferred`, or `required`.'
                        type: string
                    user:
                        description: Username for authentication
                        type: string
                  required:
                    - driver
                  title: MySQL
                  type: object
                - examples:
                    - api_key: my-api-key
                      api_type: openai
                      api_version: "2023-05-15"
                      base_url: https://api.openai.com/v1
                      driver: openai
                      model: gpt-4o
                      type: connector
                  properties:
                    api_key:
                        description: API key for connecting to OpenAI
                        type: string
                    api_type:
                        description: The type of OpenAI API to use
                        type: string
                    api_version:
                        description: The version of the OpenAI API to use (e.g., '2023-05-15'). Required when API Type is AZURE or AZURE_AD
                        type: string
                    base_url:
                        description: The base URL for the OpenAI API (e.g., 'https://api.openai.com/v1')
                        type: string
                    driver:
                        description: The driver type, must be set to "openai"
                        type: string
                    model:
                        description: The OpenAI model to use (e.g., 'gpt-4o')
                        type: string
                  required:
                    - api_key
                  title: OpenAI
                  type: object
                - examples:
                    - broker_host: localhost
                      broker_port: 9000
                      controller_host: localhost
                      controller_port: 9000
                      driver: pinot
                      log_queries: true
                      max_open_conns: 100
                      password: mypassword
                      ssl: true
                      type: connector
                      username: myusername
                  properties:
                    broker_host:
                        description: Hostname of the Pinot broker
                        type: string
                    broker_port:
                        description: Port number for the Pinot broker
                        type: integer
                    controller_host:
                        description: Hostname of the Pinot controller
                        type: string
                    controller_port:
                        description: Port number for the Pinot controller
                        type: integer
                    driver:
                        description: Refers to the driver type and must be driver `pinot`
                        type: string
                    dsn:
                        description: DSN(Data Source Name) for the Pinot connection
                        type: string
                    log_queries:
                        description: Log raw SQL queries executed through Pinot
                        type: boolean
                    max_open_conns:
                        description: Maximum number of open connections to the Pinot database
                        type: integer
                    password:
                        description: Password for authenticating with Pinot
                        type: string
                    ssl:
                        description: Enable SSL connection to Pinot
                        type: boolean
                    username:
                        description: Username for authenticating with Pinot
                        type: string
                  required:
                    - driver
                    - dsn
                    - broker_host
                    - controller_host
                  title: Pinot
                  type: object
                - examples:
                    - dbname: mydatabase
                      driver: postgres
                      host: localhost
                      password: mypassword
                      port: 5432
                      sslmode: prefer
                      type: connector
                      user: myusername
                    - driver: postgres
                      dsn: user=myusername password='my pass\'word' host=localhost port=5432 dbname=mydatabase sslmode=prefer
                      type: connector
                    - driver: postgres
                      dsn: postgres://myusername:p%40ss@localhost:5432/mydatabase?sslmode=prefer
                      type: connector
                  properties:
                    dbname:
                        description: Name of the Postgres database
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `postgres`
                        type: string
                    dsn:
                        description: |
                            **Data Source Name (DSN)** for the PostgreSQL connection, provided in
                            [PostgreSQL connection string format](https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING).
                            PostgreSQL supports both **key=value format** and **URI format**.

                            key=value format example:
                            ```text
                            user=user password=password host=host port=5432 dbname=mydb
                            ```
                            Rules for key=value format for special characters:
                            - To write an empty value, or a value containing spaces, `=`, single quotes, or backslashes, surround it with single quotes.
                            - Single quotes and backslashes inside a value must be escaped with a backslash (`\'` and `\\`).

                            URI format example:
                            ```text
                            postgres://user:password@host:5432/mydb
                            ```

                            Rules for URI format:
                            - The following characters are allowed [unescaped in the URI](https://datatracker.ietf.org/doc/html/rfc3986#section-2.3): `~` `.` `_` `-`
                            - All other special characters must be percent-encoded (`%XX` format).

                            Examples (URI format with encoded characters):
                            ```text
                            postgres://user:pa%40ss@localhost:5432/my-db   # '@' is encoded as %40
                            postgres://user:pa%3Ass@localhost:5432/my-db   # ':' is encoded as %3A
                            ```
                        type: string
                    host:
                        description: Hostname of the Postgres server
                        type: string
                    password:
                        description: Password for authentication
                        type: string
                    port:
                        description: Port number for the Postgres server
                        type: string
                    sslmode:
                        description: 'ssl mode options: `disable`, `allow`, `prefer` or `require`.'
                        type: string
                    user:
                        description: Username for authentication
                        type: string
                  required:
                    - driver
                  title: Postgres
                  type: object
                - examples:
                    - aws_access_key_id: my-access-key-id
                      aws_access_token: my-access-token
                      aws_secret_access_key: my-secret-access-key
                      cluster_identifier: my-cluster-identifier
                      database: mydatabase
                      driver: redshift
                      region: us-east-1
                      type: connector
                      workgroup: my-workgroup
                  properties:
                    aws_access_key_id:
                        description: AWS Access Key ID used for authenticating with Redshift.
                        type: string
                    aws_access_token:
                        description: AWS Session Token for temporary credentials (optional).
                        type: string
                    aws_secret_access_key:
                        description: AWS Secret Access Key used for authenticating with Redshift.
                        type: string
                    cluster_identifier:
                        description: Cluster identifier for provisioned Redshift clusters, in case of Redshift Serverless use 'workgroup' .
                        type: string
                    database:
                        description: Name of the Redshift database to query.
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `redshift`
                        type: string
                    region:
                        description: AWS region where the Redshift cluster or workgroup is hosted (e.g., 'us-east-1').
                        type: string
                    workgroup:
                        description: Workgroup name for Redshift Serverless, in case of provisioned Redshift clusters use 'cluster_identifier'.
                        type: string
                  required:
                    - driver
                    - aws_access_key_id
                    - aws_secret_access_key
                    - database
                  title: Redshift
                  type: object
                - examples:
                    - aws_access_key_id: my-access-key-id
                      aws_access_token: my-access-token
                      aws_secret_access_key: my-secret-access-key
                      driver: s3
                      endpoint: https://my-s3-endpoint.com
                      region: us-east-1
                      type: connector
                  properties:
                    allow_host_access:
                        description: Allow access to host environment configuration
                        type: boolean
                    aws_access_key_id:
                        description: AWS Access Key ID used for authentication
                        type: string
                    aws_access_token:
                        description: Optional AWS session token for temporary credentials
                        type: string
                    aws_secret_access_key:
                        description: AWS Secret Access Key used for authentication
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `s3`
                        type: string
                    endpoint:
                        description: Optional custom endpoint URL for S3-compatible storage
                        type: string
                    path_prefixes:
                        description: |
                            A list of bucket path prefixes that this connector is allowed to access.
                            Useful when different buckets or bucket prefixes use different credentials,
                            allowing the system to select the appropriate connector based on the bucket path.
                            Example: `s3://my-bucket/`, ` s3://my-bucket/path/` ,`s3://my-bucket/path/prefix`
                        type:
                            - string
                            - array
                    region:
                        description: AWS region of the S3 bucket
                        type: string
                  required:
                    - driver
                    - bucket
                  title: S3
                  type: object
                - examples:
                    - client_id: my-client-id
                      driver: salesforce
                      endpoint: https://login.salesforce.com
                      password: mypassword
                      type: connector
                      username: myusername
                  properties:
                    client_id:
                        description: Client ID used for Salesforce OAuth authentication
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `salesforce`
                        type: string
                    endpoint:
                        description: Salesforce API endpoint URL
                        type: string
                    key:
                        description: Authentication key for Salesforce (secret)
                        type: string
                    password:
                        description: Salesforce account password (secret)
                        type: string
                    username:
                        description: Salesforce account username
                        type: string
                  required:
                    - driver
                    - username
                    - endpoint
                    - client_id
                  title: Salesforce
                  type: object
                - examples:
                    - bot_token: xoxb-my-bot-token
                      driver: slack
                      type: connector
                  properties:
                    bot_token:
                        description: Bot token used for authenticating Slack API requests
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `slack`
                        type: string
                  required:
                    - driver
                    - bot_token
                  title: Slack
                  type: object
                - examples:
                    - account: my_account_identifier
                      database: my_db
                      driver: snowflake
                      parallel_fetch_limit: 2
                      privateKey: '{{ .env.SNOWFLAKE_PRIVATE_KEY }}'
                      role: my_role
                      schema: my_schema
                      type: connector
                      user: my_user
                      warehouse: my_wh
                    - driver: snowflake
                      dsn: '{{ .env.SNOWFLAKE_DSN }}'
                      parallel_fetch_limit: 2
                      type: connector
                  properties:
                    account:
                        description: Snowflake account identifier. To find your Snowflake account identifier, look at your Snowflake account URL. The account identifier is everything before .snowflakecomputing.com
                        type: string
                    authenticator:
                        description: Optional authenticator type (e.g., SNOWFLAKE_JWT).
                        type: string
                    database:
                        description: Name of the Snowflake database.
                        type: string
                    driver:
                        const: snowflake
                        description: Refers to the driver type and must be driver `snowflake`
                        type: string
                    dsn:
                        description: "DSN (Data Source Name) for the Snowflake connection.\n\nThis is intended for **advanced configuration** where you want to specify\nproperties that are not explicitly defined above.  \nIt can only be used when the other connection fields (account, user, password,\ndatabase, schema, warehouse, role, authenticator, privateKey) are **not used**.\n\nFor details on private key generation and encoding, see the `privateKey` property.\n"
                        type: string
                    parallel_fetch_limit:
                        description: Maximum number of concurrent fetches during query execution.
                        type: integer
                    password:
                        description: Password for the Snowflake connection. _(deprecated, use privateKey instead)_
                        type: string
                    privateKey:
                        description: |
                            Private key for JWT authentication.
                            :::tip
                            Private key must be generated as a **PKCS#8 (nocrypt) key**, since the Snowflake Go driver
                            only supports unencrypted private keys. After generating, it must be **base64 URL encoded**.

                            Example commands to generate and encode:

                            ```bash
                            # Generate a 2048-bit unencrypted PKCS#8 private key
                            openssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out rsa_key.p8 -nocrypt

                            # Convert URL safe format for Snowflake
                            cat rsa_key.p8 | grep -v "\----" | tr -d '\n' | tr '+/' '-_'
                            ```
                            See: https://docs.snowflake.com/en/guide/key-pair-auth
                            :::
                        type: string
                    role:
                        description: Snowflake role to use.
                        type: string
                    schema:
                        description: Schema within the database to use.
                        type: string
                    user:
                        description: Username for the Snowflake connection.
                        type: string
                    warehouse:
                        description: Compute warehouse to use for queries.
                        type: string
                  required:
                    - type
                    - driver
                  title: Snowflake
                  type: object
                - examples:
                    - driver: sqlite
                      dsn: file:mydatabase.db
                      type: connector
                  properties:
                    driver:
                        description: Refers to the driver type and must be driver `sqlite`
                        type: string
                    dsn:
                        description: DSN(Data Source Name) for the sqlite connection
                        type: string
                  required:
                    - driver
                    - dsn
                  title: SQLite
                  type: object
        description: |
            Connector YAML files define how Rill connects to external data sources and OLAP engines. Each connector specifies a driver type and its required connection parameters.

            ## Available Connector Types

            ### _OLAP Engines_
            - [**ClickHouse**](#clickhouse) - ClickHouse analytical database
            - [**Druid**](#druid) - Apache Druid
            - [**DuckDB**](#duckdb) - Embedded DuckDB engine (default)
            - [**MotherDuck**](#motherduck) - MotherDuck cloud database
            - [**Pinot**](#pinot) - Apache Pinot

            ### _Data Warehouses_
            - [**Athena**](#athena) - Amazon Athena
            - [**BigQuery**](#bigquery) - Google BigQuery
            - [**Redshift**](#redshift) - Amazon Redshift
            - [**Snowflake**](#snowflake) - Snowflake data warehouse

            ### _Databases_
            - [**MySQL**](#mysql) - MySQL databases
            - [**PostgreSQL**](#postgres) - PostgreSQL databases
            - [**SQLite**](#sqlite) - SQLite databases

            ### _Cloud Storage_
            - [**Azure**](#azure) - Azure Blob Storage
            - [**GCS**](#gcs) - Google Cloud Storage
            - [**S3**](#s3) - Amazon S3 storage

            ### _Other_
            - [**Extenral DuckDB**](#external-duckdb) - External DuckDB database
            - [**HTTPS**](#https) - Public files via HTTP/HTTPS
            - [**OpenAI**](#openai) - OpenAI data
            - [**Salesforce**](#salesforce) - Salesforce data
            - [**Slack**](#slack) - Slack data

            :::warning Security Recommendation
            For all credential parameters (passwords, tokens, keys), use environment variables with the syntax `{{.env.connector.<connector_driver>.<parameter_name>}}`. This keeps sensitive data out of your YAML files and version control. See our [credentials documentation](/developers/build/connectors/credentials/) for complete setup instructions.
            :::
        id: connectors
        title: Connector YAML
        type: object
        ```

        ## Examples

        ### DuckDB: Managed

        Explicit:

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb
        managed: true
        ```

        or relying on defaults:

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb
        ```

        ### DuckDB: With init_sql for S3 secrets

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb

        init_sql: |
          CREATE SECRET IF NOT EXISTS s3 (
            TYPE S3,
            KEY_ID '{{ .env.aws_access_key_id }}',
            SECRET '{{ .env.aws_secret_access_key }}',
            REGION 'us-east-1'
          )
        ```

        This is now deprecated in favor of creating a dedicated `s3.yaml` connector file, which Rill will automatically load and create as a secret in DuckDB.

        ### DuckDB: With extensions

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb

        init_sql: |
          INSTALL spatial;
          LOAD spatial;
        ```

        ### DuckDB: MotherDuck database with existing tables

        ```yaml
        # connectors/motherduck.yaml
        type: connector
        driver: duckdb
        path: "md:my_database"
        token: "{{ .env.motherduck_token }}"
        schema_name: main
        mode: read
        ```

        ### ClickHouse: Cloud with SSL

        ```yaml
        # connectors/clickhouse.yaml
        type: connector
        driver: clickhouse
        host: "abc123.us-east-1.aws.clickhouse.cloud"
        port: 8443
        username: "default"
        password: "{{ .env.clickhouse_password }}"
        database: "default"
        ssl: true
        ```

        ### ClickHouse: Readwrite with cluster

        ```yaml
        # connectors/clickhouse.yaml
        type: connector
        driver: clickhouse
        mode: readwrite

        host: "{{ .env.clickhouse_host }}"
        port: 9440
        username: "{{ .env.clickhouse_user }}"
        password: "{{ .env.clickhouse_password }}"
        database: "default"
        cluster: "my_cluster"
        ssl: true
        ```

        ### ClickHouse: Dev/prod configuration

        ```yaml
        # connectors/clickhouse.yaml
        type: connector
        driver: clickhouse
        mode: readwrite

        dev:
          managed: true

        prod:
          host: "{{ .env.clickhouse_host }}"
          port: 9440
          username: "{{ .env.clickhouse_user }}"
          password: "{{ .env.clickhouse_password }}"
          database: "default"
          cluster: "{{ .env.clickhouse_cluster }}"
          ssl: true
        ```

        ### S3: Basic with credentials and region

        ```yaml
        # connectors/s3.yaml
        type: connector
        driver: s3
        aws_access_key_id: "{{ .env.aws_access_key_id }}"
        aws_secret_access_key: "{{ .env.aws_secret_access_key }}"
        region: us-west-2
        ```

        ### S3: Cloudflare R2 (S3-compatible)

        ```yaml
        # connectors/r2.yaml
        type: connector
        driver: s3
        aws_access_key_id: "{{ .env.r2_access_key_id }}"
        aws_secret_access_key: "{{ .env.r2_secret_access_key }}"
        endpoint: "https://{{ .env.r2_account_id }}.r2.cloudflarestorage.com"
        region: auto
        ```

        ### GCS: Minimal (uses default credentials)

        ```yaml
        # connectors/gcs.yaml
        type: connector
        driver: gcs
        ```

        ### GCS: With explicit credentials

        ```yaml
        # connectors/gcs.yaml
        type: connector
        driver: gcs
        google_application_credentials: "{{ .env.gcs_service_account_json }}"
        ```

        ### BigQuery

        ```yaml
        # connectors/bigquery.yaml
        type: connector
        driver: bigquery
        project_id: "my-gcp-project"
        google_application_credentials: "{{ .env.bigquery_service_account_json }}"
        ```

        ### Snowflake: Basic with DSN

        ```yaml
        # connectors/snowflake.yaml
        type: connector
        driver: snowflake
        dsn: "{{ .env.snowflake_dsn }}"
        ```

        ### Postgres

        ```yaml
        # connectors/postgres.yaml
        type: connector
        driver: postgres
        host: "{{ .env.postgres_host }}"
        port: 5432
        user: "{{ .env.postgres_user }}"
        password: "{{ .env.postgres_password }}"
        dbname: "analytics"
        sslmode: require
        ```

        ### Druid: Host-based

        ```yaml
        # connectors/druid.yaml
        type: connector
        driver: druid
        host: "{{ .env.druid_host }}"
        port: 8888
        username: "{{ .env.druid_user }}"
        password: "{{ .env.druid_password }}"
        ssl: true
        ```

        ### Redshift: Serverless

        ```yaml
        # connectors/redshift.yaml
        type: connector
        driver: redshift
        aws_access_key_id: "{{ .env.aws_access_key_id }}"
        aws_secret_access_key: "{{ .env.aws_secret_access_key }}"
        workgroup: "my-workgroup"
        region: us-east-1
        database: "analytics"
        ```

        ### OpenAI

        ```yaml
        # connectors/openai.yaml
        type: connector
        driver: openai
        api_key: "{{ .env.openai_api_key }}"
        ```

        ### Slack

        ```yaml
        # connectors/slack.yaml
        type: connector
        driver: slack
        bot_token: "{{ .env.slack_bot_token }}"
        ```

        ### Pinot

        ```yaml
        # connectors/pinot.yaml
        type: connector
        driver: pinot
        broker_host: "{{ .env.pinot_broker_host }}"
        controller_host: "{{ .env.pinot_controller_host }}"
        username: "{{ .env.pinot_user }}"
        password: "{{ .env.pinot_password }}"
        ssl: true
        ```
    - role: user
      content_type: text
      content: |-
        You should develop a Rill project file based on the following task description: Create a connector for the public ClickHouse playground database at play.clickhouse.com:9440. Use the 'clickhouse' driver. Set the username to 'play' and leave the password blank. This connector will be used as an OLAP connector for dashboards. Set 'mode' to 'read' to prevent writes. Name the connector 'clickhouse'.

        Here is some important context:
        - You should develop the file at path: /connectors/clickhouse.yaml
        - The file should be of type: connector
        - You are running as a sub-agent of a larger developer agent. Stay aligned on your specific task, avoid extra discovery, and report back your success or failure to the parent agent.

        Here is some additional context that may or may not be relevant to your task:
        - The project's default OLAP connector is named duckdb (driver: duckdb).
    - role: assistant
      content_type: tool_call
      id: b2d9775e2c09499b
      tool_name: write_file
      content: '{"contents":"type: connector\ndriver: clickhouse\nhost: play.clickhouse.com\nport: 9440\nusername: play\npassword: \"\"\nmode: read\n","path":"/connectors/clickhouse.yaml"}'
    - role: tool
      content_type: tool_response
      id: b2d9775e2c09499b
      content: '{"diff":"--- /connectors/clickhouse.yaml\n+++ /connectors/clickhouse.yaml\n@@ -1 +1,8 @@\n+type: connector\n+driver: clickhouse\n+host: play.clickhouse.com\n+port: 9440\n+username: play\n+password: \"\"\n+mode: read\n \n","is_new_file":true,"resources":[{"kind":"rill.runtime.v1.Connector","name":"clickhouse","reconcile_error":"handshake failed (this usually happens due to SSL/TLS mismatch): [handshake] unexpected packet [21] from server","reconcile_status":"RECONCILE_STATUS_IDLE"}]}'
  response:
    - role: assistant
      content_type: text
      content: |-
        The connector file /connectors/clickhouse.yaml was created as specified:

        - type: connector
        - driver: clickhouse
        - host: play.clickhouse.com
        - port: 9440
        - username: play
        - password: "" (blank)
        - mode: read

        The file was written successfully. However, there is a reconcile error indicating a handshake failure, likely due to an SSL/TLS mismatch. If SSL is required for this endpoint, you may need to add ssl: true to the configuration. Please advise if you want to update the file to include SSL.
- index: 5
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. Non-materialized models become views, which re-execute on every query.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: user
      content_type: text
      content: |-
        This may not relate to the user's task, but for context, the project's default OLAP connector is named duckdb (driver: duckdb).
         The default OLAP is in readwrite mode, so you can use it in models if you want.


        Task: Clickhouse has a public playground database on play.clickhouse.com:9440 (username `play` and no password). Can you set it up as the OLAP connector and develop a dashboard based on one of its tables? Please proceed without asking clarifying questions.
    - role: assistant
      content_type: tool_call
      id: 5f915383ab074f81
      tool_name: develop_file
      content: '{"path":"connectors/clickhouse.yaml","prompt":"Create a connector for the public ClickHouse playground database at play.clickhouse.com:9440. Use the ''clickhouse'' driver. Set the username to ''play'' and leave the password blank. This connector will be used as an OLAP connector for dashboards. Set ''mode'' to ''read'' to prevent writes. Name the connector ''clickhouse''.","type":"connector"}'
    - role: tool
      content_type: tool_response
      id: 5f915383ab074f81
      content: '{"response":"The connector file /connectors/clickhouse.yaml was created as specified:\n\n- type: connector\n- driver: clickhouse\n- host: play.clickhouse.com\n- port: 9440\n- username: play\n- password: \"\" (blank)\n- mode: read\n\nThe file was written successfully. However, there is a reconcile error indicating a handshake failure, likely due to an SSL/TLS mismatch. If SSL is required for this endpoint, you may need to add ssl: true to the configuration. Please advise if you want to update the file to include SSL."}'
  response:
    - role: assistant
      content_type: tool_call
      id: call_xVuAJFutWzNHjgulxK18vubT
      tool_name: develop_file
      content: '{"path":"connectors/clickhouse.yaml","prompt":"Update the ClickHouse connector configuration to include SSL support, as the public playground likely requires a secure connection. Add ''ssl: true'' to the YAML file. Ensure all other settings remain the same: driver clickhouse, host play.clickhouse.com, port 9440, username play, blank password, mode read.","type":"connector"}'
- index: 6
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. Non-materialized models become views, which re-execute on every query.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: system
      content_type: text
      content: |-
        # Instructions for developing a connector in Rill

        ## Introduction

        Connectors are resources that contain credentials and settings for connecting to external systems. They are typically found at the root of the project's DAG, providing access to data sources and services that power downstream resources like models and metrics views.

        Connectors are usually lightweight resources. When reconciled, they validate the connection to the external system but do not move or process data. The main exception is managed OLAP connectors (with `managed: true`), which trigger database provisioning.

        ### Driver capabilities

        Each connector uses a **driver** that implements one or more capabilities:

        - **OLAP database**: Can power metrics views and dashboards (e.g., `duckdb`, `clickhouse`)
        - **SQL database**: Can run SQL queries as model inputs (e.g., `postgres`, `bigquery`, `snowflake`)
        - **Information schema**: Can list tables and their schemas (e.g., `duckdb`, `bigquery`)
        - **Object store**: Can list, read, and write flat files (e.g., `s3`, `gcs`)
        - **Notifier**: Can send notifications and alerts (e.g., `slack`)
        - **AI**: Can generate embeddings or responses (e.g., `openai`)

        ## Core Concepts

        ### Naming conventions

        Connectors are typically named after their driver (e.g., a file `connectors/duckdb.yaml` creates a connector named `duckdb`). Use descriptive names when you have multiple connectors of the same type:
        - `connectors/s3_data.yaml` and `connectors/s3_staging.yaml`
        - `connectors/clickhouse_prod.yaml` and `connectors/clickhouse_dev.yaml`

        ### Secrets management

        Always store sensitive credentials in `.env` and reference them using template syntax:

        ```yaml
        type: connector
        driver: s3
        aws_access_key_id: "{{ .env.aws_access_key_id }}"
        aws_secret_access_key: "{{ .env.aws_secret_access_key }}"
        ```

        NOTE: Some legacy projects use the deprecated `.vars` instead of `.env`.

        ### Managed connectors

        OLAP connectors can be provisioned automatically by Rill using `managed: true`. This is supported for `duckdb` and `clickhouse` drivers:

        ```yaml
        type: connector
        driver: duckdb
        managed: true
        ```

        When a managed connector is reconciled, Rill provisions the database infrastructure. The user is billed for the CPU, memory, and disk usage of the provisioned database.

        ### Access modes

        Control read/write access using the `mode` property:

        - `mode: read`: Prevents Rill models from writing to this connector
        - `mode: readwrite`: Allows both reading and writing (default for managed connectors)

        Use `mode: read` when connecting to external databases with pre-existing tables to prevent unintended modifications.

        ### Dev/Prod configuration

        Use `dev:` and `prod:` blocks for environment-specific settings:

        ```yaml
        type: connector
        driver: clickhouse
        mode: readwrite

        # Use a local database in development to prevent overwriting data in the production cluster when iterating on changes.
        dev:
          managed: true

        prod:
          host: "{{ .env.clickhouse_host }}"
          port: 9440
          username: "{{ .env.clickhouse_user }}"
          password: "{{ .env.clickhouse_password }}"
          ssl: true
        ```

        ## Driver-Specific Notes

        ### DuckDB

        DuckDB is Rill's default embedded OLAP database. Key properties:

        - `managed: true`: Rill provisions and manages the database
        - `init_sql`: SQL to run at startup (install extensions, create secrets, attach databases)

        For MotherDuck (cloud DuckDB), use the `path` property with `md:` prefix:

        ```yaml
        type: connector
        driver: duckdb
        path: "md:my_database"
        token: "{{ .env.motherduck_token }}"
        ```

        ### ClickHouse

        ClickHouse can be user-managed or Rill-managed. Key properties:

        - `managed: true`: Rill provisions and manages an empty Clickhouse cluster. If set, don't set any other connector properties.
        - `host`, `port`, `username`, `password`: Connection credentials
        - `database`: Target database name
        - `ssl: true`: Required for ClickHouse Cloud
        - `cluster`: Cluster name for multi-node Clickhouse clusters
        - `dsn`: Alternative connection string format (format: `clickhouse://host:port?username=<username>&...`)

        Common ports:
        - `8443`: HTTPS native protocol (ClickHouse Cloud)
        - `9440`: Secure native protocol
        - `9000`: Native protocol (non-SSL)

        ### S3

        AWS S3 and S3-compatible storage. Key properties:

        - `aws_access_key_id`, `aws_secret_access_key`: AWS credentials
        - `region`: AWS region
        - `endpoint`: Custom endpoint for S3-compatible services (R2, MinIO, GCS interop)
        - `path_prefixes`: A list of bucket paths that the connector can access, e.g. `[s3://my-bucket]`; useful for improving bucket introspection

        ### GCS

        Google Cloud Storage. Key properties:

        - `google_application_credentials`: Service account JSON (must be a literal JSON string value)
        - `key_id`: HMAC key ID to use instead of a service account JSON; required for direct use with DuckDB and Clickhouse through S3 compatibility
        - `secret`: HMAC secret to use instead of a service account JSON; required for direct use with DuckDB and Clickhouse through S3 compatibility

        ### BigQuery

        Google BigQuery. Key properties:

        - `project_id`: GCP project ID
        - `google_application_credentials`: Service account JSON

        ### Snowflake

        Snowflake data warehouse. Key properties:

        - `account`, `user`, `privateKey`, `database`, `schema`, `warehouse`, `role`: Connection parameters
        - `dsn`: Connection string to use instead of separate connection parameters

        ### Postgres

        PostgreSQL database. Key properties:

        - `host`, `port`, `user`, `password`, `dbname`: Connection parameters
        - `sslmode`: SSL mode (`disable`, `require`, `verify-full`, etc.)

        ### Druid

        Apache Druid. Can be configured via host/port or DSN:

        - `host`, `port`, `username`, `password`, `ssl`: Direct connection
        - `dsn`: Full connection string

        ### Redshift

        Amazon Redshift. Key properties:

        - `aws_access_key_id`, `aws_secret_access_key`: AWS credentials
        - `workgroup`: Redshift Serverless workgroup name
        - `region`: AWS region
        - `database`: Database name

        ### Athena

        Amazon Athena. Key properties:

        - `aws_access_key_id`, `aws_secret_access_key`: AWS credentials
        - `workgroup`: Redshift Serverless workgroup name
        - `region`: AWS region
        - `output_location`: S3 path in format `s3://bucket/path` to store temporary query results in (Athena only)

        ### Other drivers

        - **Slack**: Use `bot_token` for alert notifications
        - **OpenAI**: Use `api_key` for AI-powered features
        - **HTTPS**: Simple connector for public HTTP sources
        - **Pinot**: Use `broker_host`, `controller_host`, `username`, `password`

        ## JSON Schema

        Here is a full JSON schema for the connector syntax:

        ```
        allOf:
            - properties:
                type:
                    const: connector
                    description: Refers to the resource type and must be `connector`
                    type: string
              required:
                - type
              title: Properties
              type: object
            - $ref: '#/definitions/common_properties'
            - oneOf:
                - examples:
                    - allow_host_access: true
                      aws_access_key_id: myawsaccesskey
                      aws_access_token: mytemporarytoken
                      aws_region: us-east-1
                      aws_secret_access_key: myawssecretkey
                      driver: athena
                      external_id: MyExternalID
                      output_location: s3://my-bucket/athena-output/
                      role_arn: arn:aws:iam::123456789012:role/MyRole
                      role_session_name: MySession
                      type: connector
                      workgroup: primary
                  properties:
                    allow_host_access:
                        description: Allow the Athena client to access host environment configurations such as environment variables or local AWS credential files. Defaults to true, enabling use of credentials and settings from the host environment unless explicitly disabled.
                        type: boolean
                    aws_access_key_id:
                        description: AWS Access Key ID used for authentication. Required when using static credentials directly or as base credentials for assuming a role.
                        type: string
                    aws_access_token:
                        description: AWS session token used with temporary credentials. Required only if the Access Key and Secret Key are part of a temporary session credentials.
                        type: string
                    aws_region:
                        description: AWS region where Athena and the result S3 bucket are located (e.g., us-east-1). Defaults to 'us-east-1' if not specified.
                        type: string
                    aws_secret_access_key:
                        description: AWS Secret Access Key paired with the Access Key ID. Required when using static credentials directly or as base credentials for assuming a role.
                        type: string
                    driver:
                        const: athena
                        description: Refers to the driver type and must be driver `athena`
                        type: string
                    external_id:
                        description: External ID required by some roles when assuming them, typically for cross-account access. Used only if 'role_arn' is specified and the role's trust policy requires it.
                        type: string
                    output_location:
                        description: S3 URI where Athena query results should be stored (e.g., s3://your-bucket/athena/results/). Optional if the selected workgroup has a default result configuration.
                        type: string
                    role_arn:
                        description: ARN of the IAM role to assume. When specified, the SDK uses the base credentials to call STS AssumeRole and obtain temporary credentials scoped to this role.
                        type: string
                    role_session_name:
                        description: Session name to associate with the STS AssumeRole session. Used only if 'role_arn' is specified. Useful for identifying and auditing the session.
                        type: string
                    workgroup:
                        description: Athena workgroup to use for query execution. Defaults to 'primary' if not specified.
                        type: string
                  required:
                    - driver
                  title: Athena
                  type: object
                - examples:
                    - azure_storage_account: mystorageaccount
                      azure_storage_key: credentialstring
                      driver: azure
                      type: connector
                  properties:
                    allow_host_access:
                        description: Allow access to host environment configuration
                        type: boolean
                    azure_storage_account:
                        description: Azure storage account name
                        type: string
                    azure_storage_connection_string:
                        description: Optional azure connection string for storage account
                        type: string
                    azure_storage_key:
                        description: Azure storage access key
                        type: string
                    azure_storage_sas_token:
                        description: Optional azure SAS token for authentication
                        type: string
                    driver:
                        const: azure
                        description: Refers to the driver type and must be driver `azure`
                        type: string
                    path_prefixes:
                        description: |
                            A list of container or virtual directory prefixes that this connector is allowed to access.
                            Useful when different containers or paths use different credentials, allowing the system
                            to route access through the appropriate connector based on the blob path.
                            Example: `azure://my-bucket/`, ` azure://my-bucket/path/` ,`azure://my-bucket/path/prefix`
                        type:
                            - string
                            - array
                  required:
                    - driver
                    - azure_storage_account
                    - azure_storage_key
                  title: Azure
                  type: object
                - examples:
                    - allow_host_access: true
                      driver: bigquery
                      google_application_credentials: credentialjsonstring
                      project_id: my-project-id
                      type: connector
                  properties:
                    allow_host_access:
                        description: Enable the BigQuery client to use credentials from the host environment when no service account JSON is provided. This includes Application Default Credentials from environment variables, local credential files, or the Google Compute Engine metadata server. Defaults to true, allowing seamless authentication in GCP environments.
                        type: boolean
                    dataset_id:
                        description: BigQuery dataset ID
                        type: string
                    driver:
                        const: bigquery
                        description: Refers to the driver type and must be driver `bigquery`
                        type: string
                    google_application_credentials:
                        description: Raw contents of the Google Cloud service account key (in JSON format) used for authentication.
                        type: string
                    location:
                        description: BigQuery dataset location
                        type: string
                    project_id:
                        description: Google Cloud project ID
                        type: string
                  required:
                    - driver
                  title: BigQuery
                  type: object
                - examples:
                    - cluster: mycluster
                      database: mydatabase
                      driver: clickhouse
                      host: localhost
                      managed: false
                      mode: readwrite
                      password: mypassword
                      port: 9000
                      ssl: true
                      type: connector
                      username: myusername
                  properties:
                    can_scale_to_zero:
                        description: Indicates if the database can scale to zero
                        type: boolean
                    cluster:
                        description: Cluster name, required for running distributed queries
                        type: string
                    conn_max_lifetime:
                        description: Maximum time a connection may be reused
                        type: string
                    database:
                        description: Name of the ClickHouse database within the cluster
                        type: string
                    dial_timeout:
                        description: Timeout for dialing the ClickHouse server
                        type: string
                    driver:
                        const: clickhouse
                        description: Refers to the driver type and must be driver `clickhouse`
                        type: string
                    dsn:
                        description: DSN(Data Source Name) for the ClickHouse connection
                        type: string
                    embed_port:
                        description: Port to run ClickHouse locally (0 for random port)
                        type: integer
                    host:
                        description: Host where the ClickHouse instance is running
                        type: string
                    log_queries:
                        description: Controls whether to log raw SQL queries
                        type: boolean
                    managed:
                        description: '`true` means Rill will provision the connector using the default provisioner. `false` disables automatic provisioning.'
                        type: boolean
                    max_idle_conns:
                        description: Maximum number of idle connections in the pool
                        type: integer
                    max_open_conns:
                        description: Maximum number of open connections to the database
                        type: integer
                    mode:
                        description: '`read` - Controls the operation mode for the ClickHouse connection. Defaults to ''read'' for safe operation with external databases. Set to ''readwrite'' to enable model creation and table mutations. Note: When ''managed: true'', this is automatically set to ''readwrite''.'
                        type: string
                    password:
                        description: Password for authentication
                        type: string
                    port:
                        description: Port where the ClickHouse instance is accessible
                        type: integer
                    query_settings:
                        description: query settings to be set on dashboard queries. `query_settings_override` takes precedence over these settings and if set these are ignored. Each setting must be separated by a comma. Example `max_threads = 8, max_memory_usage = 10000000000`
                        type: string
                    query_settings_override:
                        description: override the default settings used in queries. Changing the default settings can lead to incorrect query results and is generally not recommended. If you need to add settings, use `query_settings`
                        type: string
                    read_timeout:
                        description: Maximum time for a connection to read data
                        type: string
                    ssl:
                        description: Indicates whether a secured SSL connection is required
                        type: boolean
                    username:
                        description: Username for authentication
                        type: string
                  required:
                    - driver
                  title: ClickHouse
                  type: object
                - examples:
                    - driver: druid
                      host: localhost
                      password: mypassword
                      port: 8082
                      ssl: true
                      type: connector
                      username: myusername
                  properties:
                    driver:
                        const: druid
                        description: Refers to the driver type and must be driver `druid`
                        type: string
                    dsn:
                        description: Data Source Name (DSN) for connecting to Druid
                        type: string
                    host:
                        description: Hostname of the Druid coordinator or broker
                        type: string
                    log_queries:
                        description: Log raw SQL queries sent to Druid
                        type: boolean
                    max_open_conns:
                        description: Maximum number of open database connections (0 = default, -1 = unlimited)
                        type: integer
                    password:
                        description: Password for authenticating with Druid
                        type: string
                    port:
                        description: Port number of the Druid service
                        type: integer
                    skip_version_check:
                        description: Skip checking Druid version compatibility
                        type: boolean
                    ssl:
                        description: Enable SSL for secure connection
                        type: boolean
                    username:
                        description: Username for authenticating with Druid
                        type: string
                  required:
                    - driver
                    - dsn
                  title: Druid
                  type: object
                - examples:
                    - allow_host_access: true
                      cpu: 4
                      driver: duckdb
                      init_sql: INSTALL httpfs; LOAD httpfs;
                      log_queries: true
                      memory_limit_gb: 16
                      mode: readwrite
                      pool_size: 5
                      read_write_ratio: 0.7
                      type: connector
                  properties:
                    allow_host_access:
                        description: Whether access to local environment and file system is allowed
                        type: boolean
                    attach:
                        description: Full ATTACH statement to attach a DuckDB database
                        type: string
                    boot_queries:
                        description: Deprecated - Use init_sql instead
                        type: string
                    conn_init_sql:
                        description: SQL executed when a new connection is initialized
                        type: string
                    cpu:
                        description: Number of CPU cores available to the database
                        minimum: 1
                        type: integer
                    create_secrets_from_connectors:
                        description: List of connector names for which temporary secrets should be created before executing the SQL.
                        type:
                            - string
                            - array
                    database_name:
                        description: Name of the attached DuckDB database (auto-detected if not set)
                        type: string
                    driver:
                        const: duckdb
                        description: Must be "duckdb"
                        type: string
                    init_sql:
                        description: SQL executed during database initialization
                        type: string
                    log_queries:
                        description: Whether to log raw SQL queries executed through OLAP
                        type: boolean
                    memory_limit_gb:
                        description: Amount of memory in GB available to the database
                        minimum: 1
                        type: integer
                    mode:
                        default: read
                        description: Set the mode for the DuckDB connection.
                        enum:
                            - read
                            - readwrite
                        type: string
                    path:
                        description: Path to external DuckDB database
                        type: string
                    pool_size:
                        description: Number of concurrent connections and queries allowed
                        minimum: 1
                        type: integer
                    read_write_ratio:
                        default: 0.5
                        description: Ratio of resources allocated to read vs write operations
                        maximum: 1
                        minimum: 0
                        type: number
                    schema_name:
                        description: Default schema used by the DuckDB database
                        type: string
                  required:
                    - driver
                  title: DuckDB
                  type: object
                - examples:
                    - driver: duckdb
                      mode: read
                      path: /path/to/my-duckdb-database.db
                      type: connector
                  properties:
                    driver:
                        const: duckdb
                        description: Refers to the driver type and must be driver `duckdb`
                        type: string
                    mode:
                        default: read
                        description: Set the mode for the DuckDB connection.
                        enum:
                            - read
                            - readwrite
                        type: string
                    path:
                        description: Path to the DuckDB database
                        type: string
                  required:
                    - driver
                    - db
                  title: External DuckDB
                  type: object
                - examples:
                    - driver: gcs
                      google_application_credentials: credentialjsonstring
                      type: connector
                  properties:
                    allow_host_access:
                        description: Allow access to host environment configuration
                        type: boolean
                    driver:
                        const: gcs
                        description: Refers to the driver type and must be driver `gcs`
                        type: string
                    google_application_credentials:
                        description: Google Cloud credentials JSON string
                        type: string
                    key_id:
                        description: Optional S3-compatible Key ID when used in compatibility mode
                        type: string
                    path_prefixes:
                        description: "A list of bucket path prefixes that this connector is allowed to access. \nUseful when different buckets or bucket prefixes use different credentials, \nallowing the system to select the appropriate connector based on the bucket path.\nExample: `gs://my-bucket/`, ` gs://my-bucket/path/` ,`gs://my-bucket/path/prefix`\n"
                        type:
                            - string
                            - array
                    secret:
                        description: Optional S3-compatible Secret when used in compatibility mode
                        type: string
                  required:
                    - driver
                    - bucket
                  title: GCS
                  type: object
                - examples:
                    - driver: https
                      headers:
                        Authorization: Bearer my-token
                      type: connector
                  properties:
                    driver:
                        const: https
                        description: Refers to the driver type and must be driver `https`
                        type: string
                    headers:
                        additionalProperties:
                            type: string
                        description: HTTP headers to include in the request
                        type: object
                    path_prefixes:
                        description: |
                            A list of HTTP/HTTPS URL prefixes that this connector is allowed to access.
                            Useful when different URL namespaces use different credentials, enabling the
                            system to choose the appropriate connector based on the URL path.
                            Example: `https://example.com/`, ` https://example.com/path/` ,`https://example.com/path/prefix`
                        type:
                            - string
                            - array
                  required:
                    - driver
                    - path
                  title: HTTPS
                  type: object
                - examples:
                    - driver: duckdb
                      path: md:my_database
                      schema_name: my_schema
                      token: '{{ .env.connector.motherduck.token }}'
                      type: connector
                  properties:
                    create_secrets_from_connectors:
                        description: List of connector names for which temporary secrets should be created before executing the SQL.
                        type:
                            - string
                            - array
                    driver:
                        const: duckdb
                        description: Refers to the driver type and must be driver `duckdb`.
                        type: string
                    init_sql:
                        description: SQL executed during database initialization.
                        type: string
                    mode:
                        default: read
                        description: Set the mode for the MotherDuck connection. By default, it is set to 'read' which allows only read operations. Set to 'readwrite' to enable model creation and table mutations.
                        enum:
                            - read
                            - readwrite
                        type: string
                    path:
                        description: Path to your MD database
                        type: string
                    schema_name:
                        description: Define your schema if not main, uses main by default
                        type: string
                    token:
                        description: MotherDuck token
                        type: string
                  required:
                    - driver
                    - path
                    - token
                  title: MotherDuck
                  type: object
                - examples:
                    - database: mydb
                      driver: mysql
                      host: localhost
                      password: p@ss
                      port: 3306
                      ssl-mode: preferred
                      type: connector
                      user: user
                    - driver: mysql
                      dsn: mysql://user:p%40ss@localhost:3306/mydb?ssl-mode=preferred
                      type: connector
                  properties:
                    database:
                        description: Name of the MySQL database
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `mysql`
                        type: string
                    dsn:
                        description: |
                            **Data Source Name (DSN)** for the MySQL connection, provided in [MySQL URI format](https://dev.mysql.com/doc/refman/8.4/en/connecting-using-uri-or-key-value-pairs.html#connecting-using-uri).
                            The DSN must follow the standard MySQL URI scheme:
                            ```text
                            mysql://user:password@host:3306/my-db
                            ```
                            Rules for special characters in password:
                            - The following characters are allowed [unescaped in the URI](https://datatracker.ietf.org/doc/html/rfc3986#section-2.3): `~` `.` `_` `-`
                            - All other special characters must be percent-encoded (`%XX` format).
                            ```text
                            mysql://user:pa%40ss@localhost:3306/my-db   # password contains '@'
                            mysql://user:pa%3Ass@localhost:3306/my-db   # password contains ':'
                            ```
                        type: string
                    host:
                        description: Hostname of the MySQL server
                        type: string
                    password:
                        description: Password for authentication
                        type: string
                    port:
                        description: Port number for the MySQL server
                        type: integer
                    ssl-mode:
                        description: 'ssl mode options: `disabled`, `preferred`, or `required`.'
                        type: string
                    user:
                        description: Username for authentication
                        type: string
                  required:
                    - driver
                  title: MySQL
                  type: object
                - examples:
                    - api_key: my-api-key
                      api_type: openai
                      api_version: "2023-05-15"
                      base_url: https://api.openai.com/v1
                      driver: openai
                      model: gpt-4o
                      type: connector
                  properties:
                    api_key:
                        description: API key for connecting to OpenAI
                        type: string
                    api_type:
                        description: The type of OpenAI API to use
                        type: string
                    api_version:
                        description: The version of the OpenAI API to use (e.g., '2023-05-15'). Required when API Type is AZURE or AZURE_AD
                        type: string
                    base_url:
                        description: The base URL for the OpenAI API (e.g., 'https://api.openai.com/v1')
                        type: string
                    driver:
                        description: The driver type, must be set to "openai"
                        type: string
                    model:
                        description: The OpenAI model to use (e.g., 'gpt-4o')
                        type: string
                  required:
                    - api_key
                  title: OpenAI
                  type: object
                - examples:
                    - broker_host: localhost
                      broker_port: 9000
                      controller_host: localhost
                      controller_port: 9000
                      driver: pinot
                      log_queries: true
                      max_open_conns: 100
                      password: mypassword
                      ssl: true
                      type: connector
                      username: myusername
                  properties:
                    broker_host:
                        description: Hostname of the Pinot broker
                        type: string
                    broker_port:
                        description: Port number for the Pinot broker
                        type: integer
                    controller_host:
                        description: Hostname of the Pinot controller
                        type: string
                    controller_port:
                        description: Port number for the Pinot controller
                        type: integer
                    driver:
                        description: Refers to the driver type and must be driver `pinot`
                        type: string
                    dsn:
                        description: DSN(Data Source Name) for the Pinot connection
                        type: string
                    log_queries:
                        description: Log raw SQL queries executed through Pinot
                        type: boolean
                    max_open_conns:
                        description: Maximum number of open connections to the Pinot database
                        type: integer
                    password:
                        description: Password for authenticating with Pinot
                        type: string
                    ssl:
                        description: Enable SSL connection to Pinot
                        type: boolean
                    username:
                        description: Username for authenticating with Pinot
                        type: string
                  required:
                    - driver
                    - dsn
                    - broker_host
                    - controller_host
                  title: Pinot
                  type: object
                - examples:
                    - dbname: mydatabase
                      driver: postgres
                      host: localhost
                      password: mypassword
                      port: 5432
                      sslmode: prefer
                      type: connector
                      user: myusername
                    - driver: postgres
                      dsn: user=myusername password='my pass\'word' host=localhost port=5432 dbname=mydatabase sslmode=prefer
                      type: connector
                    - driver: postgres
                      dsn: postgres://myusername:p%40ss@localhost:5432/mydatabase?sslmode=prefer
                      type: connector
                  properties:
                    dbname:
                        description: Name of the Postgres database
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `postgres`
                        type: string
                    dsn:
                        description: |
                            **Data Source Name (DSN)** for the PostgreSQL connection, provided in
                            [PostgreSQL connection string format](https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING).
                            PostgreSQL supports both **key=value format** and **URI format**.

                            key=value format example:
                            ```text
                            user=user password=password host=host port=5432 dbname=mydb
                            ```
                            Rules for key=value format for special characters:
                            - To write an empty value, or a value containing spaces, `=`, single quotes, or backslashes, surround it with single quotes.
                            - Single quotes and backslashes inside a value must be escaped with a backslash (`\'` and `\\`).

                            URI format example:
                            ```text
                            postgres://user:password@host:5432/mydb
                            ```

                            Rules for URI format:
                            - The following characters are allowed [unescaped in the URI](https://datatracker.ietf.org/doc/html/rfc3986#section-2.3): `~` `.` `_` `-`
                            - All other special characters must be percent-encoded (`%XX` format).

                            Examples (URI format with encoded characters):
                            ```text
                            postgres://user:pa%40ss@localhost:5432/my-db   # '@' is encoded as %40
                            postgres://user:pa%3Ass@localhost:5432/my-db   # ':' is encoded as %3A
                            ```
                        type: string
                    host:
                        description: Hostname of the Postgres server
                        type: string
                    password:
                        description: Password for authentication
                        type: string
                    port:
                        description: Port number for the Postgres server
                        type: string
                    sslmode:
                        description: 'ssl mode options: `disable`, `allow`, `prefer` or `require`.'
                        type: string
                    user:
                        description: Username for authentication
                        type: string
                  required:
                    - driver
                  title: Postgres
                  type: object
                - examples:
                    - aws_access_key_id: my-access-key-id
                      aws_access_token: my-access-token
                      aws_secret_access_key: my-secret-access-key
                      cluster_identifier: my-cluster-identifier
                      database: mydatabase
                      driver: redshift
                      region: us-east-1
                      type: connector
                      workgroup: my-workgroup
                  properties:
                    aws_access_key_id:
                        description: AWS Access Key ID used for authenticating with Redshift.
                        type: string
                    aws_access_token:
                        description: AWS Session Token for temporary credentials (optional).
                        type: string
                    aws_secret_access_key:
                        description: AWS Secret Access Key used for authenticating with Redshift.
                        type: string
                    cluster_identifier:
                        description: Cluster identifier for provisioned Redshift clusters, in case of Redshift Serverless use 'workgroup' .
                        type: string
                    database:
                        description: Name of the Redshift database to query.
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `redshift`
                        type: string
                    region:
                        description: AWS region where the Redshift cluster or workgroup is hosted (e.g., 'us-east-1').
                        type: string
                    workgroup:
                        description: Workgroup name for Redshift Serverless, in case of provisioned Redshift clusters use 'cluster_identifier'.
                        type: string
                  required:
                    - driver
                    - aws_access_key_id
                    - aws_secret_access_key
                    - database
                  title: Redshift
                  type: object
                - examples:
                    - aws_access_key_id: my-access-key-id
                      aws_access_token: my-access-token
                      aws_secret_access_key: my-secret-access-key
                      driver: s3
                      endpoint: https://my-s3-endpoint.com
                      region: us-east-1
                      type: connector
                  properties:
                    allow_host_access:
                        description: Allow access to host environment configuration
                        type: boolean
                    aws_access_key_id:
                        description: AWS Access Key ID used for authentication
                        type: string
                    aws_access_token:
                        description: Optional AWS session token for temporary credentials
                        type: string
                    aws_secret_access_key:
                        description: AWS Secret Access Key used for authentication
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `s3`
                        type: string
                    endpoint:
                        description: Optional custom endpoint URL for S3-compatible storage
                        type: string
                    path_prefixes:
                        description: |
                            A list of bucket path prefixes that this connector is allowed to access.
                            Useful when different buckets or bucket prefixes use different credentials,
                            allowing the system to select the appropriate connector based on the bucket path.
                            Example: `s3://my-bucket/`, ` s3://my-bucket/path/` ,`s3://my-bucket/path/prefix`
                        type:
                            - string
                            - array
                    region:
                        description: AWS region of the S3 bucket
                        type: string
                  required:
                    - driver
                    - bucket
                  title: S3
                  type: object
                - examples:
                    - client_id: my-client-id
                      driver: salesforce
                      endpoint: https://login.salesforce.com
                      password: mypassword
                      type: connector
                      username: myusername
                  properties:
                    client_id:
                        description: Client ID used for Salesforce OAuth authentication
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `salesforce`
                        type: string
                    endpoint:
                        description: Salesforce API endpoint URL
                        type: string
                    key:
                        description: Authentication key for Salesforce (secret)
                        type: string
                    password:
                        description: Salesforce account password (secret)
                        type: string
                    username:
                        description: Salesforce account username
                        type: string
                  required:
                    - driver
                    - username
                    - endpoint
                    - client_id
                  title: Salesforce
                  type: object
                - examples:
                    - bot_token: xoxb-my-bot-token
                      driver: slack
                      type: connector
                  properties:
                    bot_token:
                        description: Bot token used for authenticating Slack API requests
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `slack`
                        type: string
                  required:
                    - driver
                    - bot_token
                  title: Slack
                  type: object
                - examples:
                    - account: my_account_identifier
                      database: my_db
                      driver: snowflake
                      parallel_fetch_limit: 2
                      privateKey: '{{ .env.SNOWFLAKE_PRIVATE_KEY }}'
                      role: my_role
                      schema: my_schema
                      type: connector
                      user: my_user
                      warehouse: my_wh
                    - driver: snowflake
                      dsn: '{{ .env.SNOWFLAKE_DSN }}'
                      parallel_fetch_limit: 2
                      type: connector
                  properties:
                    account:
                        description: Snowflake account identifier. To find your Snowflake account identifier, look at your Snowflake account URL. The account identifier is everything before .snowflakecomputing.com
                        type: string
                    authenticator:
                        description: Optional authenticator type (e.g., SNOWFLAKE_JWT).
                        type: string
                    database:
                        description: Name of the Snowflake database.
                        type: string
                    driver:
                        const: snowflake
                        description: Refers to the driver type and must be driver `snowflake`
                        type: string
                    dsn:
                        description: "DSN (Data Source Name) for the Snowflake connection.\n\nThis is intended for **advanced configuration** where you want to specify\nproperties that are not explicitly defined above.  \nIt can only be used when the other connection fields (account, user, password,\ndatabase, schema, warehouse, role, authenticator, privateKey) are **not used**.\n\nFor details on private key generation and encoding, see the `privateKey` property.\n"
                        type: string
                    parallel_fetch_limit:
                        description: Maximum number of concurrent fetches during query execution.
                        type: integer
                    password:
                        description: Password for the Snowflake connection. _(deprecated, use privateKey instead)_
                        type: string
                    privateKey:
                        description: |
                            Private key for JWT authentication.
                            :::tip
                            Private key must be generated as a **PKCS#8 (nocrypt) key**, since the Snowflake Go driver
                            only supports unencrypted private keys. After generating, it must be **base64 URL encoded**.

                            Example commands to generate and encode:

                            ```bash
                            # Generate a 2048-bit unencrypted PKCS#8 private key
                            openssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out rsa_key.p8 -nocrypt

                            # Convert URL safe format for Snowflake
                            cat rsa_key.p8 | grep -v "\----" | tr -d '\n' | tr '+/' '-_'
                            ```
                            See: https://docs.snowflake.com/en/guide/key-pair-auth
                            :::
                        type: string
                    role:
                        description: Snowflake role to use.
                        type: string
                    schema:
                        description: Schema within the database to use.
                        type: string
                    user:
                        description: Username for the Snowflake connection.
                        type: string
                    warehouse:
                        description: Compute warehouse to use for queries.
                        type: string
                  required:
                    - type
                    - driver
                  title: Snowflake
                  type: object
                - examples:
                    - driver: sqlite
                      dsn: file:mydatabase.db
                      type: connector
                  properties:
                    driver:
                        description: Refers to the driver type and must be driver `sqlite`
                        type: string
                    dsn:
                        description: DSN(Data Source Name) for the sqlite connection
                        type: string
                  required:
                    - driver
                    - dsn
                  title: SQLite
                  type: object
        description: |
            Connector YAML files define how Rill connects to external data sources and OLAP engines. Each connector specifies a driver type and its required connection parameters.

            ## Available Connector Types

            ### _OLAP Engines_
            - [**ClickHouse**](#clickhouse) - ClickHouse analytical database
            - [**Druid**](#druid) - Apache Druid
            - [**DuckDB**](#duckdb) - Embedded DuckDB engine (default)
            - [**MotherDuck**](#motherduck) - MotherDuck cloud database
            - [**Pinot**](#pinot) - Apache Pinot

            ### _Data Warehouses_
            - [**Athena**](#athena) - Amazon Athena
            - [**BigQuery**](#bigquery) - Google BigQuery
            - [**Redshift**](#redshift) - Amazon Redshift
            - [**Snowflake**](#snowflake) - Snowflake data warehouse

            ### _Databases_
            - [**MySQL**](#mysql) - MySQL databases
            - [**PostgreSQL**](#postgres) - PostgreSQL databases
            - [**SQLite**](#sqlite) - SQLite databases

            ### _Cloud Storage_
            - [**Azure**](#azure) - Azure Blob Storage
            - [**GCS**](#gcs) - Google Cloud Storage
            - [**S3**](#s3) - Amazon S3 storage

            ### _Other_
            - [**Extenral DuckDB**](#external-duckdb) - External DuckDB database
            - [**HTTPS**](#https) - Public files via HTTP/HTTPS
            - [**OpenAI**](#openai) - OpenAI data
            - [**Salesforce**](#salesforce) - Salesforce data
            - [**Slack**](#slack) - Slack data

            :::warning Security Recommendation
            For all credential parameters (passwords, tokens, keys), use environment variables with the syntax `{{.env.connector.<connector_driver>.<parameter_name>}}`. This keeps sensitive data out of your YAML files and version control. See our [credentials documentation](/developers/build/connectors/credentials/) for complete setup instructions.
            :::
        id: connectors
        title: Connector YAML
        type: object
        ```

        ## Examples

        ### DuckDB: Managed

        Explicit:

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb
        managed: true
        ```

        or relying on defaults:

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb
        ```

        ### DuckDB: With init_sql for S3 secrets

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb

        init_sql: |
          CREATE SECRET IF NOT EXISTS s3 (
            TYPE S3,
            KEY_ID '{{ .env.aws_access_key_id }}',
            SECRET '{{ .env.aws_secret_access_key }}',
            REGION 'us-east-1'
          )
        ```

        This is now deprecated in favor of creating a dedicated `s3.yaml` connector file, which Rill will automatically load and create as a secret in DuckDB.

        ### DuckDB: With extensions

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb

        init_sql: |
          INSTALL spatial;
          LOAD spatial;
        ```

        ### DuckDB: MotherDuck database with existing tables

        ```yaml
        # connectors/motherduck.yaml
        type: connector
        driver: duckdb
        path: "md:my_database"
        token: "{{ .env.motherduck_token }}"
        schema_name: main
        mode: read
        ```

        ### ClickHouse: Cloud with SSL

        ```yaml
        # connectors/clickhouse.yaml
        type: connector
        driver: clickhouse
        host: "abc123.us-east-1.aws.clickhouse.cloud"
        port: 8443
        username: "default"
        password: "{{ .env.clickhouse_password }}"
        database: "default"
        ssl: true
        ```

        ### ClickHouse: Readwrite with cluster

        ```yaml
        # connectors/clickhouse.yaml
        type: connector
        driver: clickhouse
        mode: readwrite

        host: "{{ .env.clickhouse_host }}"
        port: 9440
        username: "{{ .env.clickhouse_user }}"
        password: "{{ .env.clickhouse_password }}"
        database: "default"
        cluster: "my_cluster"
        ssl: true
        ```

        ### ClickHouse: Dev/prod configuration

        ```yaml
        # connectors/clickhouse.yaml
        type: connector
        driver: clickhouse
        mode: readwrite

        dev:
          managed: true

        prod:
          host: "{{ .env.clickhouse_host }}"
          port: 9440
          username: "{{ .env.clickhouse_user }}"
          password: "{{ .env.clickhouse_password }}"
          database: "default"
          cluster: "{{ .env.clickhouse_cluster }}"
          ssl: true
        ```

        ### S3: Basic with credentials and region

        ```yaml
        # connectors/s3.yaml
        type: connector
        driver: s3
        aws_access_key_id: "{{ .env.aws_access_key_id }}"
        aws_secret_access_key: "{{ .env.aws_secret_access_key }}"
        region: us-west-2
        ```

        ### S3: Cloudflare R2 (S3-compatible)

        ```yaml
        # connectors/r2.yaml
        type: connector
        driver: s3
        aws_access_key_id: "{{ .env.r2_access_key_id }}"
        aws_secret_access_key: "{{ .env.r2_secret_access_key }}"
        endpoint: "https://{{ .env.r2_account_id }}.r2.cloudflarestorage.com"
        region: auto
        ```

        ### GCS: Minimal (uses default credentials)

        ```yaml
        # connectors/gcs.yaml
        type: connector
        driver: gcs
        ```

        ### GCS: With explicit credentials

        ```yaml
        # connectors/gcs.yaml
        type: connector
        driver: gcs
        google_application_credentials: "{{ .env.gcs_service_account_json }}"
        ```

        ### BigQuery

        ```yaml
        # connectors/bigquery.yaml
        type: connector
        driver: bigquery
        project_id: "my-gcp-project"
        google_application_credentials: "{{ .env.bigquery_service_account_json }}"
        ```

        ### Snowflake: Basic with DSN

        ```yaml
        # connectors/snowflake.yaml
        type: connector
        driver: snowflake
        dsn: "{{ .env.snowflake_dsn }}"
        ```

        ### Postgres

        ```yaml
        # connectors/postgres.yaml
        type: connector
        driver: postgres
        host: "{{ .env.postgres_host }}"
        port: 5432
        user: "{{ .env.postgres_user }}"
        password: "{{ .env.postgres_password }}"
        dbname: "analytics"
        sslmode: require
        ```

        ### Druid: Host-based

        ```yaml
        # connectors/druid.yaml
        type: connector
        driver: druid
        host: "{{ .env.druid_host }}"
        port: 8888
        username: "{{ .env.druid_user }}"
        password: "{{ .env.druid_password }}"
        ssl: true
        ```

        ### Redshift: Serverless

        ```yaml
        # connectors/redshift.yaml
        type: connector
        driver: redshift
        aws_access_key_id: "{{ .env.aws_access_key_id }}"
        aws_secret_access_key: "{{ .env.aws_secret_access_key }}"
        workgroup: "my-workgroup"
        region: us-east-1
        database: "analytics"
        ```

        ### OpenAI

        ```yaml
        # connectors/openai.yaml
        type: connector
        driver: openai
        api_key: "{{ .env.openai_api_key }}"
        ```

        ### Slack

        ```yaml
        # connectors/slack.yaml
        type: connector
        driver: slack
        bot_token: "{{ .env.slack_bot_token }}"
        ```

        ### Pinot

        ```yaml
        # connectors/pinot.yaml
        type: connector
        driver: pinot
        broker_host: "{{ .env.pinot_broker_host }}"
        controller_host: "{{ .env.pinot_controller_host }}"
        username: "{{ .env.pinot_user }}"
        password: "{{ .env.pinot_password }}"
        ssl: true
        ```
    - role: user
      content_type: text
      content: |-
        You should develop a Rill project file based on the following task description: Update the ClickHouse connector configuration to include SSL support, as the public playground likely requires a secure connection. Add 'ssl: true' to the YAML file. Ensure all other settings remain the same: driver clickhouse, host play.clickhouse.com, port 9440, username play, blank password, mode read.

        Here is some important context:
        - You should develop the file at path: /connectors/clickhouse.yaml
        - The file should be of type: connector
        - You are running as a sub-agent of a larger developer agent. Stay aligned on your specific task, avoid extra discovery, and report back your success or failure to the parent agent.

        Here is some additional context that may or may not be relevant to your task:
        - The project's default OLAP connector is named duckdb (driver: duckdb).
  response:
    - role: assistant
      content_type: tool_call
      id: call_bBw9dJIiSEfjkbBHNMQO4Arz
      tool_name: write_file
      content: '{"contents":"type: connector\ndriver: clickhouse\nhost: play.clickhouse.com\nport: 9440\nusername: play\npassword: \"\"\nmode: read\nssl: true\n","path":"/connectors/clickhouse.yaml"}'
- index: 7
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. Non-materialized models become views, which re-execute on every query.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: system
      content_type: text
      content: |-
        # Instructions for developing a connector in Rill

        ## Introduction

        Connectors are resources that contain credentials and settings for connecting to external systems. They are typically found at the root of the project's DAG, providing access to data sources and services that power downstream resources like models and metrics views.

        Connectors are usually lightweight resources. When reconciled, they validate the connection to the external system but do not move or process data. The main exception is managed OLAP connectors (with `managed: true`), which trigger database provisioning.

        ### Driver capabilities

        Each connector uses a **driver** that implements one or more capabilities:

        - **OLAP database**: Can power metrics views and dashboards (e.g., `duckdb`, `clickhouse`)
        - **SQL database**: Can run SQL queries as model inputs (e.g., `postgres`, `bigquery`, `snowflake`)
        - **Information schema**: Can list tables and their schemas (e.g., `duckdb`, `bigquery`)
        - **Object store**: Can list, read, and write flat files (e.g., `s3`, `gcs`)
        - **Notifier**: Can send notifications and alerts (e.g., `slack`)
        - **AI**: Can generate embeddings or responses (e.g., `openai`)

        ## Core Concepts

        ### Naming conventions

        Connectors are typically named after their driver (e.g., a file `connectors/duckdb.yaml` creates a connector named `duckdb`). Use descriptive names when you have multiple connectors of the same type:
        - `connectors/s3_data.yaml` and `connectors/s3_staging.yaml`
        - `connectors/clickhouse_prod.yaml` and `connectors/clickhouse_dev.yaml`

        ### Secrets management

        Always store sensitive credentials in `.env` and reference them using template syntax:

        ```yaml
        type: connector
        driver: s3
        aws_access_key_id: "{{ .env.aws_access_key_id }}"
        aws_secret_access_key: "{{ .env.aws_secret_access_key }}"
        ```

        NOTE: Some legacy projects use the deprecated `.vars` instead of `.env`.

        ### Managed connectors

        OLAP connectors can be provisioned automatically by Rill using `managed: true`. This is supported for `duckdb` and `clickhouse` drivers:

        ```yaml
        type: connector
        driver: duckdb
        managed: true
        ```

        When a managed connector is reconciled, Rill provisions the database infrastructure. The user is billed for the CPU, memory, and disk usage of the provisioned database.

        ### Access modes

        Control read/write access using the `mode` property:

        - `mode: read`: Prevents Rill models from writing to this connector
        - `mode: readwrite`: Allows both reading and writing (default for managed connectors)

        Use `mode: read` when connecting to external databases with pre-existing tables to prevent unintended modifications.

        ### Dev/Prod configuration

        Use `dev:` and `prod:` blocks for environment-specific settings:

        ```yaml
        type: connector
        driver: clickhouse
        mode: readwrite

        # Use a local database in development to prevent overwriting data in the production cluster when iterating on changes.
        dev:
          managed: true

        prod:
          host: "{{ .env.clickhouse_host }}"
          port: 9440
          username: "{{ .env.clickhouse_user }}"
          password: "{{ .env.clickhouse_password }}"
          ssl: true
        ```

        ## Driver-Specific Notes

        ### DuckDB

        DuckDB is Rill's default embedded OLAP database. Key properties:

        - `managed: true`: Rill provisions and manages the database
        - `init_sql`: SQL to run at startup (install extensions, create secrets, attach databases)

        For MotherDuck (cloud DuckDB), use the `path` property with `md:` prefix:

        ```yaml
        type: connector
        driver: duckdb
        path: "md:my_database"
        token: "{{ .env.motherduck_token }}"
        ```

        ### ClickHouse

        ClickHouse can be user-managed or Rill-managed. Key properties:

        - `managed: true`: Rill provisions and manages an empty Clickhouse cluster. If set, don't set any other connector properties.
        - `host`, `port`, `username`, `password`: Connection credentials
        - `database`: Target database name
        - `ssl: true`: Required for ClickHouse Cloud
        - `cluster`: Cluster name for multi-node Clickhouse clusters
        - `dsn`: Alternative connection string format (format: `clickhouse://host:port?username=<username>&...`)

        Common ports:
        - `8443`: HTTPS native protocol (ClickHouse Cloud)
        - `9440`: Secure native protocol
        - `9000`: Native protocol (non-SSL)

        ### S3

        AWS S3 and S3-compatible storage. Key properties:

        - `aws_access_key_id`, `aws_secret_access_key`: AWS credentials
        - `region`: AWS region
        - `endpoint`: Custom endpoint for S3-compatible services (R2, MinIO, GCS interop)
        - `path_prefixes`: A list of bucket paths that the connector can access, e.g. `[s3://my-bucket]`; useful for improving bucket introspection

        ### GCS

        Google Cloud Storage. Key properties:

        - `google_application_credentials`: Service account JSON (must be a literal JSON string value)
        - `key_id`: HMAC key ID to use instead of a service account JSON; required for direct use with DuckDB and Clickhouse through S3 compatibility
        - `secret`: HMAC secret to use instead of a service account JSON; required for direct use with DuckDB and Clickhouse through S3 compatibility

        ### BigQuery

        Google BigQuery. Key properties:

        - `project_id`: GCP project ID
        - `google_application_credentials`: Service account JSON

        ### Snowflake

        Snowflake data warehouse. Key properties:

        - `account`, `user`, `privateKey`, `database`, `schema`, `warehouse`, `role`: Connection parameters
        - `dsn`: Connection string to use instead of separate connection parameters

        ### Postgres

        PostgreSQL database. Key properties:

        - `host`, `port`, `user`, `password`, `dbname`: Connection parameters
        - `sslmode`: SSL mode (`disable`, `require`, `verify-full`, etc.)

        ### Druid

        Apache Druid. Can be configured via host/port or DSN:

        - `host`, `port`, `username`, `password`, `ssl`: Direct connection
        - `dsn`: Full connection string

        ### Redshift

        Amazon Redshift. Key properties:

        - `aws_access_key_id`, `aws_secret_access_key`: AWS credentials
        - `workgroup`: Redshift Serverless workgroup name
        - `region`: AWS region
        - `database`: Database name

        ### Athena

        Amazon Athena. Key properties:

        - `aws_access_key_id`, `aws_secret_access_key`: AWS credentials
        - `workgroup`: Redshift Serverless workgroup name
        - `region`: AWS region
        - `output_location`: S3 path in format `s3://bucket/path` to store temporary query results in (Athena only)

        ### Other drivers

        - **Slack**: Use `bot_token` for alert notifications
        - **OpenAI**: Use `api_key` for AI-powered features
        - **HTTPS**: Simple connector for public HTTP sources
        - **Pinot**: Use `broker_host`, `controller_host`, `username`, `password`

        ## JSON Schema

        Here is a full JSON schema for the connector syntax:

        ```
        allOf:
            - properties:
                type:
                    const: connector
                    description: Refers to the resource type and must be `connector`
                    type: string
              required:
                - type
              title: Properties
              type: object
            - $ref: '#/definitions/common_properties'
            - oneOf:
                - examples:
                    - allow_host_access: true
                      aws_access_key_id: myawsaccesskey
                      aws_access_token: mytemporarytoken
                      aws_region: us-east-1
                      aws_secret_access_key: myawssecretkey
                      driver: athena
                      external_id: MyExternalID
                      output_location: s3://my-bucket/athena-output/
                      role_arn: arn:aws:iam::123456789012:role/MyRole
                      role_session_name: MySession
                      type: connector
                      workgroup: primary
                  properties:
                    allow_host_access:
                        description: Allow the Athena client to access host environment configurations such as environment variables or local AWS credential files. Defaults to true, enabling use of credentials and settings from the host environment unless explicitly disabled.
                        type: boolean
                    aws_access_key_id:
                        description: AWS Access Key ID used for authentication. Required when using static credentials directly or as base credentials for assuming a role.
                        type: string
                    aws_access_token:
                        description: AWS session token used with temporary credentials. Required only if the Access Key and Secret Key are part of a temporary session credentials.
                        type: string
                    aws_region:
                        description: AWS region where Athena and the result S3 bucket are located (e.g., us-east-1). Defaults to 'us-east-1' if not specified.
                        type: string
                    aws_secret_access_key:
                        description: AWS Secret Access Key paired with the Access Key ID. Required when using static credentials directly or as base credentials for assuming a role.
                        type: string
                    driver:
                        const: athena
                        description: Refers to the driver type and must be driver `athena`
                        type: string
                    external_id:
                        description: External ID required by some roles when assuming them, typically for cross-account access. Used only if 'role_arn' is specified and the role's trust policy requires it.
                        type: string
                    output_location:
                        description: S3 URI where Athena query results should be stored (e.g., s3://your-bucket/athena/results/). Optional if the selected workgroup has a default result configuration.
                        type: string
                    role_arn:
                        description: ARN of the IAM role to assume. When specified, the SDK uses the base credentials to call STS AssumeRole and obtain temporary credentials scoped to this role.
                        type: string
                    role_session_name:
                        description: Session name to associate with the STS AssumeRole session. Used only if 'role_arn' is specified. Useful for identifying and auditing the session.
                        type: string
                    workgroup:
                        description: Athena workgroup to use for query execution. Defaults to 'primary' if not specified.
                        type: string
                  required:
                    - driver
                  title: Athena
                  type: object
                - examples:
                    - azure_storage_account: mystorageaccount
                      azure_storage_key: credentialstring
                      driver: azure
                      type: connector
                  properties:
                    allow_host_access:
                        description: Allow access to host environment configuration
                        type: boolean
                    azure_storage_account:
                        description: Azure storage account name
                        type: string
                    azure_storage_connection_string:
                        description: Optional azure connection string for storage account
                        type: string
                    azure_storage_key:
                        description: Azure storage access key
                        type: string
                    azure_storage_sas_token:
                        description: Optional azure SAS token for authentication
                        type: string
                    driver:
                        const: azure
                        description: Refers to the driver type and must be driver `azure`
                        type: string
                    path_prefixes:
                        description: |
                            A list of container or virtual directory prefixes that this connector is allowed to access.
                            Useful when different containers or paths use different credentials, allowing the system
                            to route access through the appropriate connector based on the blob path.
                            Example: `azure://my-bucket/`, ` azure://my-bucket/path/` ,`azure://my-bucket/path/prefix`
                        type:
                            - string
                            - array
                  required:
                    - driver
                    - azure_storage_account
                    - azure_storage_key
                  title: Azure
                  type: object
                - examples:
                    - allow_host_access: true
                      driver: bigquery
                      google_application_credentials: credentialjsonstring
                      project_id: my-project-id
                      type: connector
                  properties:
                    allow_host_access:
                        description: Enable the BigQuery client to use credentials from the host environment when no service account JSON is provided. This includes Application Default Credentials from environment variables, local credential files, or the Google Compute Engine metadata server. Defaults to true, allowing seamless authentication in GCP environments.
                        type: boolean
                    dataset_id:
                        description: BigQuery dataset ID
                        type: string
                    driver:
                        const: bigquery
                        description: Refers to the driver type and must be driver `bigquery`
                        type: string
                    google_application_credentials:
                        description: Raw contents of the Google Cloud service account key (in JSON format) used for authentication.
                        type: string
                    location:
                        description: BigQuery dataset location
                        type: string
                    project_id:
                        description: Google Cloud project ID
                        type: string
                  required:
                    - driver
                  title: BigQuery
                  type: object
                - examples:
                    - cluster: mycluster
                      database: mydatabase
                      driver: clickhouse
                      host: localhost
                      managed: false
                      mode: readwrite
                      password: mypassword
                      port: 9000
                      ssl: true
                      type: connector
                      username: myusername
                  properties:
                    can_scale_to_zero:
                        description: Indicates if the database can scale to zero
                        type: boolean
                    cluster:
                        description: Cluster name, required for running distributed queries
                        type: string
                    conn_max_lifetime:
                        description: Maximum time a connection may be reused
                        type: string
                    database:
                        description: Name of the ClickHouse database within the cluster
                        type: string
                    dial_timeout:
                        description: Timeout for dialing the ClickHouse server
                        type: string
                    driver:
                        const: clickhouse
                        description: Refers to the driver type and must be driver `clickhouse`
                        type: string
                    dsn:
                        description: DSN(Data Source Name) for the ClickHouse connection
                        type: string
                    embed_port:
                        description: Port to run ClickHouse locally (0 for random port)
                        type: integer
                    host:
                        description: Host where the ClickHouse instance is running
                        type: string
                    log_queries:
                        description: Controls whether to log raw SQL queries
                        type: boolean
                    managed:
                        description: '`true` means Rill will provision the connector using the default provisioner. `false` disables automatic provisioning.'
                        type: boolean
                    max_idle_conns:
                        description: Maximum number of idle connections in the pool
                        type: integer
                    max_open_conns:
                        description: Maximum number of open connections to the database
                        type: integer
                    mode:
                        description: '`read` - Controls the operation mode for the ClickHouse connection. Defaults to ''read'' for safe operation with external databases. Set to ''readwrite'' to enable model creation and table mutations. Note: When ''managed: true'', this is automatically set to ''readwrite''.'
                        type: string
                    password:
                        description: Password for authentication
                        type: string
                    port:
                        description: Port where the ClickHouse instance is accessible
                        type: integer
                    query_settings:
                        description: query settings to be set on dashboard queries. `query_settings_override` takes precedence over these settings and if set these are ignored. Each setting must be separated by a comma. Example `max_threads = 8, max_memory_usage = 10000000000`
                        type: string
                    query_settings_override:
                        description: override the default settings used in queries. Changing the default settings can lead to incorrect query results and is generally not recommended. If you need to add settings, use `query_settings`
                        type: string
                    read_timeout:
                        description: Maximum time for a connection to read data
                        type: string
                    ssl:
                        description: Indicates whether a secured SSL connection is required
                        type: boolean
                    username:
                        description: Username for authentication
                        type: string
                  required:
                    - driver
                  title: ClickHouse
                  type: object
                - examples:
                    - driver: druid
                      host: localhost
                      password: mypassword
                      port: 8082
                      ssl: true
                      type: connector
                      username: myusername
                  properties:
                    driver:
                        const: druid
                        description: Refers to the driver type and must be driver `druid`
                        type: string
                    dsn:
                        description: Data Source Name (DSN) for connecting to Druid
                        type: string
                    host:
                        description: Hostname of the Druid coordinator or broker
                        type: string
                    log_queries:
                        description: Log raw SQL queries sent to Druid
                        type: boolean
                    max_open_conns:
                        description: Maximum number of open database connections (0 = default, -1 = unlimited)
                        type: integer
                    password:
                        description: Password for authenticating with Druid
                        type: string
                    port:
                        description: Port number of the Druid service
                        type: integer
                    skip_version_check:
                        description: Skip checking Druid version compatibility
                        type: boolean
                    ssl:
                        description: Enable SSL for secure connection
                        type: boolean
                    username:
                        description: Username for authenticating with Druid
                        type: string
                  required:
                    - driver
                    - dsn
                  title: Druid
                  type: object
                - examples:
                    - allow_host_access: true
                      cpu: 4
                      driver: duckdb
                      init_sql: INSTALL httpfs; LOAD httpfs;
                      log_queries: true
                      memory_limit_gb: 16
                      mode: readwrite
                      pool_size: 5
                      read_write_ratio: 0.7
                      type: connector
                  properties:
                    allow_host_access:
                        description: Whether access to local environment and file system is allowed
                        type: boolean
                    attach:
                        description: Full ATTACH statement to attach a DuckDB database
                        type: string
                    boot_queries:
                        description: Deprecated - Use init_sql instead
                        type: string
                    conn_init_sql:
                        description: SQL executed when a new connection is initialized
                        type: string
                    cpu:
                        description: Number of CPU cores available to the database
                        minimum: 1
                        type: integer
                    create_secrets_from_connectors:
                        description: List of connector names for which temporary secrets should be created before executing the SQL.
                        type:
                            - string
                            - array
                    database_name:
                        description: Name of the attached DuckDB database (auto-detected if not set)
                        type: string
                    driver:
                        const: duckdb
                        description: Must be "duckdb"
                        type: string
                    init_sql:
                        description: SQL executed during database initialization
                        type: string
                    log_queries:
                        description: Whether to log raw SQL queries executed through OLAP
                        type: boolean
                    memory_limit_gb:
                        description: Amount of memory in GB available to the database
                        minimum: 1
                        type: integer
                    mode:
                        default: read
                        description: Set the mode for the DuckDB connection.
                        enum:
                            - read
                            - readwrite
                        type: string
                    path:
                        description: Path to external DuckDB database
                        type: string
                    pool_size:
                        description: Number of concurrent connections and queries allowed
                        minimum: 1
                        type: integer
                    read_write_ratio:
                        default: 0.5
                        description: Ratio of resources allocated to read vs write operations
                        maximum: 1
                        minimum: 0
                        type: number
                    schema_name:
                        description: Default schema used by the DuckDB database
                        type: string
                  required:
                    - driver
                  title: DuckDB
                  type: object
                - examples:
                    - driver: duckdb
                      mode: read
                      path: /path/to/my-duckdb-database.db
                      type: connector
                  properties:
                    driver:
                        const: duckdb
                        description: Refers to the driver type and must be driver `duckdb`
                        type: string
                    mode:
                        default: read
                        description: Set the mode for the DuckDB connection.
                        enum:
                            - read
                            - readwrite
                        type: string
                    path:
                        description: Path to the DuckDB database
                        type: string
                  required:
                    - driver
                    - db
                  title: External DuckDB
                  type: object
                - examples:
                    - driver: gcs
                      google_application_credentials: credentialjsonstring
                      type: connector
                  properties:
                    allow_host_access:
                        description: Allow access to host environment configuration
                        type: boolean
                    driver:
                        const: gcs
                        description: Refers to the driver type and must be driver `gcs`
                        type: string
                    google_application_credentials:
                        description: Google Cloud credentials JSON string
                        type: string
                    key_id:
                        description: Optional S3-compatible Key ID when used in compatibility mode
                        type: string
                    path_prefixes:
                        description: "A list of bucket path prefixes that this connector is allowed to access. \nUseful when different buckets or bucket prefixes use different credentials, \nallowing the system to select the appropriate connector based on the bucket path.\nExample: `gs://my-bucket/`, ` gs://my-bucket/path/` ,`gs://my-bucket/path/prefix`\n"
                        type:
                            - string
                            - array
                    secret:
                        description: Optional S3-compatible Secret when used in compatibility mode
                        type: string
                  required:
                    - driver
                    - bucket
                  title: GCS
                  type: object
                - examples:
                    - driver: https
                      headers:
                        Authorization: Bearer my-token
                      type: connector
                  properties:
                    driver:
                        const: https
                        description: Refers to the driver type and must be driver `https`
                        type: string
                    headers:
                        additionalProperties:
                            type: string
                        description: HTTP headers to include in the request
                        type: object
                    path_prefixes:
                        description: |
                            A list of HTTP/HTTPS URL prefixes that this connector is allowed to access.
                            Useful when different URL namespaces use different credentials, enabling the
                            system to choose the appropriate connector based on the URL path.
                            Example: `https://example.com/`, ` https://example.com/path/` ,`https://example.com/path/prefix`
                        type:
                            - string
                            - array
                  required:
                    - driver
                    - path
                  title: HTTPS
                  type: object
                - examples:
                    - driver: duckdb
                      path: md:my_database
                      schema_name: my_schema
                      token: '{{ .env.connector.motherduck.token }}'
                      type: connector
                  properties:
                    create_secrets_from_connectors:
                        description: List of connector names for which temporary secrets should be created before executing the SQL.
                        type:
                            - string
                            - array
                    driver:
                        const: duckdb
                        description: Refers to the driver type and must be driver `duckdb`.
                        type: string
                    init_sql:
                        description: SQL executed during database initialization.
                        type: string
                    mode:
                        default: read
                        description: Set the mode for the MotherDuck connection. By default, it is set to 'read' which allows only read operations. Set to 'readwrite' to enable model creation and table mutations.
                        enum:
                            - read
                            - readwrite
                        type: string
                    path:
                        description: Path to your MD database
                        type: string
                    schema_name:
                        description: Define your schema if not main, uses main by default
                        type: string
                    token:
                        description: MotherDuck token
                        type: string
                  required:
                    - driver
                    - path
                    - token
                  title: MotherDuck
                  type: object
                - examples:
                    - database: mydb
                      driver: mysql
                      host: localhost
                      password: p@ss
                      port: 3306
                      ssl-mode: preferred
                      type: connector
                      user: user
                    - driver: mysql
                      dsn: mysql://user:p%40ss@localhost:3306/mydb?ssl-mode=preferred
                      type: connector
                  properties:
                    database:
                        description: Name of the MySQL database
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `mysql`
                        type: string
                    dsn:
                        description: |
                            **Data Source Name (DSN)** for the MySQL connection, provided in [MySQL URI format](https://dev.mysql.com/doc/refman/8.4/en/connecting-using-uri-or-key-value-pairs.html#connecting-using-uri).
                            The DSN must follow the standard MySQL URI scheme:
                            ```text
                            mysql://user:password@host:3306/my-db
                            ```
                            Rules for special characters in password:
                            - The following characters are allowed [unescaped in the URI](https://datatracker.ietf.org/doc/html/rfc3986#section-2.3): `~` `.` `_` `-`
                            - All other special characters must be percent-encoded (`%XX` format).
                            ```text
                            mysql://user:pa%40ss@localhost:3306/my-db   # password contains '@'
                            mysql://user:pa%3Ass@localhost:3306/my-db   # password contains ':'
                            ```
                        type: string
                    host:
                        description: Hostname of the MySQL server
                        type: string
                    password:
                        description: Password for authentication
                        type: string
                    port:
                        description: Port number for the MySQL server
                        type: integer
                    ssl-mode:
                        description: 'ssl mode options: `disabled`, `preferred`, or `required`.'
                        type: string
                    user:
                        description: Username for authentication
                        type: string
                  required:
                    - driver
                  title: MySQL
                  type: object
                - examples:
                    - api_key: my-api-key
                      api_type: openai
                      api_version: "2023-05-15"
                      base_url: https://api.openai.com/v1
                      driver: openai
                      model: gpt-4o
                      type: connector
                  properties:
                    api_key:
                        description: API key for connecting to OpenAI
                        type: string
                    api_type:
                        description: The type of OpenAI API to use
                        type: string
                    api_version:
                        description: The version of the OpenAI API to use (e.g., '2023-05-15'). Required when API Type is AZURE or AZURE_AD
                        type: string
                    base_url:
                        description: The base URL for the OpenAI API (e.g., 'https://api.openai.com/v1')
                        type: string
                    driver:
                        description: The driver type, must be set to "openai"
                        type: string
                    model:
                        description: The OpenAI model to use (e.g., 'gpt-4o')
                        type: string
                  required:
                    - api_key
                  title: OpenAI
                  type: object
                - examples:
                    - broker_host: localhost
                      broker_port: 9000
                      controller_host: localhost
                      controller_port: 9000
                      driver: pinot
                      log_queries: true
                      max_open_conns: 100
                      password: mypassword
                      ssl: true
                      type: connector
                      username: myusername
                  properties:
                    broker_host:
                        description: Hostname of the Pinot broker
                        type: string
                    broker_port:
                        description: Port number for the Pinot broker
                        type: integer
                    controller_host:
                        description: Hostname of the Pinot controller
                        type: string
                    controller_port:
                        description: Port number for the Pinot controller
                        type: integer
                    driver:
                        description: Refers to the driver type and must be driver `pinot`
                        type: string
                    dsn:
                        description: DSN(Data Source Name) for the Pinot connection
                        type: string
                    log_queries:
                        description: Log raw SQL queries executed through Pinot
                        type: boolean
                    max_open_conns:
                        description: Maximum number of open connections to the Pinot database
                        type: integer
                    password:
                        description: Password for authenticating with Pinot
                        type: string
                    ssl:
                        description: Enable SSL connection to Pinot
                        type: boolean
                    username:
                        description: Username for authenticating with Pinot
                        type: string
                  required:
                    - driver
                    - dsn
                    - broker_host
                    - controller_host
                  title: Pinot
                  type: object
                - examples:
                    - dbname: mydatabase
                      driver: postgres
                      host: localhost
                      password: mypassword
                      port: 5432
                      sslmode: prefer
                      type: connector
                      user: myusername
                    - driver: postgres
                      dsn: user=myusername password='my pass\'word' host=localhost port=5432 dbname=mydatabase sslmode=prefer
                      type: connector
                    - driver: postgres
                      dsn: postgres://myusername:p%40ss@localhost:5432/mydatabase?sslmode=prefer
                      type: connector
                  properties:
                    dbname:
                        description: Name of the Postgres database
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `postgres`
                        type: string
                    dsn:
                        description: |
                            **Data Source Name (DSN)** for the PostgreSQL connection, provided in
                            [PostgreSQL connection string format](https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING).
                            PostgreSQL supports both **key=value format** and **URI format**.

                            key=value format example:
                            ```text
                            user=user password=password host=host port=5432 dbname=mydb
                            ```
                            Rules for key=value format for special characters:
                            - To write an empty value, or a value containing spaces, `=`, single quotes, or backslashes, surround it with single quotes.
                            - Single quotes and backslashes inside a value must be escaped with a backslash (`\'` and `\\`).

                            URI format example:
                            ```text
                            postgres://user:password@host:5432/mydb
                            ```

                            Rules for URI format:
                            - The following characters are allowed [unescaped in the URI](https://datatracker.ietf.org/doc/html/rfc3986#section-2.3): `~` `.` `_` `-`
                            - All other special characters must be percent-encoded (`%XX` format).

                            Examples (URI format with encoded characters):
                            ```text
                            postgres://user:pa%40ss@localhost:5432/my-db   # '@' is encoded as %40
                            postgres://user:pa%3Ass@localhost:5432/my-db   # ':' is encoded as %3A
                            ```
                        type: string
                    host:
                        description: Hostname of the Postgres server
                        type: string
                    password:
                        description: Password for authentication
                        type: string
                    port:
                        description: Port number for the Postgres server
                        type: string
                    sslmode:
                        description: 'ssl mode options: `disable`, `allow`, `prefer` or `require`.'
                        type: string
                    user:
                        description: Username for authentication
                        type: string
                  required:
                    - driver
                  title: Postgres
                  type: object
                - examples:
                    - aws_access_key_id: my-access-key-id
                      aws_access_token: my-access-token
                      aws_secret_access_key: my-secret-access-key
                      cluster_identifier: my-cluster-identifier
                      database: mydatabase
                      driver: redshift
                      region: us-east-1
                      type: connector
                      workgroup: my-workgroup
                  properties:
                    aws_access_key_id:
                        description: AWS Access Key ID used for authenticating with Redshift.
                        type: string
                    aws_access_token:
                        description: AWS Session Token for temporary credentials (optional).
                        type: string
                    aws_secret_access_key:
                        description: AWS Secret Access Key used for authenticating with Redshift.
                        type: string
                    cluster_identifier:
                        description: Cluster identifier for provisioned Redshift clusters, in case of Redshift Serverless use 'workgroup' .
                        type: string
                    database:
                        description: Name of the Redshift database to query.
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `redshift`
                        type: string
                    region:
                        description: AWS region where the Redshift cluster or workgroup is hosted (e.g., 'us-east-1').
                        type: string
                    workgroup:
                        description: Workgroup name for Redshift Serverless, in case of provisioned Redshift clusters use 'cluster_identifier'.
                        type: string
                  required:
                    - driver
                    - aws_access_key_id
                    - aws_secret_access_key
                    - database
                  title: Redshift
                  type: object
                - examples:
                    - aws_access_key_id: my-access-key-id
                      aws_access_token: my-access-token
                      aws_secret_access_key: my-secret-access-key
                      driver: s3
                      endpoint: https://my-s3-endpoint.com
                      region: us-east-1
                      type: connector
                  properties:
                    allow_host_access:
                        description: Allow access to host environment configuration
                        type: boolean
                    aws_access_key_id:
                        description: AWS Access Key ID used for authentication
                        type: string
                    aws_access_token:
                        description: Optional AWS session token for temporary credentials
                        type: string
                    aws_secret_access_key:
                        description: AWS Secret Access Key used for authentication
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `s3`
                        type: string
                    endpoint:
                        description: Optional custom endpoint URL for S3-compatible storage
                        type: string
                    path_prefixes:
                        description: |
                            A list of bucket path prefixes that this connector is allowed to access.
                            Useful when different buckets or bucket prefixes use different credentials,
                            allowing the system to select the appropriate connector based on the bucket path.
                            Example: `s3://my-bucket/`, ` s3://my-bucket/path/` ,`s3://my-bucket/path/prefix`
                        type:
                            - string
                            - array
                    region:
                        description: AWS region of the S3 bucket
                        type: string
                  required:
                    - driver
                    - bucket
                  title: S3
                  type: object
                - examples:
                    - client_id: my-client-id
                      driver: salesforce
                      endpoint: https://login.salesforce.com
                      password: mypassword
                      type: connector
                      username: myusername
                  properties:
                    client_id:
                        description: Client ID used for Salesforce OAuth authentication
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `salesforce`
                        type: string
                    endpoint:
                        description: Salesforce API endpoint URL
                        type: string
                    key:
                        description: Authentication key for Salesforce (secret)
                        type: string
                    password:
                        description: Salesforce account password (secret)
                        type: string
                    username:
                        description: Salesforce account username
                        type: string
                  required:
                    - driver
                    - username
                    - endpoint
                    - client_id
                  title: Salesforce
                  type: object
                - examples:
                    - bot_token: xoxb-my-bot-token
                      driver: slack
                      type: connector
                  properties:
                    bot_token:
                        description: Bot token used for authenticating Slack API requests
                        type: string
                    driver:
                        description: Refers to the driver type and must be driver `slack`
                        type: string
                  required:
                    - driver
                    - bot_token
                  title: Slack
                  type: object
                - examples:
                    - account: my_account_identifier
                      database: my_db
                      driver: snowflake
                      parallel_fetch_limit: 2
                      privateKey: '{{ .env.SNOWFLAKE_PRIVATE_KEY }}'
                      role: my_role
                      schema: my_schema
                      type: connector
                      user: my_user
                      warehouse: my_wh
                    - driver: snowflake
                      dsn: '{{ .env.SNOWFLAKE_DSN }}'
                      parallel_fetch_limit: 2
                      type: connector
                  properties:
                    account:
                        description: Snowflake account identifier. To find your Snowflake account identifier, look at your Snowflake account URL. The account identifier is everything before .snowflakecomputing.com
                        type: string
                    authenticator:
                        description: Optional authenticator type (e.g., SNOWFLAKE_JWT).
                        type: string
                    database:
                        description: Name of the Snowflake database.
                        type: string
                    driver:
                        const: snowflake
                        description: Refers to the driver type and must be driver `snowflake`
                        type: string
                    dsn:
                        description: "DSN (Data Source Name) for the Snowflake connection.\n\nThis is intended for **advanced configuration** where you want to specify\nproperties that are not explicitly defined above.  \nIt can only be used when the other connection fields (account, user, password,\ndatabase, schema, warehouse, role, authenticator, privateKey) are **not used**.\n\nFor details on private key generation and encoding, see the `privateKey` property.\n"
                        type: string
                    parallel_fetch_limit:
                        description: Maximum number of concurrent fetches during query execution.
                        type: integer
                    password:
                        description: Password for the Snowflake connection. _(deprecated, use privateKey instead)_
                        type: string
                    privateKey:
                        description: |
                            Private key for JWT authentication.
                            :::tip
                            Private key must be generated as a **PKCS#8 (nocrypt) key**, since the Snowflake Go driver
                            only supports unencrypted private keys. After generating, it must be **base64 URL encoded**.

                            Example commands to generate and encode:

                            ```bash
                            # Generate a 2048-bit unencrypted PKCS#8 private key
                            openssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out rsa_key.p8 -nocrypt

                            # Convert URL safe format for Snowflake
                            cat rsa_key.p8 | grep -v "\----" | tr -d '\n' | tr '+/' '-_'
                            ```
                            See: https://docs.snowflake.com/en/guide/key-pair-auth
                            :::
                        type: string
                    role:
                        description: Snowflake role to use.
                        type: string
                    schema:
                        description: Schema within the database to use.
                        type: string
                    user:
                        description: Username for the Snowflake connection.
                        type: string
                    warehouse:
                        description: Compute warehouse to use for queries.
                        type: string
                  required:
                    - type
                    - driver
                  title: Snowflake
                  type: object
                - examples:
                    - driver: sqlite
                      dsn: file:mydatabase.db
                      type: connector
                  properties:
                    driver:
                        description: Refers to the driver type and must be driver `sqlite`
                        type: string
                    dsn:
                        description: DSN(Data Source Name) for the sqlite connection
                        type: string
                  required:
                    - driver
                    - dsn
                  title: SQLite
                  type: object
        description: |
            Connector YAML files define how Rill connects to external data sources and OLAP engines. Each connector specifies a driver type and its required connection parameters.

            ## Available Connector Types

            ### _OLAP Engines_
            - [**ClickHouse**](#clickhouse) - ClickHouse analytical database
            - [**Druid**](#druid) - Apache Druid
            - [**DuckDB**](#duckdb) - Embedded DuckDB engine (default)
            - [**MotherDuck**](#motherduck) - MotherDuck cloud database
            - [**Pinot**](#pinot) - Apache Pinot

            ### _Data Warehouses_
            - [**Athena**](#athena) - Amazon Athena
            - [**BigQuery**](#bigquery) - Google BigQuery
            - [**Redshift**](#redshift) - Amazon Redshift
            - [**Snowflake**](#snowflake) - Snowflake data warehouse

            ### _Databases_
            - [**MySQL**](#mysql) - MySQL databases
            - [**PostgreSQL**](#postgres) - PostgreSQL databases
            - [**SQLite**](#sqlite) - SQLite databases

            ### _Cloud Storage_
            - [**Azure**](#azure) - Azure Blob Storage
            - [**GCS**](#gcs) - Google Cloud Storage
            - [**S3**](#s3) - Amazon S3 storage

            ### _Other_
            - [**Extenral DuckDB**](#external-duckdb) - External DuckDB database
            - [**HTTPS**](#https) - Public files via HTTP/HTTPS
            - [**OpenAI**](#openai) - OpenAI data
            - [**Salesforce**](#salesforce) - Salesforce data
            - [**Slack**](#slack) - Slack data

            :::warning Security Recommendation
            For all credential parameters (passwords, tokens, keys), use environment variables with the syntax `{{.env.connector.<connector_driver>.<parameter_name>}}`. This keeps sensitive data out of your YAML files and version control. See our [credentials documentation](/developers/build/connectors/credentials/) for complete setup instructions.
            :::
        id: connectors
        title: Connector YAML
        type: object
        ```

        ## Examples

        ### DuckDB: Managed

        Explicit:

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb
        managed: true
        ```

        or relying on defaults:

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb
        ```

        ### DuckDB: With init_sql for S3 secrets

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb

        init_sql: |
          CREATE SECRET IF NOT EXISTS s3 (
            TYPE S3,
            KEY_ID '{{ .env.aws_access_key_id }}',
            SECRET '{{ .env.aws_secret_access_key }}',
            REGION 'us-east-1'
          )
        ```

        This is now deprecated in favor of creating a dedicated `s3.yaml` connector file, which Rill will automatically load and create as a secret in DuckDB.

        ### DuckDB: With extensions

        ```yaml
        # connectors/duckdb.yaml
        type: connector
        driver: duckdb

        init_sql: |
          INSTALL spatial;
          LOAD spatial;
        ```

        ### DuckDB: MotherDuck database with existing tables

        ```yaml
        # connectors/motherduck.yaml
        type: connector
        driver: duckdb
        path: "md:my_database"
        token: "{{ .env.motherduck_token }}"
        schema_name: main
        mode: read
        ```

        ### ClickHouse: Cloud with SSL

        ```yaml
        # connectors/clickhouse.yaml
        type: connector
        driver: clickhouse
        host: "abc123.us-east-1.aws.clickhouse.cloud"
        port: 8443
        username: "default"
        password: "{{ .env.clickhouse_password }}"
        database: "default"
        ssl: true
        ```

        ### ClickHouse: Readwrite with cluster

        ```yaml
        # connectors/clickhouse.yaml
        type: connector
        driver: clickhouse
        mode: readwrite

        host: "{{ .env.clickhouse_host }}"
        port: 9440
        username: "{{ .env.clickhouse_user }}"
        password: "{{ .env.clickhouse_password }}"
        database: "default"
        cluster: "my_cluster"
        ssl: true
        ```

        ### ClickHouse: Dev/prod configuration

        ```yaml
        # connectors/clickhouse.yaml
        type: connector
        driver: clickhouse
        mode: readwrite

        dev:
          managed: true

        prod:
          host: "{{ .env.clickhouse_host }}"
          port: 9440
          username: "{{ .env.clickhouse_user }}"
          password: "{{ .env.clickhouse_password }}"
          database: "default"
          cluster: "{{ .env.clickhouse_cluster }}"
          ssl: true
        ```

        ### S3: Basic with credentials and region

        ```yaml
        # connectors/s3.yaml
        type: connector
        driver: s3
        aws_access_key_id: "{{ .env.aws_access_key_id }}"
        aws_secret_access_key: "{{ .env.aws_secret_access_key }}"
        region: us-west-2
        ```

        ### S3: Cloudflare R2 (S3-compatible)

        ```yaml
        # connectors/r2.yaml
        type: connector
        driver: s3
        aws_access_key_id: "{{ .env.r2_access_key_id }}"
        aws_secret_access_key: "{{ .env.r2_secret_access_key }}"
        endpoint: "https://{{ .env.r2_account_id }}.r2.cloudflarestorage.com"
        region: auto
        ```

        ### GCS: Minimal (uses default credentials)

        ```yaml
        # connectors/gcs.yaml
        type: connector
        driver: gcs
        ```

        ### GCS: With explicit credentials

        ```yaml
        # connectors/gcs.yaml
        type: connector
        driver: gcs
        google_application_credentials: "{{ .env.gcs_service_account_json }}"
        ```

        ### BigQuery

        ```yaml
        # connectors/bigquery.yaml
        type: connector
        driver: bigquery
        project_id: "my-gcp-project"
        google_application_credentials: "{{ .env.bigquery_service_account_json }}"
        ```

        ### Snowflake: Basic with DSN

        ```yaml
        # connectors/snowflake.yaml
        type: connector
        driver: snowflake
        dsn: "{{ .env.snowflake_dsn }}"
        ```

        ### Postgres

        ```yaml
        # connectors/postgres.yaml
        type: connector
        driver: postgres
        host: "{{ .env.postgres_host }}"
        port: 5432
        user: "{{ .env.postgres_user }}"
        password: "{{ .env.postgres_password }}"
        dbname: "analytics"
        sslmode: require
        ```

        ### Druid: Host-based

        ```yaml
        # connectors/druid.yaml
        type: connector
        driver: druid
        host: "{{ .env.druid_host }}"
        port: 8888
        username: "{{ .env.druid_user }}"
        password: "{{ .env.druid_password }}"
        ssl: true
        ```

        ### Redshift: Serverless

        ```yaml
        # connectors/redshift.yaml
        type: connector
        driver: redshift
        aws_access_key_id: "{{ .env.aws_access_key_id }}"
        aws_secret_access_key: "{{ .env.aws_secret_access_key }}"
        workgroup: "my-workgroup"
        region: us-east-1
        database: "analytics"
        ```

        ### OpenAI

        ```yaml
        # connectors/openai.yaml
        type: connector
        driver: openai
        api_key: "{{ .env.openai_api_key }}"
        ```

        ### Slack

        ```yaml
        # connectors/slack.yaml
        type: connector
        driver: slack
        bot_token: "{{ .env.slack_bot_token }}"
        ```

        ### Pinot

        ```yaml
        # connectors/pinot.yaml
        type: connector
        driver: pinot
        broker_host: "{{ .env.pinot_broker_host }}"
        controller_host: "{{ .env.pinot_controller_host }}"
        username: "{{ .env.pinot_user }}"
        password: "{{ .env.pinot_password }}"
        ssl: true
        ```
    - role: user
      content_type: text
      content: |-
        You should develop a Rill project file based on the following task description: Update the ClickHouse connector configuration to include SSL support, as the public playground likely requires a secure connection. Add 'ssl: true' to the YAML file. Ensure all other settings remain the same: driver clickhouse, host play.clickhouse.com, port 9440, username play, blank password, mode read.

        Here is some important context:
        - You should develop the file at path: /connectors/clickhouse.yaml
        - The file should be of type: connector
        - You are running as a sub-agent of a larger developer agent. Stay aligned on your specific task, avoid extra discovery, and report back your success or failure to the parent agent.

        Here is some additional context that may or may not be relevant to your task:
        - The project's default OLAP connector is named duckdb (driver: duckdb).
    - role: assistant
      content_type: tool_call
      id: 7b5f31233bc044b4
      tool_name: write_file
      content: '{"contents":"type: connector\ndriver: clickhouse\nhost: play.clickhouse.com\nport: 9440\nusername: play\npassword: \"\"\nmode: read\nssl: true\n","path":"/connectors/clickhouse.yaml"}'
    - role: tool
      content_type: tool_response
      id: 7b5f31233bc044b4
      content: '{"diff":"--- /connectors/clickhouse.yaml\n+++ /connectors/clickhouse.yaml\n@@ -5,4 +5,5 @@\n username: play\n password: \"\"\n mode: read\n+ssl: true\n \n","resources":[{"kind":"rill.runtime.v1.Connector","name":"clickhouse","reconcile_error":"","reconcile_status":"RECONCILE_STATUS_IDLE"}]}'
  response:
    - role: assistant
      content_type: text
      content: 'Success: The ClickHouse connector configuration at /connectors/clickhouse.yaml has been updated to include ssl: true, as required for secure connections to the public playground. All other settings (driver, host, port, username, blank password, and mode) remain unchanged. No errors were encountered during the update.'
- index: 8
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. Non-materialized models become views, which re-execute on every query.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: user
      content_type: text
      content: |-
        This may not relate to the user's task, but for context, the project's default OLAP connector is named duckdb (driver: duckdb).
         The default OLAP is in readwrite mode, so you can use it in models if you want.


        Task: Clickhouse has a public playground database on play.clickhouse.com:9440 (username `play` and no password). Can you set it up as the OLAP connector and develop a dashboard based on one of its tables? Please proceed without asking clarifying questions.
    - role: assistant
      content_type: tool_call
      id: 5f915383ab074f81
      tool_name: develop_file
      content: '{"path":"connectors/clickhouse.yaml","prompt":"Create a connector for the public ClickHouse playground database at play.clickhouse.com:9440. Use the ''clickhouse'' driver. Set the username to ''play'' and leave the password blank. This connector will be used as an OLAP connector for dashboards. Set ''mode'' to ''read'' to prevent writes. Name the connector ''clickhouse''.","type":"connector"}'
    - role: tool
      content_type: tool_response
      id: 5f915383ab074f81
      content: '{"response":"The connector file /connectors/clickhouse.yaml was created as specified:\n\n- type: connector\n- driver: clickhouse\n- host: play.clickhouse.com\n- port: 9440\n- username: play\n- password: \"\" (blank)\n- mode: read\n\nThe file was written successfully. However, there is a reconcile error indicating a handshake failure, likely due to an SSL/TLS mismatch. If SSL is required for this endpoint, you may need to add ssl: true to the configuration. Please advise if you want to update the file to include SSL."}'
    - role: assistant
      content_type: tool_call
      id: 3925a34a97214026
      tool_name: develop_file
      content: '{"path":"connectors/clickhouse.yaml","prompt":"Update the ClickHouse connector configuration to include SSL support, as the public playground likely requires a secure connection. Add ''ssl: true'' to the YAML file. Ensure all other settings remain the same: driver clickhouse, host play.clickhouse.com, port 9440, username play, blank password, mode read.","type":"connector"}'
    - role: tool
      content_type: tool_response
      id: 3925a34a97214026
      content: '{"response":"Success: The ClickHouse connector configuration at /connectors/clickhouse.yaml has been updated to include ssl: true, as required for secure connections to the public playground. All other settings (driver, host, port, username, blank password, and mode) remain unchanged. No errors were encountered during the update."}'
  response:
    - role: assistant
      content_type: tool_call
      id: call_wCgwHdrrNzSOVxvu4gO5LdkJ
      tool_name: develop_file
      content: '{"path":"rill.yaml","prompt":"Update the project configuration to set the default OLAP connector to ''clickhouse'' instead of ''duckdb''. Ensure that the ''olap_connector'' property is set to ''clickhouse''. Retain any other existing configuration in rill.yaml.","type":"rill.yaml"}'
- index: 9
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. Non-materialized models become views, which re-execute on every query.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: system
      content_type: text
      content: "# Instructions for developing `rill.yaml`\n\n## Introduction\n\n`rill.yaml` is a required configuration file located at the root of every Rill project. It defines project-wide settings, similar to `package.json` in Node.js or `dbt_project.yml` in dbt.\n\n## Core Concepts\n\n### Project metadata\n\nThere are no required properties in `rill.yaml`, but it is common to configure:\n\n- `display_name`: Human-readable name shown in the UI\n- `description`: Brief description of the project's purpose\n- `compiler`: Deprecated property that is commonly found in old projects\n\n### Default OLAP connector\n\nThe `olap_connector` property sets the default OLAP database for the project. Models output to this connector by default, and metrics views query from it unless explicitly overridden.\n\nCommon values are `duckdb` or `clickhouse`. If not specified, Rill initializes a managed DuckDB database and uses it as the default OLAP connector. \n\n### Mock users for security testing\n\nThe `mock_users` property defines test users for validating security policies during local development. Each mock user can have:\n\n- `email` (required): The user's email address\n- `name`: Display name\n- `admin`: Boolean indicating admin privileges\n- `groups`: List of group memberships\n- Custom attributes for use in security policy expressions\n\nWhen mock users are defined and security policies exist, a \"View as\" dropdown appears in the dashboard preview.\n\n### Environment variables\n\nThe `env` property sets default values for non-sensitive variables. These can be referenced in resource files using templating syntax (`{{ .env.<variable> }}`). Sensitive secrets should go in `.env` instead.\n\n### Resource type defaults\n\nProject-wide defaults can be set for resource types using plural keys:\n\n- `models`: Default settings for all models (e.g., refresh schedules)\n- `metrics_views`: Default settings for all metrics views (e.g., `first_day_of_week`)\n- `explores`: Default settings for explore dashboards (e.g., `time_ranges`, `time_zones`)\n- `canvases`: Default settings for canvas dashboards\n\nIndividual resources can override these defaults.\n\n### Path management\n\n- `ignore_paths`: List of paths to exclude from parsing (use leading `/`)\n- `public_paths`: List of paths to expose over HTTP (defaults to `['./public']`)\n\n### Environment overrides\n\nThe `dev` and `prod` properties allow environment-specific configuration overrides.\n\n## JSON Schema\n\nHere is a full JSON schema for the `rill.yaml` syntax:\n\n```\n$schema: http://json-schema.org/draft-07/schema#\nallOf:\n    - properties:\n        ai_connector:\n            description: Specifies the default AI connector for the project. Defaults to Rill's internal AI connector if not set.\n            type: string\n        ai_instructions:\n            description: Extra instructions for LLM/AI features. Used to guide natural language question answering and routing.\n            type: string\n        compiler:\n            description: Specifies the parser version to use for compiling resources\n            type: string\n        description:\n            description: A brief description of the project\n            type: string\n        display_name:\n            description: The display name of the project, shown in the upper-left corner of the UI\n            type: string\n        features:\n            description: Optional feature flags. Can be specified as a map of feature names to booleans.\n            type: object\n      title: Properties\n      type: object\n    - description: |\n        Rill allows you to specify the default OLAP engine to use in your project via `rill.yaml`.\n        :::info Curious about OLAP Engines?\n        Please see our reference documentation on [OLAP Engines](/developers/build/connectors/olap).\n        :::\n      properties:\n        olap_connector:\n            description: Specifies the default OLAP engine for the project. Defaults to duckdb if not set.\n            examples:\n                - olap_connector: clickhouse\n            type: string\n      title: Configuring the default OLAP Engine\n      type: object\n    - description: |\n        In `rill.yaml`, project-wide defaults can be specified for a resource type within a project. Unless otherwise specified, _individual resources will inherit any defaults_ that have been specified in `rill.yaml`. For available properties that can be configured, please refer to the YAML specification for each individual resource type - [model](models.md), [metrics_view](metrics-views.md), and [explore](explore-dashboards.md)\n\n        :::note Use plurals when specifying project-wide defaults\n        In your `rill.yaml`, the top level property for the resource type needs to be **plural**, such as `models`, `metrics_views` and `explores`.\n        :::\n\n        :::info Hierarchy of inheritance and property overrides\n        As a general rule of thumb, properties that have been specified at a more _granular_ level will supercede or override higher level properties that have been inherited. Therefore, in order of inheritance, Rill will prioritize properties in the following order:\n        1. Individual [models](models.md)/[metrics_views](metrics-views.md)/[explore](explore-dashboards.md) object level properties (e.g. `models.yaml` or `explore-dashboards.yaml`)\n        2. [Environment](/developers/build/models/templating) level properties (e.g. a specific property that have been set for `dev`)\n        3. [Project-wide defaults](#project-wide-defaults) for a specific property and resource type\n        :::\n      properties:\n        canvases:\n            description: Defines project-wide default settings for canvases. Unless overridden, individual canvases will inherit these defaults.\n            examples:\n                - canvases:\n                    defaults:\n                        time_range: P7D\n                    time_ranges:\n                        - PT24H\n                        - P7D\n                    time_zones:\n                        - UTC\n                  explores:\n                    defaults:\n                        time_range: P24M\n                    time_ranges:\n                        - PT24H\n                        - P6M\n                    time_zones:\n                        - UTC\n                  metrics_views:\n                    first_day_of_week: 1\n                    smallest_time_grain: month\n                  models:\n                    refresh:\n                        cron: 0 * * * *\n            type: object\n        explores:\n            description: Defines project-wide default settings for explores. Unless overridden, individual explores will inherit these defaults.\n            type: object\n        metrics_views:\n            description: Defines project-wide default settings for metrics_views. Unless overridden, individual metrics_views will inherit these defaults.\n            type: object\n        models:\n            description: Defines project-wide default settings for models. Unless overridden, individual models will inherit these defaults.\n            type: object\n      title: Project-wide defaults\n      type: object\n    - description: \"Primarily useful for [templating](/developers/build/connectors/templating), variables can be set in the `rill.yaml` file directly. This allows variables to be set for your projects deployed to Rill Cloud while still being able to use different variable values locally if you prefer. \\n:::info Overriding variables locally\\nVariables also follow an order of precedence and can be overridden locally. By default, any variables defined will be inherited from `rill.yaml`. However, if you manually pass in a variable when starting Rill Developer locally via the CLI, this value will be used instead for the current instance of your running project:\\n```bash\\nrill start --env numeric_var=100 --env string_var=\\\"different_value\\\"\\n```\\n:::\\n:::tip Setting variables through `.env`\\nVariables can also be set through your project's `<RILL_PROJECT_HOME>/.env` file (or using the `rill env set` CLI command), such as:\\n```bash\\nvariable=xyz\\n```\\nSimilar to how [connector credentials can be pushed / pulled](/developers/build/connectors/credentials#pulling-credentials-and-variables-from-a-deployed-project-on-rill-cloud) from local to cloud or vice versa, project variables set locally in Rill Developer can be pushed to Rill Cloud and/or pulled back to your local instance from your deployed project by using the `rill env push` and `rill env pull` commands respectively.\\n:::\\n\"\n      properties:\n        env:\n            description: To define a variable in `rill.yaml`, pass in the appropriate key-value pair for the variable under the `env` key\n            examples:\n                - env:\n                    numeric_var: 10\n                    string_var: string_value\n            type: object\n      title: Setting variables\n      type: object\n    - description: |\n        The public_paths and ignore_paths properties in the rill.yaml file provide control over which files and directories are processed or exposed by Rill. The public_paths property defines a list of file or directory paths to expose over HTTP. By default, it includes ['./public']. The ignore_paths property specifies a list of files or directories that Rill excludes during ingestion and parsing. This prevents unnecessary or incompatible content from affecting the project.\n        :::tip\n        Don't forget the leading `/` when specifying the path for `ignore_paths` and this path is also assuming the relative path from your project root.\n        :::\n      properties:\n        ignore_paths:\n            description: A list of file or directory paths to exclude from parsing. Useful for ignoring extraneous or non-Rill files in the project\n            examples:\n                - ignore_paths:\n                    - /path/to/ignore\n                    - /file_to_ignore.yaml\n            items:\n                type: string\n            type: array\n        public_paths:\n            description: List of file or directory paths to expose over HTTP. Defaults to ['./public']\n            items:\n                type: string\n            type: array\n      title: Managing Paths in Rill\n      type: object\n    - description: \"During development, it is always a good idea to check if your [access policies](/developers/build/metrics-view/security) are behaving the way you designed them to before pushing these changes into production. You can set mock users which enables a drop down in the dashboard preview to view as a specific user. \\n:::info The View as selector is not visible in my dashboard, why?\\nThis feature is _only_ enabled when you have set a security policy on the dashboard. By default, the dashboard and it's contents is viewable by every user.\\n:::\\n\"\n      properties:\n        mock_users:\n            description: A list of mock users used to test dashboard security policies within the project\n            examples:\n                - mock_users:\n                    - admin: true\n                      email: john@yourcompany.com\n                      name: John Doe\n                    - email: jane@partnercompany.com\n                      groups:\n                        - partners\n                    - email: anon@unknown.com\n                    - custom_variable_1: Value_1\n                      custom_variable_2: Value_2\n                      email: embed@rilldata.com\n                      name: embed\n            items:\n                properties:\n                    admin:\n                        description: Indicates whether the mock user has administrative privileges\n                        type: boolean\n                    email:\n                        description: The email address of the mock user. This field is required\n                        type: string\n                    groups:\n                        description: An array of group names that the mock user is a member of\n                        items:\n                            type: string\n                        type: array\n                    name:\n                        description: The name of the mock user.\n                        type: string\n                required:\n                    - email\n                type: object\n            type: array\n      title: Testing access policies\n      type: object\n    - properties:\n        dev:\n            description: Overrides any properties in development environment.\n            type: object\n        prod:\n            description: Overrides any properties in production environment.\n            type: object\n      title: Common Properties\n      type: object\ndescription: The `rill.yaml` file contains metadata about your project.\nid: rill-yaml.schema.yaml\ntitle: Project YAML\ntype: object\n```\n\n## Minimal Example\n\nA minimal `rill.yaml` for a new project:\n\n```yaml\ndisplay_name: My Analytics Project\n```\n\n## Complete Example\n\nA comprehensive `rill.yaml` demonstrating common configurations:\n\n```yaml\ndisplay_name: Sales Analytics\ndescription: Sales performance dashboards with partner access controls\n\nolap_connector: duckdb\n\n# Non-sensitive environment variables\nenv:\n  default_lookback: P30D\n  data_bucket: gs://my-company-data\n\n# Mock users for testing security policies locally\nmock_users:\n  - email: admin@mycompany.com\n    name: Admin User\n    admin: true\n  - email: partner@external.com\n    groups:\n      - partners\n  - email: viewer@mycompany.com\n    tenant_id: xyz\n\n# Project-wide defaults for models\nmodels:\n  refresh:\n    cron: 0 0 * * *\n\n# Project-wide defaults for metrics views\nmetrics_views:\n  smallest_time_grain: day\n\n# Project-wide defaults for explore dashboards\nexplores:\n  defaults:\n    time_range: P3M\n  time_zones:\n    - UTC\n    - America/New_York\n    - Europe/London\n  time_ranges:\n    - PT24H\n    - P7D\n    - P30D\n    - P3M\n    - P12M\n\n# Exclude non-Rill files from parsing\nignore_paths:\n  - /docs\n```"
    - role: user
      content_type: text
      content: |-
        You should develop a Rill project file based on the following task description: Update the project configuration to set the default OLAP connector to 'clickhouse' instead of 'duckdb'. Ensure that the 'olap_connector' property is set to 'clickhouse'. Retain any other existing configuration in rill.yaml.

        Here is some important context:
        - You should develop the file at path: /rill.yaml
        - The file should be of type: rill.yaml
        - You are running as a sub-agent of a larger developer agent. Stay aligned on your specific task, avoid extra discovery, and report back your success or failure to the parent agent.

        Here is some additional context that may or may not be relevant to your task:
        - The project's default OLAP connector is named duckdb (driver: duckdb).
  response:
    - role: assistant
      content_type: tool_call
      id: call_L7dxZ5lIQ9ah3mSxS4JD5Bkq
      tool_name: read_file
      content: '{"path":"/rill.yaml"}'
- index: 10
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. Non-materialized models become views, which re-execute on every query.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: system
      content_type: text
      content: "# Instructions for developing `rill.yaml`\n\n## Introduction\n\n`rill.yaml` is a required configuration file located at the root of every Rill project. It defines project-wide settings, similar to `package.json` in Node.js or `dbt_project.yml` in dbt.\n\n## Core Concepts\n\n### Project metadata\n\nThere are no required properties in `rill.yaml`, but it is common to configure:\n\n- `display_name`: Human-readable name shown in the UI\n- `description`: Brief description of the project's purpose\n- `compiler`: Deprecated property that is commonly found in old projects\n\n### Default OLAP connector\n\nThe `olap_connector` property sets the default OLAP database for the project. Models output to this connector by default, and metrics views query from it unless explicitly overridden.\n\nCommon values are `duckdb` or `clickhouse`. If not specified, Rill initializes a managed DuckDB database and uses it as the default OLAP connector. \n\n### Mock users for security testing\n\nThe `mock_users` property defines test users for validating security policies during local development. Each mock user can have:\n\n- `email` (required): The user's email address\n- `name`: Display name\n- `admin`: Boolean indicating admin privileges\n- `groups`: List of group memberships\n- Custom attributes for use in security policy expressions\n\nWhen mock users are defined and security policies exist, a \"View as\" dropdown appears in the dashboard preview.\n\n### Environment variables\n\nThe `env` property sets default values for non-sensitive variables. These can be referenced in resource files using templating syntax (`{{ .env.<variable> }}`). Sensitive secrets should go in `.env` instead.\n\n### Resource type defaults\n\nProject-wide defaults can be set for resource types using plural keys:\n\n- `models`: Default settings for all models (e.g., refresh schedules)\n- `metrics_views`: Default settings for all metrics views (e.g., `first_day_of_week`)\n- `explores`: Default settings for explore dashboards (e.g., `time_ranges`, `time_zones`)\n- `canvases`: Default settings for canvas dashboards\n\nIndividual resources can override these defaults.\n\n### Path management\n\n- `ignore_paths`: List of paths to exclude from parsing (use leading `/`)\n- `public_paths`: List of paths to expose over HTTP (defaults to `['./public']`)\n\n### Environment overrides\n\nThe `dev` and `prod` properties allow environment-specific configuration overrides.\n\n## JSON Schema\n\nHere is a full JSON schema for the `rill.yaml` syntax:\n\n```\n$schema: http://json-schema.org/draft-07/schema#\nallOf:\n    - properties:\n        ai_connector:\n            description: Specifies the default AI connector for the project. Defaults to Rill's internal AI connector if not set.\n            type: string\n        ai_instructions:\n            description: Extra instructions for LLM/AI features. Used to guide natural language question answering and routing.\n            type: string\n        compiler:\n            description: Specifies the parser version to use for compiling resources\n            type: string\n        description:\n            description: A brief description of the project\n            type: string\n        display_name:\n            description: The display name of the project, shown in the upper-left corner of the UI\n            type: string\n        features:\n            description: Optional feature flags. Can be specified as a map of feature names to booleans.\n            type: object\n      title: Properties\n      type: object\n    - description: |\n        Rill allows you to specify the default OLAP engine to use in your project via `rill.yaml`.\n        :::info Curious about OLAP Engines?\n        Please see our reference documentation on [OLAP Engines](/developers/build/connectors/olap).\n        :::\n      properties:\n        olap_connector:\n            description: Specifies the default OLAP engine for the project. Defaults to duckdb if not set.\n            examples:\n                - olap_connector: clickhouse\n            type: string\n      title: Configuring the default OLAP Engine\n      type: object\n    - description: |\n        In `rill.yaml`, project-wide defaults can be specified for a resource type within a project. Unless otherwise specified, _individual resources will inherit any defaults_ that have been specified in `rill.yaml`. For available properties that can be configured, please refer to the YAML specification for each individual resource type - [model](models.md), [metrics_view](metrics-views.md), and [explore](explore-dashboards.md)\n\n        :::note Use plurals when specifying project-wide defaults\n        In your `rill.yaml`, the top level property for the resource type needs to be **plural**, such as `models`, `metrics_views` and `explores`.\n        :::\n\n        :::info Hierarchy of inheritance and property overrides\n        As a general rule of thumb, properties that have been specified at a more _granular_ level will supercede or override higher level properties that have been inherited. Therefore, in order of inheritance, Rill will prioritize properties in the following order:\n        1. Individual [models](models.md)/[metrics_views](metrics-views.md)/[explore](explore-dashboards.md) object level properties (e.g. `models.yaml` or `explore-dashboards.yaml`)\n        2. [Environment](/developers/build/models/templating) level properties (e.g. a specific property that have been set for `dev`)\n        3. [Project-wide defaults](#project-wide-defaults) for a specific property and resource type\n        :::\n      properties:\n        canvases:\n            description: Defines project-wide default settings for canvases. Unless overridden, individual canvases will inherit these defaults.\n            examples:\n                - canvases:\n                    defaults:\n                        time_range: P7D\n                    time_ranges:\n                        - PT24H\n                        - P7D\n                    time_zones:\n                        - UTC\n                  explores:\n                    defaults:\n                        time_range: P24M\n                    time_ranges:\n                        - PT24H\n                        - P6M\n                    time_zones:\n                        - UTC\n                  metrics_views:\n                    first_day_of_week: 1\n                    smallest_time_grain: month\n                  models:\n                    refresh:\n                        cron: 0 * * * *\n            type: object\n        explores:\n            description: Defines project-wide default settings for explores. Unless overridden, individual explores will inherit these defaults.\n            type: object\n        metrics_views:\n            description: Defines project-wide default settings for metrics_views. Unless overridden, individual metrics_views will inherit these defaults.\n            type: object\n        models:\n            description: Defines project-wide default settings for models. Unless overridden, individual models will inherit these defaults.\n            type: object\n      title: Project-wide defaults\n      type: object\n    - description: \"Primarily useful for [templating](/developers/build/connectors/templating), variables can be set in the `rill.yaml` file directly. This allows variables to be set for your projects deployed to Rill Cloud while still being able to use different variable values locally if you prefer. \\n:::info Overriding variables locally\\nVariables also follow an order of precedence and can be overridden locally. By default, any variables defined will be inherited from `rill.yaml`. However, if you manually pass in a variable when starting Rill Developer locally via the CLI, this value will be used instead for the current instance of your running project:\\n```bash\\nrill start --env numeric_var=100 --env string_var=\\\"different_value\\\"\\n```\\n:::\\n:::tip Setting variables through `.env`\\nVariables can also be set through your project's `<RILL_PROJECT_HOME>/.env` file (or using the `rill env set` CLI command), such as:\\n```bash\\nvariable=xyz\\n```\\nSimilar to how [connector credentials can be pushed / pulled](/developers/build/connectors/credentials#pulling-credentials-and-variables-from-a-deployed-project-on-rill-cloud) from local to cloud or vice versa, project variables set locally in Rill Developer can be pushed to Rill Cloud and/or pulled back to your local instance from your deployed project by using the `rill env push` and `rill env pull` commands respectively.\\n:::\\n\"\n      properties:\n        env:\n            description: To define a variable in `rill.yaml`, pass in the appropriate key-value pair for the variable under the `env` key\n            examples:\n                - env:\n                    numeric_var: 10\n                    string_var: string_value\n            type: object\n      title: Setting variables\n      type: object\n    - description: |\n        The public_paths and ignore_paths properties in the rill.yaml file provide control over which files and directories are processed or exposed by Rill. The public_paths property defines a list of file or directory paths to expose over HTTP. By default, it includes ['./public']. The ignore_paths property specifies a list of files or directories that Rill excludes during ingestion and parsing. This prevents unnecessary or incompatible content from affecting the project.\n        :::tip\n        Don't forget the leading `/` when specifying the path for `ignore_paths` and this path is also assuming the relative path from your project root.\n        :::\n      properties:\n        ignore_paths:\n            description: A list of file or directory paths to exclude from parsing. Useful for ignoring extraneous or non-Rill files in the project\n            examples:\n                - ignore_paths:\n                    - /path/to/ignore\n                    - /file_to_ignore.yaml\n            items:\n                type: string\n            type: array\n        public_paths:\n            description: List of file or directory paths to expose over HTTP. Defaults to ['./public']\n            items:\n                type: string\n            type: array\n      title: Managing Paths in Rill\n      type: object\n    - description: \"During development, it is always a good idea to check if your [access policies](/developers/build/metrics-view/security) are behaving the way you designed them to before pushing these changes into production. You can set mock users which enables a drop down in the dashboard preview to view as a specific user. \\n:::info The View as selector is not visible in my dashboard, why?\\nThis feature is _only_ enabled when you have set a security policy on the dashboard. By default, the dashboard and it's contents is viewable by every user.\\n:::\\n\"\n      properties:\n        mock_users:\n            description: A list of mock users used to test dashboard security policies within the project\n            examples:\n                - mock_users:\n                    - admin: true\n                      email: john@yourcompany.com\n                      name: John Doe\n                    - email: jane@partnercompany.com\n                      groups:\n                        - partners\n                    - email: anon@unknown.com\n                    - custom_variable_1: Value_1\n                      custom_variable_2: Value_2\n                      email: embed@rilldata.com\n                      name: embed\n            items:\n                properties:\n                    admin:\n                        description: Indicates whether the mock user has administrative privileges\n                        type: boolean\n                    email:\n                        description: The email address of the mock user. This field is required\n                        type: string\n                    groups:\n                        description: An array of group names that the mock user is a member of\n                        items:\n                            type: string\n                        type: array\n                    name:\n                        description: The name of the mock user.\n                        type: string\n                required:\n                    - email\n                type: object\n            type: array\n      title: Testing access policies\n      type: object\n    - properties:\n        dev:\n            description: Overrides any properties in development environment.\n            type: object\n        prod:\n            description: Overrides any properties in production environment.\n            type: object\n      title: Common Properties\n      type: object\ndescription: The `rill.yaml` file contains metadata about your project.\nid: rill-yaml.schema.yaml\ntitle: Project YAML\ntype: object\n```\n\n## Minimal Example\n\nA minimal `rill.yaml` for a new project:\n\n```yaml\ndisplay_name: My Analytics Project\n```\n\n## Complete Example\n\nA comprehensive `rill.yaml` demonstrating common configurations:\n\n```yaml\ndisplay_name: Sales Analytics\ndescription: Sales performance dashboards with partner access controls\n\nolap_connector: duckdb\n\n# Non-sensitive environment variables\nenv:\n  default_lookback: P30D\n  data_bucket: gs://my-company-data\n\n# Mock users for testing security policies locally\nmock_users:\n  - email: admin@mycompany.com\n    name: Admin User\n    admin: true\n  - email: partner@external.com\n    groups:\n      - partners\n  - email: viewer@mycompany.com\n    tenant_id: xyz\n\n# Project-wide defaults for models\nmodels:\n  refresh:\n    cron: 0 0 * * *\n\n# Project-wide defaults for metrics views\nmetrics_views:\n  smallest_time_grain: day\n\n# Project-wide defaults for explore dashboards\nexplores:\n  defaults:\n    time_range: P3M\n  time_zones:\n    - UTC\n    - America/New_York\n    - Europe/London\n  time_ranges:\n    - PT24H\n    - P7D\n    - P30D\n    - P3M\n    - P12M\n\n# Exclude non-Rill files from parsing\nignore_paths:\n  - /docs\n```"
    - role: user
      content_type: text
      content: |-
        You should develop a Rill project file based on the following task description: Update the project configuration to set the default OLAP connector to 'clickhouse' instead of 'duckdb'. Ensure that the 'olap_connector' property is set to 'clickhouse'. Retain any other existing configuration in rill.yaml.

        Here is some important context:
        - You should develop the file at path: /rill.yaml
        - The file should be of type: rill.yaml
        - You are running as a sub-agent of a larger developer agent. Stay aligned on your specific task, avoid extra discovery, and report back your success or failure to the parent agent.

        Here is some additional context that may or may not be relevant to your task:
        - The project's default OLAP connector is named duckdb (driver: duckdb).
    - role: assistant
      content_type: tool_call
      id: 0314b3bc91a14fc1
      tool_name: read_file
      content: '{"path":"/rill.yaml"}'
    - role: tool
      content_type: tool_response
      id: 0314b3bc91a14fc1
      content: '{"Contents":""}'
  response:
    - role: assistant
      content_type: tool_call
      id: call_RdQl5veVCl2VgrQ724UjDJCG
      tool_name: write_file
      content: '{"contents":"olap_connector: clickhouse\n","path":"/rill.yaml"}'
- index: 11
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. Non-materialized models become views, which re-execute on every query.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: system
      content_type: text
      content: "# Instructions for developing `rill.yaml`\n\n## Introduction\n\n`rill.yaml` is a required configuration file located at the root of every Rill project. It defines project-wide settings, similar to `package.json` in Node.js or `dbt_project.yml` in dbt.\n\n## Core Concepts\n\n### Project metadata\n\nThere are no required properties in `rill.yaml`, but it is common to configure:\n\n- `display_name`: Human-readable name shown in the UI\n- `description`: Brief description of the project's purpose\n- `compiler`: Deprecated property that is commonly found in old projects\n\n### Default OLAP connector\n\nThe `olap_connector` property sets the default OLAP database for the project. Models output to this connector by default, and metrics views query from it unless explicitly overridden.\n\nCommon values are `duckdb` or `clickhouse`. If not specified, Rill initializes a managed DuckDB database and uses it as the default OLAP connector. \n\n### Mock users for security testing\n\nThe `mock_users` property defines test users for validating security policies during local development. Each mock user can have:\n\n- `email` (required): The user's email address\n- `name`: Display name\n- `admin`: Boolean indicating admin privileges\n- `groups`: List of group memberships\n- Custom attributes for use in security policy expressions\n\nWhen mock users are defined and security policies exist, a \"View as\" dropdown appears in the dashboard preview.\n\n### Environment variables\n\nThe `env` property sets default values for non-sensitive variables. These can be referenced in resource files using templating syntax (`{{ .env.<variable> }}`). Sensitive secrets should go in `.env` instead.\n\n### Resource type defaults\n\nProject-wide defaults can be set for resource types using plural keys:\n\n- `models`: Default settings for all models (e.g., refresh schedules)\n- `metrics_views`: Default settings for all metrics views (e.g., `first_day_of_week`)\n- `explores`: Default settings for explore dashboards (e.g., `time_ranges`, `time_zones`)\n- `canvases`: Default settings for canvas dashboards\n\nIndividual resources can override these defaults.\n\n### Path management\n\n- `ignore_paths`: List of paths to exclude from parsing (use leading `/`)\n- `public_paths`: List of paths to expose over HTTP (defaults to `['./public']`)\n\n### Environment overrides\n\nThe `dev` and `prod` properties allow environment-specific configuration overrides.\n\n## JSON Schema\n\nHere is a full JSON schema for the `rill.yaml` syntax:\n\n```\n$schema: http://json-schema.org/draft-07/schema#\nallOf:\n    - properties:\n        ai_connector:\n            description: Specifies the default AI connector for the project. Defaults to Rill's internal AI connector if not set.\n            type: string\n        ai_instructions:\n            description: Extra instructions for LLM/AI features. Used to guide natural language question answering and routing.\n            type: string\n        compiler:\n            description: Specifies the parser version to use for compiling resources\n            type: string\n        description:\n            description: A brief description of the project\n            type: string\n        display_name:\n            description: The display name of the project, shown in the upper-left corner of the UI\n            type: string\n        features:\n            description: Optional feature flags. Can be specified as a map of feature names to booleans.\n            type: object\n      title: Properties\n      type: object\n    - description: |\n        Rill allows you to specify the default OLAP engine to use in your project via `rill.yaml`.\n        :::info Curious about OLAP Engines?\n        Please see our reference documentation on [OLAP Engines](/developers/build/connectors/olap).\n        :::\n      properties:\n        olap_connector:\n            description: Specifies the default OLAP engine for the project. Defaults to duckdb if not set.\n            examples:\n                - olap_connector: clickhouse\n            type: string\n      title: Configuring the default OLAP Engine\n      type: object\n    - description: |\n        In `rill.yaml`, project-wide defaults can be specified for a resource type within a project. Unless otherwise specified, _individual resources will inherit any defaults_ that have been specified in `rill.yaml`. For available properties that can be configured, please refer to the YAML specification for each individual resource type - [model](models.md), [metrics_view](metrics-views.md), and [explore](explore-dashboards.md)\n\n        :::note Use plurals when specifying project-wide defaults\n        In your `rill.yaml`, the top level property for the resource type needs to be **plural**, such as `models`, `metrics_views` and `explores`.\n        :::\n\n        :::info Hierarchy of inheritance and property overrides\n        As a general rule of thumb, properties that have been specified at a more _granular_ level will supercede or override higher level properties that have been inherited. Therefore, in order of inheritance, Rill will prioritize properties in the following order:\n        1. Individual [models](models.md)/[metrics_views](metrics-views.md)/[explore](explore-dashboards.md) object level properties (e.g. `models.yaml` or `explore-dashboards.yaml`)\n        2. [Environment](/developers/build/models/templating) level properties (e.g. a specific property that have been set for `dev`)\n        3. [Project-wide defaults](#project-wide-defaults) for a specific property and resource type\n        :::\n      properties:\n        canvases:\n            description: Defines project-wide default settings for canvases. Unless overridden, individual canvases will inherit these defaults.\n            examples:\n                - canvases:\n                    defaults:\n                        time_range: P7D\n                    time_ranges:\n                        - PT24H\n                        - P7D\n                    time_zones:\n                        - UTC\n                  explores:\n                    defaults:\n                        time_range: P24M\n                    time_ranges:\n                        - PT24H\n                        - P6M\n                    time_zones:\n                        - UTC\n                  metrics_views:\n                    first_day_of_week: 1\n                    smallest_time_grain: month\n                  models:\n                    refresh:\n                        cron: 0 * * * *\n            type: object\n        explores:\n            description: Defines project-wide default settings for explores. Unless overridden, individual explores will inherit these defaults.\n            type: object\n        metrics_views:\n            description: Defines project-wide default settings for metrics_views. Unless overridden, individual metrics_views will inherit these defaults.\n            type: object\n        models:\n            description: Defines project-wide default settings for models. Unless overridden, individual models will inherit these defaults.\n            type: object\n      title: Project-wide defaults\n      type: object\n    - description: \"Primarily useful for [templating](/developers/build/connectors/templating), variables can be set in the `rill.yaml` file directly. This allows variables to be set for your projects deployed to Rill Cloud while still being able to use different variable values locally if you prefer. \\n:::info Overriding variables locally\\nVariables also follow an order of precedence and can be overridden locally. By default, any variables defined will be inherited from `rill.yaml`. However, if you manually pass in a variable when starting Rill Developer locally via the CLI, this value will be used instead for the current instance of your running project:\\n```bash\\nrill start --env numeric_var=100 --env string_var=\\\"different_value\\\"\\n```\\n:::\\n:::tip Setting variables through `.env`\\nVariables can also be set through your project's `<RILL_PROJECT_HOME>/.env` file (or using the `rill env set` CLI command), such as:\\n```bash\\nvariable=xyz\\n```\\nSimilar to how [connector credentials can be pushed / pulled](/developers/build/connectors/credentials#pulling-credentials-and-variables-from-a-deployed-project-on-rill-cloud) from local to cloud or vice versa, project variables set locally in Rill Developer can be pushed to Rill Cloud and/or pulled back to your local instance from your deployed project by using the `rill env push` and `rill env pull` commands respectively.\\n:::\\n\"\n      properties:\n        env:\n            description: To define a variable in `rill.yaml`, pass in the appropriate key-value pair for the variable under the `env` key\n            examples:\n                - env:\n                    numeric_var: 10\n                    string_var: string_value\n            type: object\n      title: Setting variables\n      type: object\n    - description: |\n        The public_paths and ignore_paths properties in the rill.yaml file provide control over which files and directories are processed or exposed by Rill. The public_paths property defines a list of file or directory paths to expose over HTTP. By default, it includes ['./public']. The ignore_paths property specifies a list of files or directories that Rill excludes during ingestion and parsing. This prevents unnecessary or incompatible content from affecting the project.\n        :::tip\n        Don't forget the leading `/` when specifying the path for `ignore_paths` and this path is also assuming the relative path from your project root.\n        :::\n      properties:\n        ignore_paths:\n            description: A list of file or directory paths to exclude from parsing. Useful for ignoring extraneous or non-Rill files in the project\n            examples:\n                - ignore_paths:\n                    - /path/to/ignore\n                    - /file_to_ignore.yaml\n            items:\n                type: string\n            type: array\n        public_paths:\n            description: List of file or directory paths to expose over HTTP. Defaults to ['./public']\n            items:\n                type: string\n            type: array\n      title: Managing Paths in Rill\n      type: object\n    - description: \"During development, it is always a good idea to check if your [access policies](/developers/build/metrics-view/security) are behaving the way you designed them to before pushing these changes into production. You can set mock users which enables a drop down in the dashboard preview to view as a specific user. \\n:::info The View as selector is not visible in my dashboard, why?\\nThis feature is _only_ enabled when you have set a security policy on the dashboard. By default, the dashboard and it's contents is viewable by every user.\\n:::\\n\"\n      properties:\n        mock_users:\n            description: A list of mock users used to test dashboard security policies within the project\n            examples:\n                - mock_users:\n                    - admin: true\n                      email: john@yourcompany.com\n                      name: John Doe\n                    - email: jane@partnercompany.com\n                      groups:\n                        - partners\n                    - email: anon@unknown.com\n                    - custom_variable_1: Value_1\n                      custom_variable_2: Value_2\n                      email: embed@rilldata.com\n                      name: embed\n            items:\n                properties:\n                    admin:\n                        description: Indicates whether the mock user has administrative privileges\n                        type: boolean\n                    email:\n                        description: The email address of the mock user. This field is required\n                        type: string\n                    groups:\n                        description: An array of group names that the mock user is a member of\n                        items:\n                            type: string\n                        type: array\n                    name:\n                        description: The name of the mock user.\n                        type: string\n                required:\n                    - email\n                type: object\n            type: array\n      title: Testing access policies\n      type: object\n    - properties:\n        dev:\n            description: Overrides any properties in development environment.\n            type: object\n        prod:\n            description: Overrides any properties in production environment.\n            type: object\n      title: Common Properties\n      type: object\ndescription: The `rill.yaml` file contains metadata about your project.\nid: rill-yaml.schema.yaml\ntitle: Project YAML\ntype: object\n```\n\n## Minimal Example\n\nA minimal `rill.yaml` for a new project:\n\n```yaml\ndisplay_name: My Analytics Project\n```\n\n## Complete Example\n\nA comprehensive `rill.yaml` demonstrating common configurations:\n\n```yaml\ndisplay_name: Sales Analytics\ndescription: Sales performance dashboards with partner access controls\n\nolap_connector: duckdb\n\n# Non-sensitive environment variables\nenv:\n  default_lookback: P30D\n  data_bucket: gs://my-company-data\n\n# Mock users for testing security policies locally\nmock_users:\n  - email: admin@mycompany.com\n    name: Admin User\n    admin: true\n  - email: partner@external.com\n    groups:\n      - partners\n  - email: viewer@mycompany.com\n    tenant_id: xyz\n\n# Project-wide defaults for models\nmodels:\n  refresh:\n    cron: 0 0 * * *\n\n# Project-wide defaults for metrics views\nmetrics_views:\n  smallest_time_grain: day\n\n# Project-wide defaults for explore dashboards\nexplores:\n  defaults:\n    time_range: P3M\n  time_zones:\n    - UTC\n    - America/New_York\n    - Europe/London\n  time_ranges:\n    - PT24H\n    - P7D\n    - P30D\n    - P3M\n    - P12M\n\n# Exclude non-Rill files from parsing\nignore_paths:\n  - /docs\n```"
    - role: user
      content_type: text
      content: |-
        You should develop a Rill project file based on the following task description: Update the project configuration to set the default OLAP connector to 'clickhouse' instead of 'duckdb'. Ensure that the 'olap_connector' property is set to 'clickhouse'. Retain any other existing configuration in rill.yaml.

        Here is some important context:
        - You should develop the file at path: /rill.yaml
        - The file should be of type: rill.yaml
        - You are running as a sub-agent of a larger developer agent. Stay aligned on your specific task, avoid extra discovery, and report back your success or failure to the parent agent.

        Here is some additional context that may or may not be relevant to your task:
        - The project's default OLAP connector is named duckdb (driver: duckdb).
    - role: assistant
      content_type: tool_call
      id: 0314b3bc91a14fc1
      tool_name: read_file
      content: '{"path":"/rill.yaml"}'
    - role: tool
      content_type: tool_response
      id: 0314b3bc91a14fc1
      content: '{"Contents":""}'
    - role: assistant
      content_type: tool_call
      id: aeb34db318244c79
      tool_name: write_file
      content: '{"contents":"olap_connector: clickhouse\n","path":"/rill.yaml"}'
    - role: tool
      content_type: tool_response
      id: aeb34db318244c79
      content: '{"diff":"--- /rill.yaml\n+++ /rill.yaml\n@@ -1 +1,2 @@\n+olap_connector: clickhouse\n \n"}'
  response:
    - role: assistant
      content_type: text
      content: 'The rill.yaml file has been updated to set olap_connector: clickhouse as the default OLAP connector, as requested. There was no other existing configuration to retain. Task complete.'
- index: 12
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. Non-materialized models become views, which re-execute on every query.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: user
      content_type: text
      content: |-
        This may not relate to the user's task, but for context, the project's default OLAP connector is named duckdb (driver: duckdb).
         The default OLAP is in readwrite mode, so you can use it in models if you want.


        Task: Clickhouse has a public playground database on play.clickhouse.com:9440 (username `play` and no password). Can you set it up as the OLAP connector and develop a dashboard based on one of its tables? Please proceed without asking clarifying questions.
    - role: assistant
      content_type: tool_call
      id: 5f915383ab074f81
      tool_name: develop_file
      content: '{"path":"connectors/clickhouse.yaml","prompt":"Create a connector for the public ClickHouse playground database at play.clickhouse.com:9440. Use the ''clickhouse'' driver. Set the username to ''play'' and leave the password blank. This connector will be used as an OLAP connector for dashboards. Set ''mode'' to ''read'' to prevent writes. Name the connector ''clickhouse''.","type":"connector"}'
    - role: tool
      content_type: tool_response
      id: 5f915383ab074f81
      content: '{"response":"The connector file /connectors/clickhouse.yaml was created as specified:\n\n- type: connector\n- driver: clickhouse\n- host: play.clickhouse.com\n- port: 9440\n- username: play\n- password: \"\" (blank)\n- mode: read\n\nThe file was written successfully. However, there is a reconcile error indicating a handshake failure, likely due to an SSL/TLS mismatch. If SSL is required for this endpoint, you may need to add ssl: true to the configuration. Please advise if you want to update the file to include SSL."}'
    - role: assistant
      content_type: tool_call
      id: 3925a34a97214026
      tool_name: develop_file
      content: '{"path":"connectors/clickhouse.yaml","prompt":"Update the ClickHouse connector configuration to include SSL support, as the public playground likely requires a secure connection. Add ''ssl: true'' to the YAML file. Ensure all other settings remain the same: driver clickhouse, host play.clickhouse.com, port 9440, username play, blank password, mode read.","type":"connector"}'
    - role: tool
      content_type: tool_response
      id: 3925a34a97214026
      content: '{"response":"Success: The ClickHouse connector configuration at /connectors/clickhouse.yaml has been updated to include ssl: true, as required for secure connections to the public playground. All other settings (driver, host, port, username, blank password, and mode) remain unchanged. No errors were encountered during the update."}'
    - role: assistant
      content_type: tool_call
      id: 83c850aa1fea4507
      tool_name: develop_file
      content: '{"path":"rill.yaml","prompt":"Update the project configuration to set the default OLAP connector to ''clickhouse'' instead of ''duckdb''. Ensure that the ''olap_connector'' property is set to ''clickhouse''. Retain any other existing configuration in rill.yaml.","type":"rill.yaml"}'
    - role: tool
      content_type: tool_response
      id: 83c850aa1fea4507
      content: '{"response":"The rill.yaml file has been updated to set olap_connector: clickhouse as the default OLAP connector, as requested. There was no other existing configuration to retain. Task complete."}'
  response:
    - role: assistant
      content_type: tool_call
      id: call_5IZiZ3E2NNdGF3oJSjVwF56o
      tool_name: list_tables
      content: '{"connector":"clickhouse","page_size":20}'
- index: 13
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. Non-materialized models become views, which re-execute on every query.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: user
      content_type: text
      content: |-
        This may not relate to the user's task, but for context, the project's default OLAP connector is named duckdb (driver: duckdb).
         The default OLAP is in readwrite mode, so you can use it in models if you want.


        Task: Clickhouse has a public playground database on play.clickhouse.com:9440 (username `play` and no password). Can you set it up as the OLAP connector and develop a dashboard based on one of its tables? Please proceed without asking clarifying questions.
    - role: assistant
      content_type: tool_call
      id: 5f915383ab074f81
      tool_name: develop_file
      content: '{"path":"connectors/clickhouse.yaml","prompt":"Create a connector for the public ClickHouse playground database at play.clickhouse.com:9440. Use the ''clickhouse'' driver. Set the username to ''play'' and leave the password blank. This connector will be used as an OLAP connector for dashboards. Set ''mode'' to ''read'' to prevent writes. Name the connector ''clickhouse''.","type":"connector"}'
    - role: tool
      content_type: tool_response
      id: 5f915383ab074f81
      content: '{"response":"The connector file /connectors/clickhouse.yaml was created as specified:\n\n- type: connector\n- driver: clickhouse\n- host: play.clickhouse.com\n- port: 9440\n- username: play\n- password: \"\" (blank)\n- mode: read\n\nThe file was written successfully. However, there is a reconcile error indicating a handshake failure, likely due to an SSL/TLS mismatch. If SSL is required for this endpoint, you may need to add ssl: true to the configuration. Please advise if you want to update the file to include SSL."}'
    - role: assistant
      content_type: tool_call
      id: 3925a34a97214026
      tool_name: develop_file
      content: '{"path":"connectors/clickhouse.yaml","prompt":"Update the ClickHouse connector configuration to include SSL support, as the public playground likely requires a secure connection. Add ''ssl: true'' to the YAML file. Ensure all other settings remain the same: driver clickhouse, host play.clickhouse.com, port 9440, username play, blank password, mode read.","type":"connector"}'
    - role: tool
      content_type: tool_response
      id: 3925a34a97214026
      content: '{"response":"Success: The ClickHouse connector configuration at /connectors/clickhouse.yaml has been updated to include ssl: true, as required for secure connections to the public playground. All other settings (driver, host, port, username, blank password, and mode) remain unchanged. No errors were encountered during the update."}'
    - role: assistant
      content_type: tool_call
      id: 83c850aa1fea4507
      tool_name: develop_file
      content: '{"path":"rill.yaml","prompt":"Update the project configuration to set the default OLAP connector to ''clickhouse'' instead of ''duckdb''. Ensure that the ''olap_connector'' property is set to ''clickhouse''. Retain any other existing configuration in rill.yaml.","type":"rill.yaml"}'
    - role: tool
      content_type: tool_response
      id: 83c850aa1fea4507
      content: '{"response":"The rill.yaml file has been updated to set olap_connector: clickhouse as the default OLAP connector, as requested. There was no other existing configuration to retain. Task complete."}'
    - role: assistant
      content_type: tool_call
      id: a902bdef4d944e31
      tool_name: list_tables
      content: '{"connector":"clickhouse","page_size":20}'
    - role: tool
      content_type: tool_response
      id: a902bdef4d944e31
      content: '{"tables":[{"database_schema":"blogs","name":"countries","is_view":false},{"database_schema":"blogs","name":"country_codes","is_view":false},{"database_schema":"blogs","name":"forex","is_view":false},{"database_schema":"blogs","name":"forex_2020s","is_view":false},{"database_schema":"blogs","name":"forex_usd","is_view":false},{"database_schema":"blogs","name":"noaa","is_view":false},{"database_schema":"blogs","name":"noaa_v2","is_view":false},{"database_schema":"blogs","name":"stop_words","is_view":false},{"database_schema":"blogs","name":"uk_codes","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"actors","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"benchmark_results","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"benchmark_runs","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"cell_towers","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"checks","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"checks_coverage_inverted","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"checks_v2","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"cisco_umbrella","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"covid","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"dish","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"dns","is_view":false}],"next_page_token":"ImRlZmF1bHQiCiJkbnMiCg"}'
  response:
    - role: assistant
      content_type: tool_call
      id: call_tlnwXTwtI6XvwUTMxDxxmyYO
      tool_name: show_table
      content: '{"connector":"clickhouse","database_schema":"default","table":"covid"}'
- index: 14
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. Non-materialized models become views, which re-execute on every query.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: user
      content_type: text
      content: |-
        This may not relate to the user's task, but for context, the project's default OLAP connector is named duckdb (driver: duckdb).
         The default OLAP is in readwrite mode, so you can use it in models if you want.


        Task: Clickhouse has a public playground database on play.clickhouse.com:9440 (username `play` and no password). Can you set it up as the OLAP connector and develop a dashboard based on one of its tables? Please proceed without asking clarifying questions.
    - role: assistant
      content_type: tool_call
      id: 5f915383ab074f81
      tool_name: develop_file
      content: '{"path":"connectors/clickhouse.yaml","prompt":"Create a connector for the public ClickHouse playground database at play.clickhouse.com:9440. Use the ''clickhouse'' driver. Set the username to ''play'' and leave the password blank. This connector will be used as an OLAP connector for dashboards. Set ''mode'' to ''read'' to prevent writes. Name the connector ''clickhouse''.","type":"connector"}'
    - role: tool
      content_type: tool_response
      id: 5f915383ab074f81
      content: '{"response":"The connector file /connectors/clickhouse.yaml was created as specified:\n\n- type: connector\n- driver: clickhouse\n- host: play.clickhouse.com\n- port: 9440\n- username: play\n- password: \"\" (blank)\n- mode: read\n\nThe file was written successfully. However, there is a reconcile error indicating a handshake failure, likely due to an SSL/TLS mismatch. If SSL is required for this endpoint, you may need to add ssl: true to the configuration. Please advise if you want to update the file to include SSL."}'
    - role: assistant
      content_type: tool_call
      id: 3925a34a97214026
      tool_name: develop_file
      content: '{"path":"connectors/clickhouse.yaml","prompt":"Update the ClickHouse connector configuration to include SSL support, as the public playground likely requires a secure connection. Add ''ssl: true'' to the YAML file. Ensure all other settings remain the same: driver clickhouse, host play.clickhouse.com, port 9440, username play, blank password, mode read.","type":"connector"}'
    - role: tool
      content_type: tool_response
      id: 3925a34a97214026
      content: '{"response":"Success: The ClickHouse connector configuration at /connectors/clickhouse.yaml has been updated to include ssl: true, as required for secure connections to the public playground. All other settings (driver, host, port, username, blank password, and mode) remain unchanged. No errors were encountered during the update."}'
    - role: assistant
      content_type: tool_call
      id: 83c850aa1fea4507
      tool_name: develop_file
      content: '{"path":"rill.yaml","prompt":"Update the project configuration to set the default OLAP connector to ''clickhouse'' instead of ''duckdb''. Ensure that the ''olap_connector'' property is set to ''clickhouse''. Retain any other existing configuration in rill.yaml.","type":"rill.yaml"}'
    - role: tool
      content_type: tool_response
      id: 83c850aa1fea4507
      content: '{"response":"The rill.yaml file has been updated to set olap_connector: clickhouse as the default OLAP connector, as requested. There was no other existing configuration to retain. Task complete."}'
    - role: assistant
      content_type: tool_call
      id: a902bdef4d944e31
      tool_name: list_tables
      content: '{"connector":"clickhouse","page_size":20}'
    - role: tool
      content_type: tool_response
      id: a902bdef4d944e31
      content: '{"tables":[{"database_schema":"blogs","name":"countries","is_view":false},{"database_schema":"blogs","name":"country_codes","is_view":false},{"database_schema":"blogs","name":"forex","is_view":false},{"database_schema":"blogs","name":"forex_2020s","is_view":false},{"database_schema":"blogs","name":"forex_usd","is_view":false},{"database_schema":"blogs","name":"noaa","is_view":false},{"database_schema":"blogs","name":"noaa_v2","is_view":false},{"database_schema":"blogs","name":"stop_words","is_view":false},{"database_schema":"blogs","name":"uk_codes","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"actors","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"benchmark_results","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"benchmark_runs","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"cell_towers","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"checks","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"checks_coverage_inverted","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"checks_v2","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"cisco_umbrella","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"covid","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"dish","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"dns","is_view":false}],"next_page_token":"ImRlZmF1bHQiCiJkbnMiCg"}'
    - role: assistant
      content_type: tool_call
      id: 46401ed1773746ab
      tool_name: show_table
      content: '{"connector":"clickhouse","database_schema":"default","table":"covid"}'
    - role: tool
      content_type: tool_response
      id: 46401ed1773746ab
      content: '{"name":"covid","is_view":false,"columns":[{"name":"date","type":"CODE_DATE"},{"name":"location_key","type":"CODE_STRING"},{"name":"new_confirmed","type":"CODE_INT32"},{"name":"new_deceased","type":"CODE_INT32"},{"name":"new_recovered","type":"CODE_INT32"},{"name":"new_tested","type":"CODE_INT32"},{"name":"cumulative_confirmed","type":"CODE_INT32"},{"name":"cumulative_deceased","type":"CODE_INT32"},{"name":"cumulative_recovered","type":"CODE_INT32"},{"name":"cumulative_tested","type":"CODE_INT32"}],"physical_size_bytes":-1}'
  response:
    - role: assistant
      content_type: tool_call
      id: call_s9Qshtgr9MmL9uaQbBtLcehi
      tool_name: develop_file
      content: '{"path":"metrics/covid.yaml","prompt":"Create a metrics view for the covid table in the ClickHouse playground (schema: default, table: covid). Use the clickhouse connector. Define the following dimensions: date (date), location_key (string). Define the following measures: new_confirmed (sum), new_deceased (sum), new_recovered (sum), new_tested (sum), cumulative_confirmed (max), cumulative_deceased (max), cumulative_recovered (max), cumulative_tested (max). Name the metrics view ''covid''.","type":"metrics_view"}'
- index: 15
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. Non-materialized models become views, which re-execute on every query.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: system
      content_type: text
      content: "# Instructions for developing a metrics view in Rill\n\n## Introduction\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database. They implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\n\nMetrics views are lightweight resources that only perform validation when reconciled. They are typically found downstream of connectors and models in the project's DAG. They power many user-facing features:\n\n- **Explore dashboards**: Interactive drill-down interfaces for data exploration\n- **Canvas dashboards**: Custom chart and table components\n- **Alerts**: Notifications when data meets certain criteria\n- **Reports**: Scheduled data exports and summaries\n- **Custom APIs**: Programmatic access to metrics\n\n## Core Concepts\n\n### Table source\n\nThe `model:` property specifies the underlying table that powers the metrics view. It can reference:\n\n1. **A model in the project**: Just use the model name (e.g., `model: events`)\n2. **An external table**: Specify the table name as it exists in the OLAP connector\n\n```yaml\n# Referencing a model in the project\nmodel: events\n\n# Referencing an external table (connector defaults to project's default OLAP)\nconnector: clickhouse  # Optional: specify if different from default\nmodel: my_external_table\n```\n\n**Note**: The `table:` property is a legacy alias for referencing external tables. Always prefer `model:` in new metrics views.\n\n### Timeseries\n\nThe `timeseries:` property identifies the timestamp column used for time-based filtering and line charts. This column must be a time/timestamp type in the underlying table.\n\n```yaml\ntimeseries: event_time\n```\n\nIf the timeseries column is not listed in `dimensions:`, Rill automatically adds it as a time dimension. You can optionally configure additional time-related settings:\n\n```yaml\ntimeseries: event_time\nsmallest_time_grain: hour      # Minimum granularity users can select\nfirst_day_of_week: 7           # Sunday (1=Monday, 7=Sunday)\nfirst_month_of_year: 4         # April (fiscal year starting in April)\n```\n\n### Dimensions\n\nDimensions are attributes you can group by or filter on. They are typically categorical (strings, enums) or temporal (dates, timestamps). Rill infers the dimension type from the underlying SQL data type:\n\n- **Categorical**: String, enum, boolean columns\n- **Time**: Timestamp, date, datetime columns\n- **Geospatial**: Geometry or geography columns\n\nDefine dimensions using either a direct column reference or a SQL expression:\n\n```yaml\ndimensions:\n  # Simple column reference\n  - name: country\n    column: country\n\n  # Computed expression\n  - name: device_category\n    expression: CASE WHEN device_type IN ('phone', 'tablet') THEN 'Mobile' ELSE 'Desktop' END\n\n  # With display name and description\n  - name: campaign_name\n    display_name: Campaign\n    description: Marketing campaign that drove the traffic\n    column: campaign_name\n```\n\n**Naming**: Each dimension needs a `name` (stable identifier used in APIs and references), which defaults to `column:` if provided. The `display_name:` is optional, and defaults to a humanized version of `name` if not specified.\n\n### Measures\n\nMeasures are aggregation expressions that compute numeric values when grouped by dimensions. They must use aggregate functions like `SUM()`, `COUNT()`, `AVG()`, `MIN()`, `MAX()`.\n\n```yaml\nmeasures:\n  - name: total_revenue\n    display_name: Total Revenue\n    expression: SUM(revenue)\n    description: Sum of all revenue in USD\n    format_preset: currency_usd\n\n  - name: unique_users\n    display_name: Unique Users\n    expression: COUNT(DISTINCT user_id)\n    format_preset: humanize\n\n  - name: conversion_rate\n    display_name: Conversion Rate\n    expression: SUM(conversions) / NULLIF(SUM(visits), 0)\n    format_preset: percentage\n    valid_percent_of_total: false  # Disable % of total for ratios\n```\n\n**Format presets**: Control how values are displayed:\n- `none`: Raw number\n- `humanize`: Round to K, M, B (e.g., 1.2M)\n- `currency_usd`: Dollar format with 2 decimals ($1,234.56)\n- `currency_eur`: Euro format\n- `percentage`: Multiply by 100 and add % sign\n- `interval_ms`: Convert milliseconds to human-readable duration\n\nFor custom formatting, use `format_d3` with a [d3-format](https://d3js.org/d3-format) string:\n\n```yaml\nformat_d3: \"$,.2f\"  # $1,234.56\nformat_d3: \".1%\"    # 12.3%\nformat_d3: \",.0f\"   # 1,235 (rounded, with thousands separator)\n```\n\n### Best practices for dimensions and measures\n\n**Naming conventions:**\n- Use `snake_case` for the `name` field (e.g., `total_revenue`, `unique_users`)\n- Only add `display_name` and `description` if they provide meaningful context beyond what `name` conveys (display names auto-humanize from the name by default)\n- Ensure measure names don't collide with column names in the underlying table\n\n**Getting started with measures:**\n- Start with a `COUNT(*)` measure as a baseline (e.g., `total_records` or `total_events`)\n- Add `SUM()` measures for numeric columns that represent quantities or values\n- Use `humanize` as the default format preset unless the data has a specific format requirement\n- Keep initial measures simple using only `COUNT`, `SUM`, `AVG`, `MIN`, `MAX` aggregations\n- Add more complex expressions (ratios, conditional aggregations) only when needed\n\n**Dimension selection:**\n- Include all categorical columns (strings, enums, booleans) that users might want to filter or group by\n- Include date/timestamp columns as dimensions for time-based analysis beyond the main timeseries\n- Start with 5-10 dimensions; add more based on user needs\n\n### Auto-generated explore\n\nWhen you create a metrics view, Rill automatically generates an explore dashboard with the same name, exposing all dimensions and measures. To customize the explore (you usually should not need to), add an `explore:` block:\n\n```yaml\nexplore:\n  display_name: Sales Dashboard\n  defaults:\n    time_range: P7D\n    measures:\n      - total_revenue\n      - order_count\n```\n\n**Legacy behavior**: Files with `version: 1` do NOT auto-generate an explore. Omit `version:` in new metrics views to get the auto-generated explore.\n\n## Full Example\n\nHere is a complete, annotated metrics view:\n\n```yaml\n# metrics/orders.yaml\ntype: metrics_view\n\n# Display metadata\ndisplay_name: Orders Analytics\ndescription: Analyze order performance by various dimensions\n\n# Data source - references the 'orders' model in the project\nmodel: orders\n\n# Time column for time-series charts and filtering\ntimeseries: order_date\nsmallest_time_grain: day\n\n# Dimensions for grouping and filtering\ndimensions:\n  - name: order_date\n    display_name: Order Date\n    column: order_date\n\n  - name: country\n    display_name: Country\n    column: shipping_country\n\n  - name: product_category\n    display_name: Product Category\n    column: category\n    description: High-level product grouping\n\n  - name: customer_segment\n    display_name: Customer Segment\n    expression: | \n      CASE\n        WHEN lifetime_value > 1000 THEN 'High Value'\n        WHEN lifetime_value > 100 THEN 'Medium Value'\n        ELSE 'Low Value'\n      END\n\n  - name: is_repeat_customer\n    display_name: Repeat Customer\n    expression: CASE WHEN order_number > 1 THEN 'Yes' ELSE 'No' END\n\n# Measures for aggregation\nmeasures:\n  - name: total_orders\n    display_name: Total Orders\n    expression: COUNT(*)\n    format_preset: humanize\n\n  - name: total_revenue\n    display_name: Total Revenue\n    expression: SUM(order_total)\n    format_preset: currency_usd\n    description: Gross revenue before refunds\n\n  - name: average_order_value\n    display_name: Avg Order Value\n    expression: SUM(order_total) / NULLIF(COUNT(*), 0)\n    format_preset: currency_usd\n    valid_percent_of_total: false\n\n  - name: unique_customers\n    display_name: Unique Customers\n    expression: COUNT(DISTINCT customer_id)\n    format_preset: humanize\n\n  - name: items_per_order\n    display_name: Items per Order\n    expression: SUM(item_count) / NULLIF(COUNT(*), 0)\n    format_d3: \",.1f\"\n    valid_percent_of_total: false\n```\n\n## Security Policies\n\nSecurity policies control who can access a metrics view and what data they can see. This is a powerful feature for multi-tenant dashboards and role-based access control.\n\n### Basic access control\n\nThe `access:` property controls whether users can view the metrics view at all:\n\n```yaml\nsecurity:\n  # Allow access for everyone\n  access: true\n\n  # Deny access for everyone (useful for draft dashboards)\n  access: false\n\n  # Conditional access based on user attributes\n  access: \"'{{ .user.admin }}' = 'true'\"\n```\n\nThe expression syntax should be a DuckDB expression, which will be evaluated in a sandbox without access to any tables.\n\n### Row-level security\n\nThe `row_filter:` property restricts which rows a user can see. It's a SQL expression that references user attributes via templating:\n\n```yaml\nsecurity:\n  access: true\n  row_filter: domain = '{{ .user.domain }}'\n```\n\nCommon user attributes:\n- `{{ .user.email }}`: User's email address\n- `{{ .user.domain }}`: Email domain (e.g., \"acme.com\")\n- `{{ .user.admin }}`: Boolean admin flag\n- Custom attributes configured in Rill Cloud\n\nThe row filter should use the SQL syntax of the metrics view's model, and can reference other tables in the model's connector.\n\n### Complex row filters\n\nUse logical operators for sophisticated access patterns:\n\n```yaml\nsecurity:\n  access: true\n  row_filter: >\n    {{ .user.admin }}\n    OR '{{ .user.domain }}' = 'acme.com'\n    {{ if hasKey .user \"tenant_id\" }}\n    OR tenant_id = '{{ .user.tenant_id }}'\n    {{ end }}\n```\n\n### Hiding dimensions and measures\n\nThe `exclude:` property conditionally hides specific dimensions or measures from certain users:\n\n```yaml\nsecurity:\n  access: true\n  exclude:\n    - if: \"NOT {{ .user.admin }}\"\n      names:\n        - cost_per_acquisition  # Hide sensitive cost data from non-admins\n        - internal_notes\n```\n\n## Advanced Features\n\n### Annotations\n\nAnnotations overlay contextual information (like events or milestones) on time-series charts:\n\n```yaml\nannotations:\n  - name: product_launches\n    model: product_launches  # Must have 'time' and 'description' columns\n    measures:\n      - total_revenue        # Only show on these measures\n\n  # Optional columns in annotation model:\n  # - time_end: For range annotations\n  # - grain: Show only at specific time grains (day, week, etc.)\n```\n\n### Unnest for array dimensions\n\nWhen a column contains arrays, use `unnest: true` to flatten it at query time:\n\n```yaml\ndimensions:\n  - name: tags\n    display_name: Tags\n    column: tags\n    unnest: true  # Allows filtering by individual array elements\n```\n\n### Cache configuration\n\nConfigure caching for slow metrics views that use external tables:\n\n```yaml\ncache:\n  enabled: true\n  key_ttl: 5m\n  key_sql: SELECT MAX(updated_at) FROM orders\n```\n\nYou should not add a `cache:` config when the metrics view references a model inside the project since Rill does automatic cache management in that case.\n\n## Dialect-Specific Notes\n\nSQL expressions in dimensions and measures use the underlying OLAP database's dialect.\n\n### DuckDB\n\nDuckDB is the default OLAP engine for local development.\n\n**Conditional aggregation with FILTER**:\n```yaml\n# DuckDB supports FILTER clause for conditional aggregation\nexpression: COUNT(*) FILTER (WHERE status = 'completed')\n```\n\n### ClickHouse\n\nClickHouse is recommended for production workloads with large datasets.\n\n**Conditional aggregation**:\n```yaml\n# ClickHouse uses IF or CASE inside aggregations\nexpression: countIf(status = 'completed')\nexpression: sumIf(revenue, status = 'completed')\n```\n\n**Date functions**:\n```yaml\nexpression: toYear(order_date)\nexpression: toStartOfMonth(order_date)\nexpression: toYYYYMMDD(order_date)\n```\n\n**Array functions**:\n```yaml\nexpression: arrayJoin(tags)  # Unnest arrays\n```\n\n### Druid\n\n**Approximate distinct counts**:\n```yaml\nexpression: APPROX_COUNT_DISTINCT_DS_HLL(user_id)\n```\n\n## JSON Schema\n\n```\nallOf:\n    - properties:\n        ai_instructions:\n            description: Extra instructions for [AI agents](/explore/mcp). Used to guide natural language question answering and routing.\n            type: string\n        annotations:\n            description: Used to define annotations that can be displayed on charts\n            items:\n                properties:\n                    connector:\n                        description: Refers to the connector to use for the annotation\n                        type: string\n                    database:\n                        description: Refers to the database to use in the OLAP engine (to be used in conjunction with table). Otherwise, will use the default database or schema if not specified\n                        type: string\n                    database_schema:\n                        description: Refers to the schema to use in the OLAP engine (to be used in conjunction with table). Otherwise, will use the default database or schema if not specified\n                        type: string\n                    measures:\n                        anyOf:\n                            - description: Simple field name as a string.\n                              type: string\n                            - description: List of field selectors, each can be a string or an object with detailed configuration.\n                              items:\n                                anyOf:\n                                    - description: Shorthand field selector, interpreted as the name.\n                                      type: string\n                                    - additionalProperties: false\n                                      description: Detailed field selector configuration with name and optional time grain.\n                                      properties:\n                                        name:\n                                            description: Name of the field to select.\n                                            type: string\n                                        time_grain:\n                                            description: Time grain for time-based dimensions.\n                                            enum:\n                                                - \"\"\n                                                - ms\n                                                - millisecond\n                                                - s\n                                                - second\n                                                - min\n                                                - minute\n                                                - hadditionalProperties: fal\n                                                - hour\n                                                - d\n                                                - day\n                                                - w\n                                                - week\n                                                - month\n                                                - q\n                                                - quarter\n                                                - \"y\"\n                                                - year\n                                            type: string\n                                      required:\n                                        - name\n                                      type: object\n                              type: array\n                        description: Specifies which measures to apply the annotation to. Applies to all measures if not specified\n                    model:\n                        description: Refers to the model powering the annotation (either table or model is required). The model must have 'time' and 'description' columns. Optional columns include 'time_end' for range annotations and 'grain' to specify when the annotation should appear based on dashboard grain level.\n                        type: string\n                    name:\n                        description: A stable identifier for the annotation. Defaults to model or table names when not specified\n                        type: string\n                    table:\n                        description: Refers to the table powering the annotation, should be used instead of model for annotations from external OLAP tables (either table or model is required)\n                        type: string\n                type: object\n            type: array\n        connector:\n            description: Refers to the connector type for the metrics view, see [OLAP engines](/developers/build/connectors/olap) for more information\n            type: string\n        database:\n            description: Refers to the database to use in the OLAP engine (to be used in conjunction with table). Otherwise, will use the default database or schema if not specified\n            type: string\n        database_schema:\n            description: Refers to the schema to use in the OLAP engine (to be used in conjunction with table). Otherwise, will use the default database or schema if not specified\n            type: string\n        description:\n            description: Refers to the description for the metrics view\n            type: string\n        dimensions:\n            description: Relates to exploring segments or dimensions of your data and filtering the dashboard\n            items:\n                anyOf:\n                    - required:\n                        - column\n                    - required:\n                        - expression\n                properties:\n                    column:\n                        description: a categorical column\n                        type: string\n                    description:\n                        description: a freeform text description of the dimension\n                        type: string\n                    display_name:\n                        description: a display name for your dimension\n                        type: string\n                    expression:\n                        description: a non-aggregate expression such as string_split(domain, '.'). One of column and expression is required but cannot have both at the same time\n                        type: string\n                    name:\n                        description: a stable identifier for the dimension\n                        type: string\n                    tags:\n                        description: optional list of tags for categorizing the dimension (defaults to empty)\n                        items:\n                            type: string\n                        type: array\n                    type:\n                        description: 'Dimension type: \"geo\" for geospatial dimensions, \"time\" for time dimensions or \"categorical\" for categorial dimensions. Default is undefined and the type will be inferred instead'\n                        type: string\n                    unnest:\n                        description: if true, allows multi-valued dimension to be unnested (such as lists) and filters will automatically switch to \"contains\" instead of exact match\n                        type: boolean\n                    uri:\n                        description: enable if your dimension is a clickable URL to enable single click navigation (boolean or valid SQL expression)\n                        type:\n                            - string\n                            - boolean\n                type: object\n            type: array\n        display_name:\n            description: Refers to the display name for the metrics view\n            type: string\n        explore:\n            $ref: '#/definitions/explore_properties'\n            description: Defines an optional inline explore view for the metrics view. If not specified a default explore will be emitted unless `skip` is set to true.\n            required:\n                - type\n        first_day_of_week:\n            description: Refers to the first day of the week for time grain aggregation (for example, Sunday instead of Monday). The valid values are 1 through 7 where Monday=1 and Sunday=7\n            type: integer\n        first_month_of_year:\n            description: Refers to the first month of the year for time grain aggregation. The valid values are 1 through 12 where January=1 and December=12\n            type: integer\n        measures:\n            description: Used to define the numeric aggregates of columns from your data model\n            items:\n                properties:\n                    description:\n                        description: a freeform text description of the measure\n                        type: string\n                    display_name:\n                        description: the display name of your measure.\n                        type: string\n                    expression:\n                        description: a combination of operators and functions for aggregations\n                        type: string\n                    format_d3:\n                        description: 'Controls the formatting of this measure using a [d3-format](https://d3js.org/d3-format) string. If an invalid format string is supplied, the measure will fall back to `format_preset: humanize`. A measure cannot have both `format_preset` and `format_d3`. If neither is provided, the humanize preset is used by default. Example: `format_d3: \".2f\"` formats using fixed-point notation with two decimal places. Example: `format_d3: \",.2r\"` formats using grouped thousands with two significant digits. (optional)'\n                        type: string\n                    format_d3_locale:\n                        description: |\n                            locale configuration passed through to D3, enabling changing the currency symbol among other things. For details, see the docs for D3's formatLocale.\n                              ```yaml\n                              format_d3: \"$,\"\n                              format_d3_locale:\n                                grouping: [3, 2]\n                                currency: [\"\", \"\"]\n                              ```\n                        properties:\n                            currency:\n                                description: the currency symbol\n                                type: array\n                            grouping:\n                                description: the grouping of the currency symbol\n                                type: array\n                        type: object\n                    format_preset:\n                        description: |\n                            Controls the formatting of this measure using a predefined preset. Measures cannot have both `format_preset` and `format_d3`. If neither is supplied, the measure will be formatted using the `humanize` preset by default.\n\n                              Available options:\n                              - `humanize`: Round numbers into thousands (K), millions(M), billions (B), etc.\n                              - `none`: Raw output.\n                              - `currency_usd`: Round to 2 decimal points with a dollar sign ($).\n                              - `currency_eur`: Round to 2 decimal points with a euro sign ().\n                              - `percentage`: Convert a rate into a percentage with a % sign.\n                              - `interval_ms`: Convert milliseconds into human-readable durations like hours (h), days (d), years (y), etc. (optional)\n                        type: string\n                    label:\n                        description: a label for your measure, deprecated use display_name\n                        type: string\n                    name:\n                        description: a stable identifier for the measure\n                        type: string\n                    per:\n                        $ref: '#/definitions/field_selectors_properties'\n                        description: for per dimensions\n                    requires:\n                        $ref: '#/definitions/field_selectors_properties'\n                        description: using an available measure or dimension in your metrics view to set a required parameter, cannot be used with simple measures. See [referencing measures](/developers/build/metrics-view/measures/referencing) for more information.\n                    tags:\n                        description: optional list of tags for categorizing the measure (defaults to empty)\n                        items:\n                            type: string\n                        type: array\n                    treat_nulls_as:\n                        description: used to configure what value to fill in for missing time buckets. This also works generally as COALESCING over non empty time buckets.\n                        type: string\n                    type:\n                        description: 'Measure calculation type: \"simple\" for basic aggregations, \"derived\" for calculations using other measures, or \"time_comparison\" for period-over-period analysis. Defaults to \"simple\" unless dependencies exist.'\n                        type: string\n                    valid_percent_of_total:\n                        description: a boolean indicating whether percent-of-total values should be rendered for this measure\n                        type: boolean\n                    window:\n                        anyOf:\n                            - description: 'Shorthand: `time` or `true` means time-partitioned, `all` means non-partitioned.'\n                              enum:\n                                - time\n                                - \"true\"\n                                - all\n                              type: string\n                            - additionalProperties: false\n                              description: Detailed window configuration for measure calculations, allowing control over partitioning, ordering, and frame definition.\n                              properties:\n                                frame:\n                                    description: Defines the window frame boundaries for calculations, specifying which rows are included in the window relative to the current row.\n                                    type: string\n                                order:\n                                    $ref: '#/definitions/field_selectors_properties'\n                                    description: Specifies the fields to order the window by, determining the sequence of rows within each partition.\n                                    type: string\n                                partition:\n                                    description: Controls whether the window is partitioned. When true, calculations are performed within each partition separately.\n                                    type: boolean\n                              type: object\n                        description: A measure window can be defined as a keyword string (e.g. 'time' or 'all') or an object with detailed window configuration. For more information, see the [window functions](/developers/build/metrics-view/measures/windows) documentation.\n                required:\n                    - name\n                    - display_name\n                    - expression\n                type: object\n            type: array\n        model:\n            description: Refers to the model powering the dashboard (either model or table is required)\n            type: string\n        parent:\n            description: Refers to the parent metrics view from which this metrics view is derived. If specified, this will inherit properties from the parent metrics view\n            type: string\n        parent_dimensions:\n            $ref: '#/definitions/field_selector_properties'\n            description: Optional field selectors for dimensions to inherit from the parent metrics view.\n        parent_measures:\n            $ref: '#/definitions/field_selector_properties'\n            description: Optional field selectors for measures to inherit from the parent metrics view.\n        security:\n            $ref: '#/definitions/security_policy_properties'\n            description: Defines a security policy for the dashboard\n        smallest_time_grain:\n            description: 'Refers to the smallest time granularity the user is allowed to view. The valid values are: millisecond, second, minute, hour, day, week, month, quarter, year'\n            type: string\n        table:\n            description: Refers to the table powering the dashboard, should be used instead of model for dashboards create from external OLAP tables (either table or model is required)\n            type: string\n        timeseries:\n            description: Refers to the timestamp column from your model that will underlie x-axis data in the line charts. If not specified, the line charts will not appear\n            type: string\n        type:\n            const: metrics_view\n            description: Refers to the resource type and must be `metrics_view`\n            type: string\n        version:\n            description: The version of the metrics view schema\n            type: string\n        watermark:\n            description: A SQL expression that tells us the max timestamp that the measures are considered valid for. Usually does not need to be overwritten\n            type: string\n      title: Properties\n      type: object\n    - $ref: '#/definitions/common_properties'\ndescription: In your Rill project directory, create a metrics view, `<metrics_view>.yaml`, file in the `metrics` directory. Rill will ingest the metric view definition next time you run `rill start`.\nid: metrics-views\ntitle: Metrics View YAML\ntype: object\n```"
    - role: user
      content_type: text
      content: |-
        You should develop a Rill project file based on the following task description: Create a metrics view for the covid table in the ClickHouse playground (schema: default, table: covid). Use the clickhouse connector. Define the following dimensions: date (date), location_key (string). Define the following measures: new_confirmed (sum), new_deceased (sum), new_recovered (sum), new_tested (sum), cumulative_confirmed (max), cumulative_deceased (max), cumulative_recovered (max), cumulative_tested (max). Name the metrics view 'covid'.

        Here is some important context:
        - You should develop the file at path: /metrics/covid.yaml
        - The file should be of type: metrics_view
        - You are running as a sub-agent of a larger developer agent. Stay aligned on your specific task, avoid extra discovery, and report back your success or failure to the parent agent.

        Here is some additional context that may or may not be relevant to your task:
        - The project's default OLAP connector is named clickhouse (driver: clickhouse).
  response:
    - role: assistant
      content_type: tool_call
      id: call_9z7sGvSnzHEE9ee38ThjPaSQ
      tool_name: write_file
      content: '{"contents":"type: metrics_view\nmodel: covid\nconnector: clickhouse\ndatabase: default\ndimensions:\n  - name: date\n    column: date\n    display_name: Date\n    description: Date of the record\n  - name: location_key\n    column: location_key\n    display_name: Location Key\n    description: Location identifier\nmeasures:\n  - name: new_confirmed\n    display_name: New Confirmed\n    expression: SUM(new_confirmed)\n    format_preset: humanize\n    description: Sum of new confirmed cases\n  - name: new_deceased\n    display_name: New Deceased\n    expression: SUM(new_deceased)\n    format_preset: humanize\n    description: Sum of new deceased cases\n  - name: new_recovered\n    display_name: New Recovered\n    expression: SUM(new_recovered)\n    format_preset: humanize\n    description: Sum of new recovered cases\n  - name: new_tested\n    display_name: New Tested\n    expression: SUM(new_tested)\n    format_preset: humanize\n    description: Sum of new tested cases\n  - name: cumulative_confirmed\n    display_name: Cumulative Confirmed\n    expression: MAX(cumulative_confirmed)\n    format_preset: humanize\n    description: Maximum cumulative confirmed cases\n  - name: cumulative_deceased\n    display_name: Cumulative Deceased\n    expression: MAX(cumulative_deceased)\n    format_preset: humanize\n    description: Maximum cumulative deceased cases\n  - name: cumulative_recovered\n    display_name: Cumulative Recovered\n    expression: MAX(cumulative_recovered)\n    format_preset: humanize\n    description: Maximum cumulative recovered cases\n  - name: cumulative_tested\n    display_name: Cumulative Tested\n    expression: MAX(cumulative_tested)\n    format_preset: humanize\n    description: Maximum cumulative tested cases\ntimeseries: date\n","path":"/metrics/covid.yaml"}'
- index: 16
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. Non-materialized models become views, which re-execute on every query.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: system
      content_type: text
      content: "# Instructions for developing a metrics view in Rill\n\n## Introduction\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database. They implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\n\nMetrics views are lightweight resources that only perform validation when reconciled. They are typically found downstream of connectors and models in the project's DAG. They power many user-facing features:\n\n- **Explore dashboards**: Interactive drill-down interfaces for data exploration\n- **Canvas dashboards**: Custom chart and table components\n- **Alerts**: Notifications when data meets certain criteria\n- **Reports**: Scheduled data exports and summaries\n- **Custom APIs**: Programmatic access to metrics\n\n## Core Concepts\n\n### Table source\n\nThe `model:` property specifies the underlying table that powers the metrics view. It can reference:\n\n1. **A model in the project**: Just use the model name (e.g., `model: events`)\n2. **An external table**: Specify the table name as it exists in the OLAP connector\n\n```yaml\n# Referencing a model in the project\nmodel: events\n\n# Referencing an external table (connector defaults to project's default OLAP)\nconnector: clickhouse  # Optional: specify if different from default\nmodel: my_external_table\n```\n\n**Note**: The `table:` property is a legacy alias for referencing external tables. Always prefer `model:` in new metrics views.\n\n### Timeseries\n\nThe `timeseries:` property identifies the timestamp column used for time-based filtering and line charts. This column must be a time/timestamp type in the underlying table.\n\n```yaml\ntimeseries: event_time\n```\n\nIf the timeseries column is not listed in `dimensions:`, Rill automatically adds it as a time dimension. You can optionally configure additional time-related settings:\n\n```yaml\ntimeseries: event_time\nsmallest_time_grain: hour      # Minimum granularity users can select\nfirst_day_of_week: 7           # Sunday (1=Monday, 7=Sunday)\nfirst_month_of_year: 4         # April (fiscal year starting in April)\n```\n\n### Dimensions\n\nDimensions are attributes you can group by or filter on. They are typically categorical (strings, enums) or temporal (dates, timestamps). Rill infers the dimension type from the underlying SQL data type:\n\n- **Categorical**: String, enum, boolean columns\n- **Time**: Timestamp, date, datetime columns\n- **Geospatial**: Geometry or geography columns\n\nDefine dimensions using either a direct column reference or a SQL expression:\n\n```yaml\ndimensions:\n  # Simple column reference\n  - name: country\n    column: country\n\n  # Computed expression\n  - name: device_category\n    expression: CASE WHEN device_type IN ('phone', 'tablet') THEN 'Mobile' ELSE 'Desktop' END\n\n  # With display name and description\n  - name: campaign_name\n    display_name: Campaign\n    description: Marketing campaign that drove the traffic\n    column: campaign_name\n```\n\n**Naming**: Each dimension needs a `name` (stable identifier used in APIs and references), which defaults to `column:` if provided. The `display_name:` is optional, and defaults to a humanized version of `name` if not specified.\n\n### Measures\n\nMeasures are aggregation expressions that compute numeric values when grouped by dimensions. They must use aggregate functions like `SUM()`, `COUNT()`, `AVG()`, `MIN()`, `MAX()`.\n\n```yaml\nmeasures:\n  - name: total_revenue\n    display_name: Total Revenue\n    expression: SUM(revenue)\n    description: Sum of all revenue in USD\n    format_preset: currency_usd\n\n  - name: unique_users\n    display_name: Unique Users\n    expression: COUNT(DISTINCT user_id)\n    format_preset: humanize\n\n  - name: conversion_rate\n    display_name: Conversion Rate\n    expression: SUM(conversions) / NULLIF(SUM(visits), 0)\n    format_preset: percentage\n    valid_percent_of_total: false  # Disable % of total for ratios\n```\n\n**Format presets**: Control how values are displayed:\n- `none`: Raw number\n- `humanize`: Round to K, M, B (e.g., 1.2M)\n- `currency_usd`: Dollar format with 2 decimals ($1,234.56)\n- `currency_eur`: Euro format\n- `percentage`: Multiply by 100 and add % sign\n- `interval_ms`: Convert milliseconds to human-readable duration\n\nFor custom formatting, use `format_d3` with a [d3-format](https://d3js.org/d3-format) string:\n\n```yaml\nformat_d3: \"$,.2f\"  # $1,234.56\nformat_d3: \".1%\"    # 12.3%\nformat_d3: \",.0f\"   # 1,235 (rounded, with thousands separator)\n```\n\n### Best practices for dimensions and measures\n\n**Naming conventions:**\n- Use `snake_case` for the `name` field (e.g., `total_revenue`, `unique_users`)\n- Only add `display_name` and `description` if they provide meaningful context beyond what `name` conveys (display names auto-humanize from the name by default)\n- Ensure measure names don't collide with column names in the underlying table\n\n**Getting started with measures:**\n- Start with a `COUNT(*)` measure as a baseline (e.g., `total_records` or `total_events`)\n- Add `SUM()` measures for numeric columns that represent quantities or values\n- Use `humanize` as the default format preset unless the data has a specific format requirement\n- Keep initial measures simple using only `COUNT`, `SUM`, `AVG`, `MIN`, `MAX` aggregations\n- Add more complex expressions (ratios, conditional aggregations) only when needed\n\n**Dimension selection:**\n- Include all categorical columns (strings, enums, booleans) that users might want to filter or group by\n- Include date/timestamp columns as dimensions for time-based analysis beyond the main timeseries\n- Start with 5-10 dimensions; add more based on user needs\n\n### Auto-generated explore\n\nWhen you create a metrics view, Rill automatically generates an explore dashboard with the same name, exposing all dimensions and measures. To customize the explore (you usually should not need to), add an `explore:` block:\n\n```yaml\nexplore:\n  display_name: Sales Dashboard\n  defaults:\n    time_range: P7D\n    measures:\n      - total_revenue\n      - order_count\n```\n\n**Legacy behavior**: Files with `version: 1` do NOT auto-generate an explore. Omit `version:` in new metrics views to get the auto-generated explore.\n\n## Full Example\n\nHere is a complete, annotated metrics view:\n\n```yaml\n# metrics/orders.yaml\ntype: metrics_view\n\n# Display metadata\ndisplay_name: Orders Analytics\ndescription: Analyze order performance by various dimensions\n\n# Data source - references the 'orders' model in the project\nmodel: orders\n\n# Time column for time-series charts and filtering\ntimeseries: order_date\nsmallest_time_grain: day\n\n# Dimensions for grouping and filtering\ndimensions:\n  - name: order_date\n    display_name: Order Date\n    column: order_date\n\n  - name: country\n    display_name: Country\n    column: shipping_country\n\n  - name: product_category\n    display_name: Product Category\n    column: category\n    description: High-level product grouping\n\n  - name: customer_segment\n    display_name: Customer Segment\n    expression: | \n      CASE\n        WHEN lifetime_value > 1000 THEN 'High Value'\n        WHEN lifetime_value > 100 THEN 'Medium Value'\n        ELSE 'Low Value'\n      END\n\n  - name: is_repeat_customer\n    display_name: Repeat Customer\n    expression: CASE WHEN order_number > 1 THEN 'Yes' ELSE 'No' END\n\n# Measures for aggregation\nmeasures:\n  - name: total_orders\n    display_name: Total Orders\n    expression: COUNT(*)\n    format_preset: humanize\n\n  - name: total_revenue\n    display_name: Total Revenue\n    expression: SUM(order_total)\n    format_preset: currency_usd\n    description: Gross revenue before refunds\n\n  - name: average_order_value\n    display_name: Avg Order Value\n    expression: SUM(order_total) / NULLIF(COUNT(*), 0)\n    format_preset: currency_usd\n    valid_percent_of_total: false\n\n  - name: unique_customers\n    display_name: Unique Customers\n    expression: COUNT(DISTINCT customer_id)\n    format_preset: humanize\n\n  - name: items_per_order\n    display_name: Items per Order\n    expression: SUM(item_count) / NULLIF(COUNT(*), 0)\n    format_d3: \",.1f\"\n    valid_percent_of_total: false\n```\n\n## Security Policies\n\nSecurity policies control who can access a metrics view and what data they can see. This is a powerful feature for multi-tenant dashboards and role-based access control.\n\n### Basic access control\n\nThe `access:` property controls whether users can view the metrics view at all:\n\n```yaml\nsecurity:\n  # Allow access for everyone\n  access: true\n\n  # Deny access for everyone (useful for draft dashboards)\n  access: false\n\n  # Conditional access based on user attributes\n  access: \"'{{ .user.admin }}' = 'true'\"\n```\n\nThe expression syntax should be a DuckDB expression, which will be evaluated in a sandbox without access to any tables.\n\n### Row-level security\n\nThe `row_filter:` property restricts which rows a user can see. It's a SQL expression that references user attributes via templating:\n\n```yaml\nsecurity:\n  access: true\n  row_filter: domain = '{{ .user.domain }}'\n```\n\nCommon user attributes:\n- `{{ .user.email }}`: User's email address\n- `{{ .user.domain }}`: Email domain (e.g., \"acme.com\")\n- `{{ .user.admin }}`: Boolean admin flag\n- Custom attributes configured in Rill Cloud\n\nThe row filter should use the SQL syntax of the metrics view's model, and can reference other tables in the model's connector.\n\n### Complex row filters\n\nUse logical operators for sophisticated access patterns:\n\n```yaml\nsecurity:\n  access: true\n  row_filter: >\n    {{ .user.admin }}\n    OR '{{ .user.domain }}' = 'acme.com'\n    {{ if hasKey .user \"tenant_id\" }}\n    OR tenant_id = '{{ .user.tenant_id }}'\n    {{ end }}\n```\n\n### Hiding dimensions and measures\n\nThe `exclude:` property conditionally hides specific dimensions or measures from certain users:\n\n```yaml\nsecurity:\n  access: true\n  exclude:\n    - if: \"NOT {{ .user.admin }}\"\n      names:\n        - cost_per_acquisition  # Hide sensitive cost data from non-admins\n        - internal_notes\n```\n\n## Advanced Features\n\n### Annotations\n\nAnnotations overlay contextual information (like events or milestones) on time-series charts:\n\n```yaml\nannotations:\n  - name: product_launches\n    model: product_launches  # Must have 'time' and 'description' columns\n    measures:\n      - total_revenue        # Only show on these measures\n\n  # Optional columns in annotation model:\n  # - time_end: For range annotations\n  # - grain: Show only at specific time grains (day, week, etc.)\n```\n\n### Unnest for array dimensions\n\nWhen a column contains arrays, use `unnest: true` to flatten it at query time:\n\n```yaml\ndimensions:\n  - name: tags\n    display_name: Tags\n    column: tags\n    unnest: true  # Allows filtering by individual array elements\n```\n\n### Cache configuration\n\nConfigure caching for slow metrics views that use external tables:\n\n```yaml\ncache:\n  enabled: true\n  key_ttl: 5m\n  key_sql: SELECT MAX(updated_at) FROM orders\n```\n\nYou should not add a `cache:` config when the metrics view references a model inside the project since Rill does automatic cache management in that case.\n\n## Dialect-Specific Notes\n\nSQL expressions in dimensions and measures use the underlying OLAP database's dialect.\n\n### DuckDB\n\nDuckDB is the default OLAP engine for local development.\n\n**Conditional aggregation with FILTER**:\n```yaml\n# DuckDB supports FILTER clause for conditional aggregation\nexpression: COUNT(*) FILTER (WHERE status = 'completed')\n```\n\n### ClickHouse\n\nClickHouse is recommended for production workloads with large datasets.\n\n**Conditional aggregation**:\n```yaml\n# ClickHouse uses IF or CASE inside aggregations\nexpression: countIf(status = 'completed')\nexpression: sumIf(revenue, status = 'completed')\n```\n\n**Date functions**:\n```yaml\nexpression: toYear(order_date)\nexpression: toStartOfMonth(order_date)\nexpression: toYYYYMMDD(order_date)\n```\n\n**Array functions**:\n```yaml\nexpression: arrayJoin(tags)  # Unnest arrays\n```\n\n### Druid\n\n**Approximate distinct counts**:\n```yaml\nexpression: APPROX_COUNT_DISTINCT_DS_HLL(user_id)\n```\n\n## JSON Schema\n\n```\nallOf:\n    - properties:\n        ai_instructions:\n            description: Extra instructions for [AI agents](/explore/mcp). Used to guide natural language question answering and routing.\n            type: string\n        annotations:\n            description: Used to define annotations that can be displayed on charts\n            items:\n                properties:\n                    connector:\n                        description: Refers to the connector to use for the annotation\n                        type: string\n                    database:\n                        description: Refers to the database to use in the OLAP engine (to be used in conjunction with table). Otherwise, will use the default database or schema if not specified\n                        type: string\n                    database_schema:\n                        description: Refers to the schema to use in the OLAP engine (to be used in conjunction with table). Otherwise, will use the default database or schema if not specified\n                        type: string\n                    measures:\n                        anyOf:\n                            - description: Simple field name as a string.\n                              type: string\n                            - description: List of field selectors, each can be a string or an object with detailed configuration.\n                              items:\n                                anyOf:\n                                    - description: Shorthand field selector, interpreted as the name.\n                                      type: string\n                                    - additionalProperties: false\n                                      description: Detailed field selector configuration with name and optional time grain.\n                                      properties:\n                                        name:\n                                            description: Name of the field to select.\n                                            type: string\n                                        time_grain:\n                                            description: Time grain for time-based dimensions.\n                                            enum:\n                                                - \"\"\n                                                - ms\n                                                - millisecond\n                                                - s\n                                                - second\n                                                - min\n                                                - minute\n                                                - hadditionalProperties: fal\n                                                - hour\n                                                - d\n                                                - day\n                                                - w\n                                                - week\n                                                - month\n                                                - q\n                                                - quarter\n                                                - \"y\"\n                                                - year\n                                            type: string\n                                      required:\n                                        - name\n                                      type: object\n                              type: array\n                        description: Specifies which measures to apply the annotation to. Applies to all measures if not specified\n                    model:\n                        description: Refers to the model powering the annotation (either table or model is required). The model must have 'time' and 'description' columns. Optional columns include 'time_end' for range annotations and 'grain' to specify when the annotation should appear based on dashboard grain level.\n                        type: string\n                    name:\n                        description: A stable identifier for the annotation. Defaults to model or table names when not specified\n                        type: string\n                    table:\n                        description: Refers to the table powering the annotation, should be used instead of model for annotations from external OLAP tables (either table or model is required)\n                        type: string\n                type: object\n            type: array\n        connector:\n            description: Refers to the connector type for the metrics view, see [OLAP engines](/developers/build/connectors/olap) for more information\n            type: string\n        database:\n            description: Refers to the database to use in the OLAP engine (to be used in conjunction with table). Otherwise, will use the default database or schema if not specified\n            type: string\n        database_schema:\n            description: Refers to the schema to use in the OLAP engine (to be used in conjunction with table). Otherwise, will use the default database or schema if not specified\n            type: string\n        description:\n            description: Refers to the description for the metrics view\n            type: string\n        dimensions:\n            description: Relates to exploring segments or dimensions of your data and filtering the dashboard\n            items:\n                anyOf:\n                    - required:\n                        - column\n                    - required:\n                        - expression\n                properties:\n                    column:\n                        description: a categorical column\n                        type: string\n                    description:\n                        description: a freeform text description of the dimension\n                        type: string\n                    display_name:\n                        description: a display name for your dimension\n                        type: string\n                    expression:\n                        description: a non-aggregate expression such as string_split(domain, '.'). One of column and expression is required but cannot have both at the same time\n                        type: string\n                    name:\n                        description: a stable identifier for the dimension\n                        type: string\n                    tags:\n                        description: optional list of tags for categorizing the dimension (defaults to empty)\n                        items:\n                            type: string\n                        type: array\n                    type:\n                        description: 'Dimension type: \"geo\" for geospatial dimensions, \"time\" for time dimensions or \"categorical\" for categorial dimensions. Default is undefined and the type will be inferred instead'\n                        type: string\n                    unnest:\n                        description: if true, allows multi-valued dimension to be unnested (such as lists) and filters will automatically switch to \"contains\" instead of exact match\n                        type: boolean\n                    uri:\n                        description: enable if your dimension is a clickable URL to enable single click navigation (boolean or valid SQL expression)\n                        type:\n                            - string\n                            - boolean\n                type: object\n            type: array\n        display_name:\n            description: Refers to the display name for the metrics view\n            type: string\n        explore:\n            $ref: '#/definitions/explore_properties'\n            description: Defines an optional inline explore view for the metrics view. If not specified a default explore will be emitted unless `skip` is set to true.\n            required:\n                - type\n        first_day_of_week:\n            description: Refers to the first day of the week for time grain aggregation (for example, Sunday instead of Monday). The valid values are 1 through 7 where Monday=1 and Sunday=7\n            type: integer\n        first_month_of_year:\n            description: Refers to the first month of the year for time grain aggregation. The valid values are 1 through 12 where January=1 and December=12\n            type: integer\n        measures:\n            description: Used to define the numeric aggregates of columns from your data model\n            items:\n                properties:\n                    description:\n                        description: a freeform text description of the measure\n                        type: string\n                    display_name:\n                        description: the display name of your measure.\n                        type: string\n                    expression:\n                        description: a combination of operators and functions for aggregations\n                        type: string\n                    format_d3:\n                        description: 'Controls the formatting of this measure using a [d3-format](https://d3js.org/d3-format) string. If an invalid format string is supplied, the measure will fall back to `format_preset: humanize`. A measure cannot have both `format_preset` and `format_d3`. If neither is provided, the humanize preset is used by default. Example: `format_d3: \".2f\"` formats using fixed-point notation with two decimal places. Example: `format_d3: \",.2r\"` formats using grouped thousands with two significant digits. (optional)'\n                        type: string\n                    format_d3_locale:\n                        description: |\n                            locale configuration passed through to D3, enabling changing the currency symbol among other things. For details, see the docs for D3's formatLocale.\n                              ```yaml\n                              format_d3: \"$,\"\n                              format_d3_locale:\n                                grouping: [3, 2]\n                                currency: [\"\", \"\"]\n                              ```\n                        properties:\n                            currency:\n                                description: the currency symbol\n                                type: array\n                            grouping:\n                                description: the grouping of the currency symbol\n                                type: array\n                        type: object\n                    format_preset:\n                        description: |\n                            Controls the formatting of this measure using a predefined preset. Measures cannot have both `format_preset` and `format_d3`. If neither is supplied, the measure will be formatted using the `humanize` preset by default.\n\n                              Available options:\n                              - `humanize`: Round numbers into thousands (K), millions(M), billions (B), etc.\n                              - `none`: Raw output.\n                              - `currency_usd`: Round to 2 decimal points with a dollar sign ($).\n                              - `currency_eur`: Round to 2 decimal points with a euro sign ().\n                              - `percentage`: Convert a rate into a percentage with a % sign.\n                              - `interval_ms`: Convert milliseconds into human-readable durations like hours (h), days (d), years (y), etc. (optional)\n                        type: string\n                    label:\n                        description: a label for your measure, deprecated use display_name\n                        type: string\n                    name:\n                        description: a stable identifier for the measure\n                        type: string\n                    per:\n                        $ref: '#/definitions/field_selectors_properties'\n                        description: for per dimensions\n                    requires:\n                        $ref: '#/definitions/field_selectors_properties'\n                        description: using an available measure or dimension in your metrics view to set a required parameter, cannot be used with simple measures. See [referencing measures](/developers/build/metrics-view/measures/referencing) for more information.\n                    tags:\n                        description: optional list of tags for categorizing the measure (defaults to empty)\n                        items:\n                            type: string\n                        type: array\n                    treat_nulls_as:\n                        description: used to configure what value to fill in for missing time buckets. This also works generally as COALESCING over non empty time buckets.\n                        type: string\n                    type:\n                        description: 'Measure calculation type: \"simple\" for basic aggregations, \"derived\" for calculations using other measures, or \"time_comparison\" for period-over-period analysis. Defaults to \"simple\" unless dependencies exist.'\n                        type: string\n                    valid_percent_of_total:\n                        description: a boolean indicating whether percent-of-total values should be rendered for this measure\n                        type: boolean\n                    window:\n                        anyOf:\n                            - description: 'Shorthand: `time` or `true` means time-partitioned, `all` means non-partitioned.'\n                              enum:\n                                - time\n                                - \"true\"\n                                - all\n                              type: string\n                            - additionalProperties: false\n                              description: Detailed window configuration for measure calculations, allowing control over partitioning, ordering, and frame definition.\n                              properties:\n                                frame:\n                                    description: Defines the window frame boundaries for calculations, specifying which rows are included in the window relative to the current row.\n                                    type: string\n                                order:\n                                    $ref: '#/definitions/field_selectors_properties'\n                                    description: Specifies the fields to order the window by, determining the sequence of rows within each partition.\n                                    type: string\n                                partition:\n                                    description: Controls whether the window is partitioned. When true, calculations are performed within each partition separately.\n                                    type: boolean\n                              type: object\n                        description: A measure window can be defined as a keyword string (e.g. 'time' or 'all') or an object with detailed window configuration. For more information, see the [window functions](/developers/build/metrics-view/measures/windows) documentation.\n                required:\n                    - name\n                    - display_name\n                    - expression\n                type: object\n            type: array\n        model:\n            description: Refers to the model powering the dashboard (either model or table is required)\n            type: string\n        parent:\n            description: Refers to the parent metrics view from which this metrics view is derived. If specified, this will inherit properties from the parent metrics view\n            type: string\n        parent_dimensions:\n            $ref: '#/definitions/field_selector_properties'\n            description: Optional field selectors for dimensions to inherit from the parent metrics view.\n        parent_measures:\n            $ref: '#/definitions/field_selector_properties'\n            description: Optional field selectors for measures to inherit from the parent metrics view.\n        security:\n            $ref: '#/definitions/security_policy_properties'\n            description: Defines a security policy for the dashboard\n        smallest_time_grain:\n            description: 'Refers to the smallest time granularity the user is allowed to view. The valid values are: millisecond, second, minute, hour, day, week, month, quarter, year'\n            type: string\n        table:\n            description: Refers to the table powering the dashboard, should be used instead of model for dashboards create from external OLAP tables (either table or model is required)\n            type: string\n        timeseries:\n            description: Refers to the timestamp column from your model that will underlie x-axis data in the line charts. If not specified, the line charts will not appear\n            type: string\n        type:\n            const: metrics_view\n            description: Refers to the resource type and must be `metrics_view`\n            type: string\n        version:\n            description: The version of the metrics view schema\n            type: string\n        watermark:\n            description: A SQL expression that tells us the max timestamp that the measures are considered valid for. Usually does not need to be overwritten\n            type: string\n      title: Properties\n      type: object\n    - $ref: '#/definitions/common_properties'\ndescription: In your Rill project directory, create a metrics view, `<metrics_view>.yaml`, file in the `metrics` directory. Rill will ingest the metric view definition next time you run `rill start`.\nid: metrics-views\ntitle: Metrics View YAML\ntype: object\n```"
    - role: user
      content_type: text
      content: |-
        You should develop a Rill project file based on the following task description: Create a metrics view for the covid table in the ClickHouse playground (schema: default, table: covid). Use the clickhouse connector. Define the following dimensions: date (date), location_key (string). Define the following measures: new_confirmed (sum), new_deceased (sum), new_recovered (sum), new_tested (sum), cumulative_confirmed (max), cumulative_deceased (max), cumulative_recovered (max), cumulative_tested (max). Name the metrics view 'covid'.

        Here is some important context:
        - You should develop the file at path: /metrics/covid.yaml
        - The file should be of type: metrics_view
        - You are running as a sub-agent of a larger developer agent. Stay aligned on your specific task, avoid extra discovery, and report back your success or failure to the parent agent.

        Here is some additional context that may or may not be relevant to your task:
        - The project's default OLAP connector is named clickhouse (driver: clickhouse).
    - role: assistant
      content_type: tool_call
      id: 74d703844a254c3f
      tool_name: write_file
      content: '{"contents":"type: metrics_view\nmodel: covid\nconnector: clickhouse\ndatabase: default\ndimensions:\n  - name: date\n    column: date\n    display_name: Date\n    description: Date of the record\n  - name: location_key\n    column: location_key\n    display_name: Location Key\n    description: Location identifier\nmeasures:\n  - name: new_confirmed\n    display_name: New Confirmed\n    expression: SUM(new_confirmed)\n    format_preset: humanize\n    description: Sum of new confirmed cases\n  - name: new_deceased\n    display_name: New Deceased\n    expression: SUM(new_deceased)\n    format_preset: humanize\n    description: Sum of new deceased cases\n  - name: new_recovered\n    display_name: New Recovered\n    expression: SUM(new_recovered)\n    format_preset: humanize\n    description: Sum of new recovered cases\n  - name: new_tested\n    display_name: New Tested\n    expression: SUM(new_tested)\n    format_preset: humanize\n    description: Sum of new tested cases\n  - name: cumulative_confirmed\n    display_name: Cumulative Confirmed\n    expression: MAX(cumulative_confirmed)\n    format_preset: humanize\n    description: Maximum cumulative confirmed cases\n  - name: cumulative_deceased\n    display_name: Cumulative Deceased\n    expression: MAX(cumulative_deceased)\n    format_preset: humanize\n    description: Maximum cumulative deceased cases\n  - name: cumulative_recovered\n    display_name: Cumulative Recovered\n    expression: MAX(cumulative_recovered)\n    format_preset: humanize\n    description: Maximum cumulative recovered cases\n  - name: cumulative_tested\n    display_name: Cumulative Tested\n    expression: MAX(cumulative_tested)\n    format_preset: humanize\n    description: Maximum cumulative tested cases\ntimeseries: date\n","path":"/metrics/covid.yaml"}'
    - role: tool
      content_type: tool_response
      id: 74d703844a254c3f
      content: '{"diff":"--- /metrics/covid.yaml\n+++ /metrics/covid.yaml\n@@ -1 +1,56 @@\n+type: metrics_view\n+model: covid\n+connector: clickhouse\n+database: default\n+dimensions:\n+  - name: date\n+    column: date\n+    display_name: Date\n+    description: Date of the record\n+  - name: location_key\n+    column: location_key\n+    display_name: Location Key\n+    description: Location identifier\n+measures:\n+  - name: new_confirmed\n+    display_name: New Confirmed\n+    expression: SUM(new_confirmed)\n+    format_preset: humanize\n+    description: Sum of new confirmed cases\n+  - name: new_deceased\n+    display_name: New Deceased\n+    expression: SUM(new_deceased)\n+    format_preset: humanize\n+    description: Sum of new deceased cases\n+  - name: new_recovered\n+    display_name: New Recovered\n+    expression: SUM(new_recovered)\n+    format_preset: humanize\n+    description: Sum of new recovered cases\n+  - name: new_tested\n+    display_name: New Tested\n+    expression: SUM(new_tested)\n+    format_preset: humanize\n+    description: Sum of new tested cases\n+  - name: cumulative_confirmed\n+    display_name: Cumulative Confirmed\n+    expression: MAX(cumulative_confirmed)\n+    format_preset: humanize\n+    description: Maximum cumulative confirmed cases\n+  - name: cumulative_deceased\n+    display_name: Cumulative Deceased\n+    expression: MAX(cumulative_deceased)\n+    format_preset: humanize\n+    description: Maximum cumulative deceased cases\n+  - name: cumulative_recovered\n+    display_name: Cumulative Recovered\n+    expression: MAX(cumulative_recovered)\n+    format_preset: humanize\n+    description: Maximum cumulative recovered cases\n+  - name: cumulative_tested\n+    display_name: Cumulative Tested\n+    expression: MAX(cumulative_tested)\n+    format_preset: humanize\n+    description: Maximum cumulative tested cases\n+timeseries: date\n \n","is_new_file":true,"resources":[{"kind":"rill.runtime.v1.MetricsView","name":"covid","reconcile_error":"invalid measure \"new_confirmed\": measures cannot have the same name as a column in the underlying table when backed by clickhouse\ninvalid measure \"new_deceased\": measures cannot have the same name as a column in the underlying table when backed by clickhouse\ninvalid measure \"new_recovered\": measures cannot have the same name as a column in the underlying table when backed by clickhouse\ninvalid measure \"new_tested\": measures cannot have the same name as a column in the underlying table when backed by clickhouse\ninvalid measure \"cumulative_confirmed\": measures cannot have the same name as a column in the underlying table when backed by clickhouse\ninvalid measure \"cumulative_deceased\": measures cannot have the same name as a column in the underlying table when backed by clickhouse\ninvalid measure \"cumulative_recovered\": measures cannot have the same name as a column in the underlying table when backed by clickhouse\ninvalid measure \"cumulative_tested\": measures cannot have the same name as a column in the underlying table when backed by clickhouse","reconcile_status":"RECONCILE_STATUS_IDLE"},{"kind":"rill.runtime.v1.Explore","name":"covid","reconcile_error":"dependency error: resource \"covid\" (rill.runtime.v1.MetricsView) has an error","reconcile_status":"RECONCILE_STATUS_IDLE"}]}'
  response:
    - role: assistant
      content_type: text
      content: |-
        The metrics view file /metrics/covid.yaml was created as specified, targeting the covid table in the ClickHouse playground (schema: default, table: covid) using the clickhouse connector. It defines the requested dimensions (date, location_key) and measures (new_confirmed, new_deceased, new_recovered, new_tested, cumulative_confirmed, cumulative_deceased, cumulative_recovered, cumulative_tested).

        However, ClickHouse-backed metrics views in Rill do not allow measures to have the same name as columns in the underlying table. All the requested measure names match column names, resulting in a validation error for each measure.

        To resolve this, you must rename each measure to something different from the corresponding column name (for example, prefix with total_ or use a suffix like _sum or _max). Please confirm if you want to proceed with renaming, and specify your preferred naming convention if any.
- index: 17
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. Non-materialized models become views, which re-execute on every query.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: user
      content_type: text
      content: |-
        This may not relate to the user's task, but for context, the project's default OLAP connector is named duckdb (driver: duckdb).
         The default OLAP is in readwrite mode, so you can use it in models if you want.


        Task: Clickhouse has a public playground database on play.clickhouse.com:9440 (username `play` and no password). Can you set it up as the OLAP connector and develop a dashboard based on one of its tables? Please proceed without asking clarifying questions.
    - role: assistant
      content_type: tool_call
      id: 5f915383ab074f81
      tool_name: develop_file
      content: '{"path":"connectors/clickhouse.yaml","prompt":"Create a connector for the public ClickHouse playground database at play.clickhouse.com:9440. Use the ''clickhouse'' driver. Set the username to ''play'' and leave the password blank. This connector will be used as an OLAP connector for dashboards. Set ''mode'' to ''read'' to prevent writes. Name the connector ''clickhouse''.","type":"connector"}'
    - role: tool
      content_type: tool_response
      id: 5f915383ab074f81
      content: '{"response":"The connector file /connectors/clickhouse.yaml was created as specified:\n\n- type: connector\n- driver: clickhouse\n- host: play.clickhouse.com\n- port: 9440\n- username: play\n- password: \"\" (blank)\n- mode: read\n\nThe file was written successfully. However, there is a reconcile error indicating a handshake failure, likely due to an SSL/TLS mismatch. If SSL is required for this endpoint, you may need to add ssl: true to the configuration. Please advise if you want to update the file to include SSL."}'
    - role: assistant
      content_type: tool_call
      id: 3925a34a97214026
      tool_name: develop_file
      content: '{"path":"connectors/clickhouse.yaml","prompt":"Update the ClickHouse connector configuration to include SSL support, as the public playground likely requires a secure connection. Add ''ssl: true'' to the YAML file. Ensure all other settings remain the same: driver clickhouse, host play.clickhouse.com, port 9440, username play, blank password, mode read.","type":"connector"}'
    - role: tool
      content_type: tool_response
      id: 3925a34a97214026
      content: '{"response":"Success: The ClickHouse connector configuration at /connectors/clickhouse.yaml has been updated to include ssl: true, as required for secure connections to the public playground. All other settings (driver, host, port, username, blank password, and mode) remain unchanged. No errors were encountered during the update."}'
    - role: assistant
      content_type: tool_call
      id: 83c850aa1fea4507
      tool_name: develop_file
      content: '{"path":"rill.yaml","prompt":"Update the project configuration to set the default OLAP connector to ''clickhouse'' instead of ''duckdb''. Ensure that the ''olap_connector'' property is set to ''clickhouse''. Retain any other existing configuration in rill.yaml.","type":"rill.yaml"}'
    - role: tool
      content_type: tool_response
      id: 83c850aa1fea4507
      content: '{"response":"The rill.yaml file has been updated to set olap_connector: clickhouse as the default OLAP connector, as requested. There was no other existing configuration to retain. Task complete."}'
    - role: assistant
      content_type: tool_call
      id: a902bdef4d944e31
      tool_name: list_tables
      content: '{"connector":"clickhouse","page_size":20}'
    - role: tool
      content_type: tool_response
      id: a902bdef4d944e31
      content: '{"tables":[{"database_schema":"blogs","name":"countries","is_view":false},{"database_schema":"blogs","name":"country_codes","is_view":false},{"database_schema":"blogs","name":"forex","is_view":false},{"database_schema":"blogs","name":"forex_2020s","is_view":false},{"database_schema":"blogs","name":"forex_usd","is_view":false},{"database_schema":"blogs","name":"noaa","is_view":false},{"database_schema":"blogs","name":"noaa_v2","is_view":false},{"database_schema":"blogs","name":"stop_words","is_view":false},{"database_schema":"blogs","name":"uk_codes","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"actors","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"benchmark_results","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"benchmark_runs","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"cell_towers","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"checks","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"checks_coverage_inverted","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"checks_v2","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"cisco_umbrella","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"covid","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"dish","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"dns","is_view":false}],"next_page_token":"ImRlZmF1bHQiCiJkbnMiCg"}'
    - role: assistant
      content_type: tool_call
      id: 46401ed1773746ab
      tool_name: show_table
      content: '{"connector":"clickhouse","database_schema":"default","table":"covid"}'
    - role: tool
      content_type: tool_response
      id: 46401ed1773746ab
      content: '{"name":"covid","is_view":false,"columns":[{"name":"date","type":"CODE_DATE"},{"name":"location_key","type":"CODE_STRING"},{"name":"new_confirmed","type":"CODE_INT32"},{"name":"new_deceased","type":"CODE_INT32"},{"name":"new_recovered","type":"CODE_INT32"},{"name":"new_tested","type":"CODE_INT32"},{"name":"cumulative_confirmed","type":"CODE_INT32"},{"name":"cumulative_deceased","type":"CODE_INT32"},{"name":"cumulative_recovered","type":"CODE_INT32"},{"name":"cumulative_tested","type":"CODE_INT32"}],"physical_size_bytes":-1}'
    - role: assistant
      content_type: tool_call
      id: 850ac6b8ec0b4f1b
      tool_name: develop_file
      content: '{"path":"metrics/covid.yaml","prompt":"Create a metrics view for the covid table in the ClickHouse playground (schema: default, table: covid). Use the clickhouse connector. Define the following dimensions: date (date), location_key (string). Define the following measures: new_confirmed (sum), new_deceased (sum), new_recovered (sum), new_tested (sum), cumulative_confirmed (max), cumulative_deceased (max), cumulative_recovered (max), cumulative_tested (max). Name the metrics view ''covid''.","type":"metrics_view"}'
    - role: tool
      content_type: tool_response
      id: 850ac6b8ec0b4f1b
      content: '{"response":"The metrics view file /metrics/covid.yaml was created as specified, targeting the covid table in the ClickHouse playground (schema: default, table: covid) using the clickhouse connector. It defines the requested dimensions (date, location_key) and measures (new_confirmed, new_deceased, new_recovered, new_tested, cumulative_confirmed, cumulative_deceased, cumulative_recovered, cumulative_tested).\n\nHowever, ClickHouse-backed metrics views in Rill do not allow measures to have the same name as columns in the underlying table. All the requested measure names match column names, resulting in a validation error for each measure.\n\nTo resolve this, you must rename each measure to something different from the corresponding column name (for example, prefix with total_ or use a suffix like _sum or _max). Please confirm if you want to proceed with renaming, and specify your preferred naming convention if any."}'
  response:
    - role: assistant
      content_type: tool_call
      id: call_sodgpxwY1JRLNG3VZNF4VX9F
      tool_name: develop_file
      content: '{"path":"metrics/covid.yaml","prompt":"Update the metrics view for the covid table to resolve the validation error: rename each measure so it does not match the underlying column name. Use the following naming convention: for sum aggregations, use the suffix _sum (e.g., new_confirmed_sum); for max aggregations, use the suffix _max (e.g., cumulative_confirmed_max). Ensure all other configuration remains unchanged.","type":"metrics_view"}'
- index: 18
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. Non-materialized models become views, which re-execute on every query.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: system
      content_type: text
      content: "# Instructions for developing a metrics view in Rill\n\n## Introduction\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database. They implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\n\nMetrics views are lightweight resources that only perform validation when reconciled. They are typically found downstream of connectors and models in the project's DAG. They power many user-facing features:\n\n- **Explore dashboards**: Interactive drill-down interfaces for data exploration\n- **Canvas dashboards**: Custom chart and table components\n- **Alerts**: Notifications when data meets certain criteria\n- **Reports**: Scheduled data exports and summaries\n- **Custom APIs**: Programmatic access to metrics\n\n## Core Concepts\n\n### Table source\n\nThe `model:` property specifies the underlying table that powers the metrics view. It can reference:\n\n1. **A model in the project**: Just use the model name (e.g., `model: events`)\n2. **An external table**: Specify the table name as it exists in the OLAP connector\n\n```yaml\n# Referencing a model in the project\nmodel: events\n\n# Referencing an external table (connector defaults to project's default OLAP)\nconnector: clickhouse  # Optional: specify if different from default\nmodel: my_external_table\n```\n\n**Note**: The `table:` property is a legacy alias for referencing external tables. Always prefer `model:` in new metrics views.\n\n### Timeseries\n\nThe `timeseries:` property identifies the timestamp column used for time-based filtering and line charts. This column must be a time/timestamp type in the underlying table.\n\n```yaml\ntimeseries: event_time\n```\n\nIf the timeseries column is not listed in `dimensions:`, Rill automatically adds it as a time dimension. You can optionally configure additional time-related settings:\n\n```yaml\ntimeseries: event_time\nsmallest_time_grain: hour      # Minimum granularity users can select\nfirst_day_of_week: 7           # Sunday (1=Monday, 7=Sunday)\nfirst_month_of_year: 4         # April (fiscal year starting in April)\n```\n\n### Dimensions\n\nDimensions are attributes you can group by or filter on. They are typically categorical (strings, enums) or temporal (dates, timestamps). Rill infers the dimension type from the underlying SQL data type:\n\n- **Categorical**: String, enum, boolean columns\n- **Time**: Timestamp, date, datetime columns\n- **Geospatial**: Geometry or geography columns\n\nDefine dimensions using either a direct column reference or a SQL expression:\n\n```yaml\ndimensions:\n  # Simple column reference\n  - name: country\n    column: country\n\n  # Computed expression\n  - name: device_category\n    expression: CASE WHEN device_type IN ('phone', 'tablet') THEN 'Mobile' ELSE 'Desktop' END\n\n  # With display name and description\n  - name: campaign_name\n    display_name: Campaign\n    description: Marketing campaign that drove the traffic\n    column: campaign_name\n```\n\n**Naming**: Each dimension needs a `name` (stable identifier used in APIs and references), which defaults to `column:` if provided. The `display_name:` is optional, and defaults to a humanized version of `name` if not specified.\n\n### Measures\n\nMeasures are aggregation expressions that compute numeric values when grouped by dimensions. They must use aggregate functions like `SUM()`, `COUNT()`, `AVG()`, `MIN()`, `MAX()`.\n\n```yaml\nmeasures:\n  - name: total_revenue\n    display_name: Total Revenue\n    expression: SUM(revenue)\n    description: Sum of all revenue in USD\n    format_preset: currency_usd\n\n  - name: unique_users\n    display_name: Unique Users\n    expression: COUNT(DISTINCT user_id)\n    format_preset: humanize\n\n  - name: conversion_rate\n    display_name: Conversion Rate\n    expression: SUM(conversions) / NULLIF(SUM(visits), 0)\n    format_preset: percentage\n    valid_percent_of_total: false  # Disable % of total for ratios\n```\n\n**Format presets**: Control how values are displayed:\n- `none`: Raw number\n- `humanize`: Round to K, M, B (e.g., 1.2M)\n- `currency_usd`: Dollar format with 2 decimals ($1,234.56)\n- `currency_eur`: Euro format\n- `percentage`: Multiply by 100 and add % sign\n- `interval_ms`: Convert milliseconds to human-readable duration\n\nFor custom formatting, use `format_d3` with a [d3-format](https://d3js.org/d3-format) string:\n\n```yaml\nformat_d3: \"$,.2f\"  # $1,234.56\nformat_d3: \".1%\"    # 12.3%\nformat_d3: \",.0f\"   # 1,235 (rounded, with thousands separator)\n```\n\n### Best practices for dimensions and measures\n\n**Naming conventions:**\n- Use `snake_case` for the `name` field (e.g., `total_revenue`, `unique_users`)\n- Only add `display_name` and `description` if they provide meaningful context beyond what `name` conveys (display names auto-humanize from the name by default)\n- Ensure measure names don't collide with column names in the underlying table\n\n**Getting started with measures:**\n- Start with a `COUNT(*)` measure as a baseline (e.g., `total_records` or `total_events`)\n- Add `SUM()` measures for numeric columns that represent quantities or values\n- Use `humanize` as the default format preset unless the data has a specific format requirement\n- Keep initial measures simple using only `COUNT`, `SUM`, `AVG`, `MIN`, `MAX` aggregations\n- Add more complex expressions (ratios, conditional aggregations) only when needed\n\n**Dimension selection:**\n- Include all categorical columns (strings, enums, booleans) that users might want to filter or group by\n- Include date/timestamp columns as dimensions for time-based analysis beyond the main timeseries\n- Start with 5-10 dimensions; add more based on user needs\n\n### Auto-generated explore\n\nWhen you create a metrics view, Rill automatically generates an explore dashboard with the same name, exposing all dimensions and measures. To customize the explore (you usually should not need to), add an `explore:` block:\n\n```yaml\nexplore:\n  display_name: Sales Dashboard\n  defaults:\n    time_range: P7D\n    measures:\n      - total_revenue\n      - order_count\n```\n\n**Legacy behavior**: Files with `version: 1` do NOT auto-generate an explore. Omit `version:` in new metrics views to get the auto-generated explore.\n\n## Full Example\n\nHere is a complete, annotated metrics view:\n\n```yaml\n# metrics/orders.yaml\ntype: metrics_view\n\n# Display metadata\ndisplay_name: Orders Analytics\ndescription: Analyze order performance by various dimensions\n\n# Data source - references the 'orders' model in the project\nmodel: orders\n\n# Time column for time-series charts and filtering\ntimeseries: order_date\nsmallest_time_grain: day\n\n# Dimensions for grouping and filtering\ndimensions:\n  - name: order_date\n    display_name: Order Date\n    column: order_date\n\n  - name: country\n    display_name: Country\n    column: shipping_country\n\n  - name: product_category\n    display_name: Product Category\n    column: category\n    description: High-level product grouping\n\n  - name: customer_segment\n    display_name: Customer Segment\n    expression: | \n      CASE\n        WHEN lifetime_value > 1000 THEN 'High Value'\n        WHEN lifetime_value > 100 THEN 'Medium Value'\n        ELSE 'Low Value'\n      END\n\n  - name: is_repeat_customer\n    display_name: Repeat Customer\n    expression: CASE WHEN order_number > 1 THEN 'Yes' ELSE 'No' END\n\n# Measures for aggregation\nmeasures:\n  - name: total_orders\n    display_name: Total Orders\n    expression: COUNT(*)\n    format_preset: humanize\n\n  - name: total_revenue\n    display_name: Total Revenue\n    expression: SUM(order_total)\n    format_preset: currency_usd\n    description: Gross revenue before refunds\n\n  - name: average_order_value\n    display_name: Avg Order Value\n    expression: SUM(order_total) / NULLIF(COUNT(*), 0)\n    format_preset: currency_usd\n    valid_percent_of_total: false\n\n  - name: unique_customers\n    display_name: Unique Customers\n    expression: COUNT(DISTINCT customer_id)\n    format_preset: humanize\n\n  - name: items_per_order\n    display_name: Items per Order\n    expression: SUM(item_count) / NULLIF(COUNT(*), 0)\n    format_d3: \",.1f\"\n    valid_percent_of_total: false\n```\n\n## Security Policies\n\nSecurity policies control who can access a metrics view and what data they can see. This is a powerful feature for multi-tenant dashboards and role-based access control.\n\n### Basic access control\n\nThe `access:` property controls whether users can view the metrics view at all:\n\n```yaml\nsecurity:\n  # Allow access for everyone\n  access: true\n\n  # Deny access for everyone (useful for draft dashboards)\n  access: false\n\n  # Conditional access based on user attributes\n  access: \"'{{ .user.admin }}' = 'true'\"\n```\n\nThe expression syntax should be a DuckDB expression, which will be evaluated in a sandbox without access to any tables.\n\n### Row-level security\n\nThe `row_filter:` property restricts which rows a user can see. It's a SQL expression that references user attributes via templating:\n\n```yaml\nsecurity:\n  access: true\n  row_filter: domain = '{{ .user.domain }}'\n```\n\nCommon user attributes:\n- `{{ .user.email }}`: User's email address\n- `{{ .user.domain }}`: Email domain (e.g., \"acme.com\")\n- `{{ .user.admin }}`: Boolean admin flag\n- Custom attributes configured in Rill Cloud\n\nThe row filter should use the SQL syntax of the metrics view's model, and can reference other tables in the model's connector.\n\n### Complex row filters\n\nUse logical operators for sophisticated access patterns:\n\n```yaml\nsecurity:\n  access: true\n  row_filter: >\n    {{ .user.admin }}\n    OR '{{ .user.domain }}' = 'acme.com'\n    {{ if hasKey .user \"tenant_id\" }}\n    OR tenant_id = '{{ .user.tenant_id }}'\n    {{ end }}\n```\n\n### Hiding dimensions and measures\n\nThe `exclude:` property conditionally hides specific dimensions or measures from certain users:\n\n```yaml\nsecurity:\n  access: true\n  exclude:\n    - if: \"NOT {{ .user.admin }}\"\n      names:\n        - cost_per_acquisition  # Hide sensitive cost data from non-admins\n        - internal_notes\n```\n\n## Advanced Features\n\n### Annotations\n\nAnnotations overlay contextual information (like events or milestones) on time-series charts:\n\n```yaml\nannotations:\n  - name: product_launches\n    model: product_launches  # Must have 'time' and 'description' columns\n    measures:\n      - total_revenue        # Only show on these measures\n\n  # Optional columns in annotation model:\n  # - time_end: For range annotations\n  # - grain: Show only at specific time grains (day, week, etc.)\n```\n\n### Unnest for array dimensions\n\nWhen a column contains arrays, use `unnest: true` to flatten it at query time:\n\n```yaml\ndimensions:\n  - name: tags\n    display_name: Tags\n    column: tags\n    unnest: true  # Allows filtering by individual array elements\n```\n\n### Cache configuration\n\nConfigure caching for slow metrics views that use external tables:\n\n```yaml\ncache:\n  enabled: true\n  key_ttl: 5m\n  key_sql: SELECT MAX(updated_at) FROM orders\n```\n\nYou should not add a `cache:` config when the metrics view references a model inside the project since Rill does automatic cache management in that case.\n\n## Dialect-Specific Notes\n\nSQL expressions in dimensions and measures use the underlying OLAP database's dialect.\n\n### DuckDB\n\nDuckDB is the default OLAP engine for local development.\n\n**Conditional aggregation with FILTER**:\n```yaml\n# DuckDB supports FILTER clause for conditional aggregation\nexpression: COUNT(*) FILTER (WHERE status = 'completed')\n```\n\n### ClickHouse\n\nClickHouse is recommended for production workloads with large datasets.\n\n**Conditional aggregation**:\n```yaml\n# ClickHouse uses IF or CASE inside aggregations\nexpression: countIf(status = 'completed')\nexpression: sumIf(revenue, status = 'completed')\n```\n\n**Date functions**:\n```yaml\nexpression: toYear(order_date)\nexpression: toStartOfMonth(order_date)\nexpression: toYYYYMMDD(order_date)\n```\n\n**Array functions**:\n```yaml\nexpression: arrayJoin(tags)  # Unnest arrays\n```\n\n### Druid\n\n**Approximate distinct counts**:\n```yaml\nexpression: APPROX_COUNT_DISTINCT_DS_HLL(user_id)\n```\n\n## JSON Schema\n\n```\nallOf:\n    - properties:\n        ai_instructions:\n            description: Extra instructions for [AI agents](/explore/mcp). Used to guide natural language question answering and routing.\n            type: string\n        annotations:\n            description: Used to define annotations that can be displayed on charts\n            items:\n                properties:\n                    connector:\n                        description: Refers to the connector to use for the annotation\n                        type: string\n                    database:\n                        description: Refers to the database to use in the OLAP engine (to be used in conjunction with table). Otherwise, will use the default database or schema if not specified\n                        type: string\n                    database_schema:\n                        description: Refers to the schema to use in the OLAP engine (to be used in conjunction with table). Otherwise, will use the default database or schema if not specified\n                        type: string\n                    measures:\n                        anyOf:\n                            - description: Simple field name as a string.\n                              type: string\n                            - description: List of field selectors, each can be a string or an object with detailed configuration.\n                              items:\n                                anyOf:\n                                    - description: Shorthand field selector, interpreted as the name.\n                                      type: string\n                                    - additionalProperties: false\n                                      description: Detailed field selector configuration with name and optional time grain.\n                                      properties:\n                                        name:\n                                            description: Name of the field to select.\n                                            type: string\n                                        time_grain:\n                                            description: Time grain for time-based dimensions.\n                                            enum:\n                                                - \"\"\n                                                - ms\n                                                - millisecond\n                                                - s\n                                                - second\n                                                - min\n                                                - minute\n                                                - hadditionalProperties: fal\n                                                - hour\n                                                - d\n                                                - day\n                                                - w\n                                                - week\n                                                - month\n                                                - q\n                                                - quarter\n                                                - \"y\"\n                                                - year\n                                            type: string\n                                      required:\n                                        - name\n                                      type: object\n                              type: array\n                        description: Specifies which measures to apply the annotation to. Applies to all measures if not specified\n                    model:\n                        description: Refers to the model powering the annotation (either table or model is required). The model must have 'time' and 'description' columns. Optional columns include 'time_end' for range annotations and 'grain' to specify when the annotation should appear based on dashboard grain level.\n                        type: string\n                    name:\n                        description: A stable identifier for the annotation. Defaults to model or table names when not specified\n                        type: string\n                    table:\n                        description: Refers to the table powering the annotation, should be used instead of model for annotations from external OLAP tables (either table or model is required)\n                        type: string\n                type: object\n            type: array\n        connector:\n            description: Refers to the connector type for the metrics view, see [OLAP engines](/developers/build/connectors/olap) for more information\n            type: string\n        database:\n            description: Refers to the database to use in the OLAP engine (to be used in conjunction with table). Otherwise, will use the default database or schema if not specified\n            type: string\n        database_schema:\n            description: Refers to the schema to use in the OLAP engine (to be used in conjunction with table). Otherwise, will use the default database or schema if not specified\n            type: string\n        description:\n            description: Refers to the description for the metrics view\n            type: string\n        dimensions:\n            description: Relates to exploring segments or dimensions of your data and filtering the dashboard\n            items:\n                anyOf:\n                    - required:\n                        - column\n                    - required:\n                        - expression\n                properties:\n                    column:\n                        description: a categorical column\n                        type: string\n                    description:\n                        description: a freeform text description of the dimension\n                        type: string\n                    display_name:\n                        description: a display name for your dimension\n                        type: string\n                    expression:\n                        description: a non-aggregate expression such as string_split(domain, '.'). One of column and expression is required but cannot have both at the same time\n                        type: string\n                    name:\n                        description: a stable identifier for the dimension\n                        type: string\n                    tags:\n                        description: optional list of tags for categorizing the dimension (defaults to empty)\n                        items:\n                            type: string\n                        type: array\n                    type:\n                        description: 'Dimension type: \"geo\" for geospatial dimensions, \"time\" for time dimensions or \"categorical\" for categorial dimensions. Default is undefined and the type will be inferred instead'\n                        type: string\n                    unnest:\n                        description: if true, allows multi-valued dimension to be unnested (such as lists) and filters will automatically switch to \"contains\" instead of exact match\n                        type: boolean\n                    uri:\n                        description: enable if your dimension is a clickable URL to enable single click navigation (boolean or valid SQL expression)\n                        type:\n                            - string\n                            - boolean\n                type: object\n            type: array\n        display_name:\n            description: Refers to the display name for the metrics view\n            type: string\n        explore:\n            $ref: '#/definitions/explore_properties'\n            description: Defines an optional inline explore view for the metrics view. If not specified a default explore will be emitted unless `skip` is set to true.\n            required:\n                - type\n        first_day_of_week:\n            description: Refers to the first day of the week for time grain aggregation (for example, Sunday instead of Monday). The valid values are 1 through 7 where Monday=1 and Sunday=7\n            type: integer\n        first_month_of_year:\n            description: Refers to the first month of the year for time grain aggregation. The valid values are 1 through 12 where January=1 and December=12\n            type: integer\n        measures:\n            description: Used to define the numeric aggregates of columns from your data model\n            items:\n                properties:\n                    description:\n                        description: a freeform text description of the measure\n                        type: string\n                    display_name:\n                        description: the display name of your measure.\n                        type: string\n                    expression:\n                        description: a combination of operators and functions for aggregations\n                        type: string\n                    format_d3:\n                        description: 'Controls the formatting of this measure using a [d3-format](https://d3js.org/d3-format) string. If an invalid format string is supplied, the measure will fall back to `format_preset: humanize`. A measure cannot have both `format_preset` and `format_d3`. If neither is provided, the humanize preset is used by default. Example: `format_d3: \".2f\"` formats using fixed-point notation with two decimal places. Example: `format_d3: \",.2r\"` formats using grouped thousands with two significant digits. (optional)'\n                        type: string\n                    format_d3_locale:\n                        description: |\n                            locale configuration passed through to D3, enabling changing the currency symbol among other things. For details, see the docs for D3's formatLocale.\n                              ```yaml\n                              format_d3: \"$,\"\n                              format_d3_locale:\n                                grouping: [3, 2]\n                                currency: [\"\", \"\"]\n                              ```\n                        properties:\n                            currency:\n                                description: the currency symbol\n                                type: array\n                            grouping:\n                                description: the grouping of the currency symbol\n                                type: array\n                        type: object\n                    format_preset:\n                        description: |\n                            Controls the formatting of this measure using a predefined preset. Measures cannot have both `format_preset` and `format_d3`. If neither is supplied, the measure will be formatted using the `humanize` preset by default.\n\n                              Available options:\n                              - `humanize`: Round numbers into thousands (K), millions(M), billions (B), etc.\n                              - `none`: Raw output.\n                              - `currency_usd`: Round to 2 decimal points with a dollar sign ($).\n                              - `currency_eur`: Round to 2 decimal points with a euro sign ().\n                              - `percentage`: Convert a rate into a percentage with a % sign.\n                              - `interval_ms`: Convert milliseconds into human-readable durations like hours (h), days (d), years (y), etc. (optional)\n                        type: string\n                    label:\n                        description: a label for your measure, deprecated use display_name\n                        type: string\n                    name:\n                        description: a stable identifier for the measure\n                        type: string\n                    per:\n                        $ref: '#/definitions/field_selectors_properties'\n                        description: for per dimensions\n                    requires:\n                        $ref: '#/definitions/field_selectors_properties'\n                        description: using an available measure or dimension in your metrics view to set a required parameter, cannot be used with simple measures. See [referencing measures](/developers/build/metrics-view/measures/referencing) for more information.\n                    tags:\n                        description: optional list of tags for categorizing the measure (defaults to empty)\n                        items:\n                            type: string\n                        type: array\n                    treat_nulls_as:\n                        description: used to configure what value to fill in for missing time buckets. This also works generally as COALESCING over non empty time buckets.\n                        type: string\n                    type:\n                        description: 'Measure calculation type: \"simple\" for basic aggregations, \"derived\" for calculations using other measures, or \"time_comparison\" for period-over-period analysis. Defaults to \"simple\" unless dependencies exist.'\n                        type: string\n                    valid_percent_of_total:\n                        description: a boolean indicating whether percent-of-total values should be rendered for this measure\n                        type: boolean\n                    window:\n                        anyOf:\n                            - description: 'Shorthand: `time` or `true` means time-partitioned, `all` means non-partitioned.'\n                              enum:\n                                - time\n                                - \"true\"\n                                - all\n                              type: string\n                            - additionalProperties: false\n                              description: Detailed window configuration for measure calculations, allowing control over partitioning, ordering, and frame definition.\n                              properties:\n                                frame:\n                                    description: Defines the window frame boundaries for calculations, specifying which rows are included in the window relative to the current row.\n                                    type: string\n                                order:\n                                    $ref: '#/definitions/field_selectors_properties'\n                                    description: Specifies the fields to order the window by, determining the sequence of rows within each partition.\n                                    type: string\n                                partition:\n                                    description: Controls whether the window is partitioned. When true, calculations are performed within each partition separately.\n                                    type: boolean\n                              type: object\n                        description: A measure window can be defined as a keyword string (e.g. 'time' or 'all') or an object with detailed window configuration. For more information, see the [window functions](/developers/build/metrics-view/measures/windows) documentation.\n                required:\n                    - name\n                    - display_name\n                    - expression\n                type: object\n            type: array\n        model:\n            description: Refers to the model powering the dashboard (either model or table is required)\n            type: string\n        parent:\n            description: Refers to the parent metrics view from which this metrics view is derived. If specified, this will inherit properties from the parent metrics view\n            type: string\n        parent_dimensions:\n            $ref: '#/definitions/field_selector_properties'\n            description: Optional field selectors for dimensions to inherit from the parent metrics view.\n        parent_measures:\n            $ref: '#/definitions/field_selector_properties'\n            description: Optional field selectors for measures to inherit from the parent metrics view.\n        security:\n            $ref: '#/definitions/security_policy_properties'\n            description: Defines a security policy for the dashboard\n        smallest_time_grain:\n            description: 'Refers to the smallest time granularity the user is allowed to view. The valid values are: millisecond, second, minute, hour, day, week, month, quarter, year'\n            type: string\n        table:\n            description: Refers to the table powering the dashboard, should be used instead of model for dashboards create from external OLAP tables (either table or model is required)\n            type: string\n        timeseries:\n            description: Refers to the timestamp column from your model that will underlie x-axis data in the line charts. If not specified, the line charts will not appear\n            type: string\n        type:\n            const: metrics_view\n            description: Refers to the resource type and must be `metrics_view`\n            type: string\n        version:\n            description: The version of the metrics view schema\n            type: string\n        watermark:\n            description: A SQL expression that tells us the max timestamp that the measures are considered valid for. Usually does not need to be overwritten\n            type: string\n      title: Properties\n      type: object\n    - $ref: '#/definitions/common_properties'\ndescription: In your Rill project directory, create a metrics view, `<metrics_view>.yaml`, file in the `metrics` directory. Rill will ingest the metric view definition next time you run `rill start`.\nid: metrics-views\ntitle: Metrics View YAML\ntype: object\n```"
    - role: user
      content_type: text
      content: |-
        You should develop a Rill project file based on the following task description: Update the metrics view for the covid table to resolve the validation error: rename each measure so it does not match the underlying column name. Use the following naming convention: for sum aggregations, use the suffix _sum (e.g., new_confirmed_sum); for max aggregations, use the suffix _max (e.g., cumulative_confirmed_max). Ensure all other configuration remains unchanged.

        Here is some important context:
        - You should develop the file at path: /metrics/covid.yaml
        - The file should be of type: metrics_view
        - You are running as a sub-agent of a larger developer agent. Stay aligned on your specific task, avoid extra discovery, and report back your success or failure to the parent agent.

        Here is some additional context that may or may not be relevant to your task:
        - The project's default OLAP connector is named clickhouse (driver: clickhouse).
  response:
    - role: assistant
      content_type: tool_call
      id: call_2G09mhn7Ryq3kk6hBc1rBpad
      tool_name: read_file
      content: '{"path":"/metrics/covid.yaml"}'
- index: 19
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. Non-materialized models become views, which re-execute on every query.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: system
      content_type: text
      content: "# Instructions for developing a metrics view in Rill\n\n## Introduction\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database. They implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\n\nMetrics views are lightweight resources that only perform validation when reconciled. They are typically found downstream of connectors and models in the project's DAG. They power many user-facing features:\n\n- **Explore dashboards**: Interactive drill-down interfaces for data exploration\n- **Canvas dashboards**: Custom chart and table components\n- **Alerts**: Notifications when data meets certain criteria\n- **Reports**: Scheduled data exports and summaries\n- **Custom APIs**: Programmatic access to metrics\n\n## Core Concepts\n\n### Table source\n\nThe `model:` property specifies the underlying table that powers the metrics view. It can reference:\n\n1. **A model in the project**: Just use the model name (e.g., `model: events`)\n2. **An external table**: Specify the table name as it exists in the OLAP connector\n\n```yaml\n# Referencing a model in the project\nmodel: events\n\n# Referencing an external table (connector defaults to project's default OLAP)\nconnector: clickhouse  # Optional: specify if different from default\nmodel: my_external_table\n```\n\n**Note**: The `table:` property is a legacy alias for referencing external tables. Always prefer `model:` in new metrics views.\n\n### Timeseries\n\nThe `timeseries:` property identifies the timestamp column used for time-based filtering and line charts. This column must be a time/timestamp type in the underlying table.\n\n```yaml\ntimeseries: event_time\n```\n\nIf the timeseries column is not listed in `dimensions:`, Rill automatically adds it as a time dimension. You can optionally configure additional time-related settings:\n\n```yaml\ntimeseries: event_time\nsmallest_time_grain: hour      # Minimum granularity users can select\nfirst_day_of_week: 7           # Sunday (1=Monday, 7=Sunday)\nfirst_month_of_year: 4         # April (fiscal year starting in April)\n```\n\n### Dimensions\n\nDimensions are attributes you can group by or filter on. They are typically categorical (strings, enums) or temporal (dates, timestamps). Rill infers the dimension type from the underlying SQL data type:\n\n- **Categorical**: String, enum, boolean columns\n- **Time**: Timestamp, date, datetime columns\n- **Geospatial**: Geometry or geography columns\n\nDefine dimensions using either a direct column reference or a SQL expression:\n\n```yaml\ndimensions:\n  # Simple column reference\n  - name: country\n    column: country\n\n  # Computed expression\n  - name: device_category\n    expression: CASE WHEN device_type IN ('phone', 'tablet') THEN 'Mobile' ELSE 'Desktop' END\n\n  # With display name and description\n  - name: campaign_name\n    display_name: Campaign\n    description: Marketing campaign that drove the traffic\n    column: campaign_name\n```\n\n**Naming**: Each dimension needs a `name` (stable identifier used in APIs and references), which defaults to `column:` if provided. The `display_name:` is optional, and defaults to a humanized version of `name` if not specified.\n\n### Measures\n\nMeasures are aggregation expressions that compute numeric values when grouped by dimensions. They must use aggregate functions like `SUM()`, `COUNT()`, `AVG()`, `MIN()`, `MAX()`.\n\n```yaml\nmeasures:\n  - name: total_revenue\n    display_name: Total Revenue\n    expression: SUM(revenue)\n    description: Sum of all revenue in USD\n    format_preset: currency_usd\n\n  - name: unique_users\n    display_name: Unique Users\n    expression: COUNT(DISTINCT user_id)\n    format_preset: humanize\n\n  - name: conversion_rate\n    display_name: Conversion Rate\n    expression: SUM(conversions) / NULLIF(SUM(visits), 0)\n    format_preset: percentage\n    valid_percent_of_total: false  # Disable % of total for ratios\n```\n\n**Format presets**: Control how values are displayed:\n- `none`: Raw number\n- `humanize`: Round to K, M, B (e.g., 1.2M)\n- `currency_usd`: Dollar format with 2 decimals ($1,234.56)\n- `currency_eur`: Euro format\n- `percentage`: Multiply by 100 and add % sign\n- `interval_ms`: Convert milliseconds to human-readable duration\n\nFor custom formatting, use `format_d3` with a [d3-format](https://d3js.org/d3-format) string:\n\n```yaml\nformat_d3: \"$,.2f\"  # $1,234.56\nformat_d3: \".1%\"    # 12.3%\nformat_d3: \",.0f\"   # 1,235 (rounded, with thousands separator)\n```\n\n### Best practices for dimensions and measures\n\n**Naming conventions:**\n- Use `snake_case` for the `name` field (e.g., `total_revenue`, `unique_users`)\n- Only add `display_name` and `description` if they provide meaningful context beyond what `name` conveys (display names auto-humanize from the name by default)\n- Ensure measure names don't collide with column names in the underlying table\n\n**Getting started with measures:**\n- Start with a `COUNT(*)` measure as a baseline (e.g., `total_records` or `total_events`)\n- Add `SUM()` measures for numeric columns that represent quantities or values\n- Use `humanize` as the default format preset unless the data has a specific format requirement\n- Keep initial measures simple using only `COUNT`, `SUM`, `AVG`, `MIN`, `MAX` aggregations\n- Add more complex expressions (ratios, conditional aggregations) only when needed\n\n**Dimension selection:**\n- Include all categorical columns (strings, enums, booleans) that users might want to filter or group by\n- Include date/timestamp columns as dimensions for time-based analysis beyond the main timeseries\n- Start with 5-10 dimensions; add more based on user needs\n\n### Auto-generated explore\n\nWhen you create a metrics view, Rill automatically generates an explore dashboard with the same name, exposing all dimensions and measures. To customize the explore (you usually should not need to), add an `explore:` block:\n\n```yaml\nexplore:\n  display_name: Sales Dashboard\n  defaults:\n    time_range: P7D\n    measures:\n      - total_revenue\n      - order_count\n```\n\n**Legacy behavior**: Files with `version: 1` do NOT auto-generate an explore. Omit `version:` in new metrics views to get the auto-generated explore.\n\n## Full Example\n\nHere is a complete, annotated metrics view:\n\n```yaml\n# metrics/orders.yaml\ntype: metrics_view\n\n# Display metadata\ndisplay_name: Orders Analytics\ndescription: Analyze order performance by various dimensions\n\n# Data source - references the 'orders' model in the project\nmodel: orders\n\n# Time column for time-series charts and filtering\ntimeseries: order_date\nsmallest_time_grain: day\n\n# Dimensions for grouping and filtering\ndimensions:\n  - name: order_date\n    display_name: Order Date\n    column: order_date\n\n  - name: country\n    display_name: Country\n    column: shipping_country\n\n  - name: product_category\n    display_name: Product Category\n    column: category\n    description: High-level product grouping\n\n  - name: customer_segment\n    display_name: Customer Segment\n    expression: | \n      CASE\n        WHEN lifetime_value > 1000 THEN 'High Value'\n        WHEN lifetime_value > 100 THEN 'Medium Value'\n        ELSE 'Low Value'\n      END\n\n  - name: is_repeat_customer\n    display_name: Repeat Customer\n    expression: CASE WHEN order_number > 1 THEN 'Yes' ELSE 'No' END\n\n# Measures for aggregation\nmeasures:\n  - name: total_orders\n    display_name: Total Orders\n    expression: COUNT(*)\n    format_preset: humanize\n\n  - name: total_revenue\n    display_name: Total Revenue\n    expression: SUM(order_total)\n    format_preset: currency_usd\n    description: Gross revenue before refunds\n\n  - name: average_order_value\n    display_name: Avg Order Value\n    expression: SUM(order_total) / NULLIF(COUNT(*), 0)\n    format_preset: currency_usd\n    valid_percent_of_total: false\n\n  - name: unique_customers\n    display_name: Unique Customers\n    expression: COUNT(DISTINCT customer_id)\n    format_preset: humanize\n\n  - name: items_per_order\n    display_name: Items per Order\n    expression: SUM(item_count) / NULLIF(COUNT(*), 0)\n    format_d3: \",.1f\"\n    valid_percent_of_total: false\n```\n\n## Security Policies\n\nSecurity policies control who can access a metrics view and what data they can see. This is a powerful feature for multi-tenant dashboards and role-based access control.\n\n### Basic access control\n\nThe `access:` property controls whether users can view the metrics view at all:\n\n```yaml\nsecurity:\n  # Allow access for everyone\n  access: true\n\n  # Deny access for everyone (useful for draft dashboards)\n  access: false\n\n  # Conditional access based on user attributes\n  access: \"'{{ .user.admin }}' = 'true'\"\n```\n\nThe expression syntax should be a DuckDB expression, which will be evaluated in a sandbox without access to any tables.\n\n### Row-level security\n\nThe `row_filter:` property restricts which rows a user can see. It's a SQL expression that references user attributes via templating:\n\n```yaml\nsecurity:\n  access: true\n  row_filter: domain = '{{ .user.domain }}'\n```\n\nCommon user attributes:\n- `{{ .user.email }}`: User's email address\n- `{{ .user.domain }}`: Email domain (e.g., \"acme.com\")\n- `{{ .user.admin }}`: Boolean admin flag\n- Custom attributes configured in Rill Cloud\n\nThe row filter should use the SQL syntax of the metrics view's model, and can reference other tables in the model's connector.\n\n### Complex row filters\n\nUse logical operators for sophisticated access patterns:\n\n```yaml\nsecurity:\n  access: true\n  row_filter: >\n    {{ .user.admin }}\n    OR '{{ .user.domain }}' = 'acme.com'\n    {{ if hasKey .user \"tenant_id\" }}\n    OR tenant_id = '{{ .user.tenant_id }}'\n    {{ end }}\n```\n\n### Hiding dimensions and measures\n\nThe `exclude:` property conditionally hides specific dimensions or measures from certain users:\n\n```yaml\nsecurity:\n  access: true\n  exclude:\n    - if: \"NOT {{ .user.admin }}\"\n      names:\n        - cost_per_acquisition  # Hide sensitive cost data from non-admins\n        - internal_notes\n```\n\n## Advanced Features\n\n### Annotations\n\nAnnotations overlay contextual information (like events or milestones) on time-series charts:\n\n```yaml\nannotations:\n  - name: product_launches\n    model: product_launches  # Must have 'time' and 'description' columns\n    measures:\n      - total_revenue        # Only show on these measures\n\n  # Optional columns in annotation model:\n  # - time_end: For range annotations\n  # - grain: Show only at specific time grains (day, week, etc.)\n```\n\n### Unnest for array dimensions\n\nWhen a column contains arrays, use `unnest: true` to flatten it at query time:\n\n```yaml\ndimensions:\n  - name: tags\n    display_name: Tags\n    column: tags\n    unnest: true  # Allows filtering by individual array elements\n```\n\n### Cache configuration\n\nConfigure caching for slow metrics views that use external tables:\n\n```yaml\ncache:\n  enabled: true\n  key_ttl: 5m\n  key_sql: SELECT MAX(updated_at) FROM orders\n```\n\nYou should not add a `cache:` config when the metrics view references a model inside the project since Rill does automatic cache management in that case.\n\n## Dialect-Specific Notes\n\nSQL expressions in dimensions and measures use the underlying OLAP database's dialect.\n\n### DuckDB\n\nDuckDB is the default OLAP engine for local development.\n\n**Conditional aggregation with FILTER**:\n```yaml\n# DuckDB supports FILTER clause for conditional aggregation\nexpression: COUNT(*) FILTER (WHERE status = 'completed')\n```\n\n### ClickHouse\n\nClickHouse is recommended for production workloads with large datasets.\n\n**Conditional aggregation**:\n```yaml\n# ClickHouse uses IF or CASE inside aggregations\nexpression: countIf(status = 'completed')\nexpression: sumIf(revenue, status = 'completed')\n```\n\n**Date functions**:\n```yaml\nexpression: toYear(order_date)\nexpression: toStartOfMonth(order_date)\nexpression: toYYYYMMDD(order_date)\n```\n\n**Array functions**:\n```yaml\nexpression: arrayJoin(tags)  # Unnest arrays\n```\n\n### Druid\n\n**Approximate distinct counts**:\n```yaml\nexpression: APPROX_COUNT_DISTINCT_DS_HLL(user_id)\n```\n\n## JSON Schema\n\n```\nallOf:\n    - properties:\n        ai_instructions:\n            description: Extra instructions for [AI agents](/explore/mcp). Used to guide natural language question answering and routing.\n            type: string\n        annotations:\n            description: Used to define annotations that can be displayed on charts\n            items:\n                properties:\n                    connector:\n                        description: Refers to the connector to use for the annotation\n                        type: string\n                    database:\n                        description: Refers to the database to use in the OLAP engine (to be used in conjunction with table). Otherwise, will use the default database or schema if not specified\n                        type: string\n                    database_schema:\n                        description: Refers to the schema to use in the OLAP engine (to be used in conjunction with table). Otherwise, will use the default database or schema if not specified\n                        type: string\n                    measures:\n                        anyOf:\n                            - description: Simple field name as a string.\n                              type: string\n                            - description: List of field selectors, each can be a string or an object with detailed configuration.\n                              items:\n                                anyOf:\n                                    - description: Shorthand field selector, interpreted as the name.\n                                      type: string\n                                    - additionalProperties: false\n                                      description: Detailed field selector configuration with name and optional time grain.\n                                      properties:\n                                        name:\n                                            description: Name of the field to select.\n                                            type: string\n                                        time_grain:\n                                            description: Time grain for time-based dimensions.\n                                            enum:\n                                                - \"\"\n                                                - ms\n                                                - millisecond\n                                                - s\n                                                - second\n                                                - min\n                                                - minute\n                                                - hadditionalProperties: fal\n                                                - hour\n                                                - d\n                                                - day\n                                                - w\n                                                - week\n                                                - month\n                                                - q\n                                                - quarter\n                                                - \"y\"\n                                                - year\n                                            type: string\n                                      required:\n                                        - name\n                                      type: object\n                              type: array\n                        description: Specifies which measures to apply the annotation to. Applies to all measures if not specified\n                    model:\n                        description: Refers to the model powering the annotation (either table or model is required). The model must have 'time' and 'description' columns. Optional columns include 'time_end' for range annotations and 'grain' to specify when the annotation should appear based on dashboard grain level.\n                        type: string\n                    name:\n                        description: A stable identifier for the annotation. Defaults to model or table names when not specified\n                        type: string\n                    table:\n                        description: Refers to the table powering the annotation, should be used instead of model for annotations from external OLAP tables (either table or model is required)\n                        type: string\n                type: object\n            type: array\n        connector:\n            description: Refers to the connector type for the metrics view, see [OLAP engines](/developers/build/connectors/olap) for more information\n            type: string\n        database:\n            description: Refers to the database to use in the OLAP engine (to be used in conjunction with table). Otherwise, will use the default database or schema if not specified\n            type: string\n        database_schema:\n            description: Refers to the schema to use in the OLAP engine (to be used in conjunction with table). Otherwise, will use the default database or schema if not specified\n            type: string\n        description:\n            description: Refers to the description for the metrics view\n            type: string\n        dimensions:\n            description: Relates to exploring segments or dimensions of your data and filtering the dashboard\n            items:\n                anyOf:\n                    - required:\n                        - column\n                    - required:\n                        - expression\n                properties:\n                    column:\n                        description: a categorical column\n                        type: string\n                    description:\n                        description: a freeform text description of the dimension\n                        type: string\n                    display_name:\n                        description: a display name for your dimension\n                        type: string\n                    expression:\n                        description: a non-aggregate expression such as string_split(domain, '.'). One of column and expression is required but cannot have both at the same time\n                        type: string\n                    name:\n                        description: a stable identifier for the dimension\n                        type: string\n                    tags:\n                        description: optional list of tags for categorizing the dimension (defaults to empty)\n                        items:\n                            type: string\n                        type: array\n                    type:\n                        description: 'Dimension type: \"geo\" for geospatial dimensions, \"time\" for time dimensions or \"categorical\" for categorial dimensions. Default is undefined and the type will be inferred instead'\n                        type: string\n                    unnest:\n                        description: if true, allows multi-valued dimension to be unnested (such as lists) and filters will automatically switch to \"contains\" instead of exact match\n                        type: boolean\n                    uri:\n                        description: enable if your dimension is a clickable URL to enable single click navigation (boolean or valid SQL expression)\n                        type:\n                            - string\n                            - boolean\n                type: object\n            type: array\n        display_name:\n            description: Refers to the display name for the metrics view\n            type: string\n        explore:\n            $ref: '#/definitions/explore_properties'\n            description: Defines an optional inline explore view for the metrics view. If not specified a default explore will be emitted unless `skip` is set to true.\n            required:\n                - type\n        first_day_of_week:\n            description: Refers to the first day of the week for time grain aggregation (for example, Sunday instead of Monday). The valid values are 1 through 7 where Monday=1 and Sunday=7\n            type: integer\n        first_month_of_year:\n            description: Refers to the first month of the year for time grain aggregation. The valid values are 1 through 12 where January=1 and December=12\n            type: integer\n        measures:\n            description: Used to define the numeric aggregates of columns from your data model\n            items:\n                properties:\n                    description:\n                        description: a freeform text description of the measure\n                        type: string\n                    display_name:\n                        description: the display name of your measure.\n                        type: string\n                    expression:\n                        description: a combination of operators and functions for aggregations\n                        type: string\n                    format_d3:\n                        description: 'Controls the formatting of this measure using a [d3-format](https://d3js.org/d3-format) string. If an invalid format string is supplied, the measure will fall back to `format_preset: humanize`. A measure cannot have both `format_preset` and `format_d3`. If neither is provided, the humanize preset is used by default. Example: `format_d3: \".2f\"` formats using fixed-point notation with two decimal places. Example: `format_d3: \",.2r\"` formats using grouped thousands with two significant digits. (optional)'\n                        type: string\n                    format_d3_locale:\n                        description: |\n                            locale configuration passed through to D3, enabling changing the currency symbol among other things. For details, see the docs for D3's formatLocale.\n                              ```yaml\n                              format_d3: \"$,\"\n                              format_d3_locale:\n                                grouping: [3, 2]\n                                currency: [\"\", \"\"]\n                              ```\n                        properties:\n                            currency:\n                                description: the currency symbol\n                                type: array\n                            grouping:\n                                description: the grouping of the currency symbol\n                                type: array\n                        type: object\n                    format_preset:\n                        description: |\n                            Controls the formatting of this measure using a predefined preset. Measures cannot have both `format_preset` and `format_d3`. If neither is supplied, the measure will be formatted using the `humanize` preset by default.\n\n                              Available options:\n                              - `humanize`: Round numbers into thousands (K), millions(M), billions (B), etc.\n                              - `none`: Raw output.\n                              - `currency_usd`: Round to 2 decimal points with a dollar sign ($).\n                              - `currency_eur`: Round to 2 decimal points with a euro sign ().\n                              - `percentage`: Convert a rate into a percentage with a % sign.\n                              - `interval_ms`: Convert milliseconds into human-readable durations like hours (h), days (d), years (y), etc. (optional)\n                        type: string\n                    label:\n                        description: a label for your measure, deprecated use display_name\n                        type: string\n                    name:\n                        description: a stable identifier for the measure\n                        type: string\n                    per:\n                        $ref: '#/definitions/field_selectors_properties'\n                        description: for per dimensions\n                    requires:\n                        $ref: '#/definitions/field_selectors_properties'\n                        description: using an available measure or dimension in your metrics view to set a required parameter, cannot be used with simple measures. See [referencing measures](/developers/build/metrics-view/measures/referencing) for more information.\n                    tags:\n                        description: optional list of tags for categorizing the measure (defaults to empty)\n                        items:\n                            type: string\n                        type: array\n                    treat_nulls_as:\n                        description: used to configure what value to fill in for missing time buckets. This also works generally as COALESCING over non empty time buckets.\n                        type: string\n                    type:\n                        description: 'Measure calculation type: \"simple\" for basic aggregations, \"derived\" for calculations using other measures, or \"time_comparison\" for period-over-period analysis. Defaults to \"simple\" unless dependencies exist.'\n                        type: string\n                    valid_percent_of_total:\n                        description: a boolean indicating whether percent-of-total values should be rendered for this measure\n                        type: boolean\n                    window:\n                        anyOf:\n                            - description: 'Shorthand: `time` or `true` means time-partitioned, `all` means non-partitioned.'\n                              enum:\n                                - time\n                                - \"true\"\n                                - all\n                              type: string\n                            - additionalProperties: false\n                              description: Detailed window configuration for measure calculations, allowing control over partitioning, ordering, and frame definition.\n                              properties:\n                                frame:\n                                    description: Defines the window frame boundaries for calculations, specifying which rows are included in the window relative to the current row.\n                                    type: string\n                                order:\n                                    $ref: '#/definitions/field_selectors_properties'\n                                    description: Specifies the fields to order the window by, determining the sequence of rows within each partition.\n                                    type: string\n                                partition:\n                                    description: Controls whether the window is partitioned. When true, calculations are performed within each partition separately.\n                                    type: boolean\n                              type: object\n                        description: A measure window can be defined as a keyword string (e.g. 'time' or 'all') or an object with detailed window configuration. For more information, see the [window functions](/developers/build/metrics-view/measures/windows) documentation.\n                required:\n                    - name\n                    - display_name\n                    - expression\n                type: object\n            type: array\n        model:\n            description: Refers to the model powering the dashboard (either model or table is required)\n            type: string\n        parent:\n            description: Refers to the parent metrics view from which this metrics view is derived. If specified, this will inherit properties from the parent metrics view\n            type: string\n        parent_dimensions:\n            $ref: '#/definitions/field_selector_properties'\n            description: Optional field selectors for dimensions to inherit from the parent metrics view.\n        parent_measures:\n            $ref: '#/definitions/field_selector_properties'\n            description: Optional field selectors for measures to inherit from the parent metrics view.\n        security:\n            $ref: '#/definitions/security_policy_properties'\n            description: Defines a security policy for the dashboard\n        smallest_time_grain:\n            description: 'Refers to the smallest time granularity the user is allowed to view. The valid values are: millisecond, second, minute, hour, day, week, month, quarter, year'\n            type: string\n        table:\n            description: Refers to the table powering the dashboard, should be used instead of model for dashboards create from external OLAP tables (either table or model is required)\n            type: string\n        timeseries:\n            description: Refers to the timestamp column from your model that will underlie x-axis data in the line charts. If not specified, the line charts will not appear\n            type: string\n        type:\n            const: metrics_view\n            description: Refers to the resource type and must be `metrics_view`\n            type: string\n        version:\n            description: The version of the metrics view schema\n            type: string\n        watermark:\n            description: A SQL expression that tells us the max timestamp that the measures are considered valid for. Usually does not need to be overwritten\n            type: string\n      title: Properties\n      type: object\n    - $ref: '#/definitions/common_properties'\ndescription: In your Rill project directory, create a metrics view, `<metrics_view>.yaml`, file in the `metrics` directory. Rill will ingest the metric view definition next time you run `rill start`.\nid: metrics-views\ntitle: Metrics View YAML\ntype: object\n```"
    - role: user
      content_type: text
      content: |-
        You should develop a Rill project file based on the following task description: Update the metrics view for the covid table to resolve the validation error: rename each measure so it does not match the underlying column name. Use the following naming convention: for sum aggregations, use the suffix _sum (e.g., new_confirmed_sum); for max aggregations, use the suffix _max (e.g., cumulative_confirmed_max). Ensure all other configuration remains unchanged.

        Here is some important context:
        - You should develop the file at path: /metrics/covid.yaml
        - The file should be of type: metrics_view
        - You are running as a sub-agent of a larger developer agent. Stay aligned on your specific task, avoid extra discovery, and report back your success or failure to the parent agent.

        Here is some additional context that may or may not be relevant to your task:
        - The project's default OLAP connector is named clickhouse (driver: clickhouse).
    - role: assistant
      content_type: tool_call
      id: f8898f24b1814422
      tool_name: read_file
      content: '{"path":"/metrics/covid.yaml"}'
    - role: tool
      content_type: tool_response
      id: f8898f24b1814422
      content: '{"Contents":"type: metrics_view\nmodel: covid\nconnector: clickhouse\ndatabase: default\ndimensions:\n  - name: date\n    column: date\n    display_name: Date\n    description: Date of the record\n  - name: location_key\n    column: location_key\n    display_name: Location Key\n    description: Location identifier\nmeasures:\n  - name: new_confirmed\n    display_name: New Confirmed\n    expression: SUM(new_confirmed)\n    format_preset: humanize\n    description: Sum of new confirmed cases\n  - name: new_deceased\n    display_name: New Deceased\n    expression: SUM(new_deceased)\n    format_preset: humanize\n    description: Sum of new deceased cases\n  - name: new_recovered\n    display_name: New Recovered\n    expression: SUM(new_recovered)\n    format_preset: humanize\n    description: Sum of new recovered cases\n  - name: new_tested\n    display_name: New Tested\n    expression: SUM(new_tested)\n    format_preset: humanize\n    description: Sum of new tested cases\n  - name: cumulative_confirmed\n    display_name: Cumulative Confirmed\n    expression: MAX(cumulative_confirmed)\n    format_preset: humanize\n    description: Maximum cumulative confirmed cases\n  - name: cumulative_deceased\n    display_name: Cumulative Deceased\n    expression: MAX(cumulative_deceased)\n    format_preset: humanize\n    description: Maximum cumulative deceased cases\n  - name: cumulative_recovered\n    display_name: Cumulative Recovered\n    expression: MAX(cumulative_recovered)\n    format_preset: humanize\n    description: Maximum cumulative recovered cases\n  - name: cumulative_tested\n    display_name: Cumulative Tested\n    expression: MAX(cumulative_tested)\n    format_preset: humanize\n    description: Maximum cumulative tested cases\ntimeseries: date\n"}'
  response:
    - role: assistant
      content_type: tool_call
      id: call_eqtQRhxTHkJDDhqgIrTMkNbu
      tool_name: write_file
      content: '{"contents":"type: metrics_view\nmodel: covid\nconnector: clickhouse\ndatabase: default\ndimensions:\n  - name: date\n    column: date\n    display_name: Date\n    description: Date of the record\n  - name: location_key\n    column: location_key\n    display_name: Location Key\n    description: Location identifier\nmeasures:\n  - name: new_confirmed_sum\n    display_name: New Confirmed\n    expression: SUM(new_confirmed)\n    format_preset: humanize\n    description: Sum of new confirmed cases\n  - name: new_deceased_sum\n    display_name: New Deceased\n    expression: SUM(new_deceased)\n    format_preset: humanize\n    description: Sum of new deceased cases\n  - name: new_recovered_sum\n    display_name: New Recovered\n    expression: SUM(new_recovered)\n    format_preset: humanize\n    description: Sum of new recovered cases\n  - name: new_tested_sum\n    display_name: New Tested\n    expression: SUM(new_tested)\n    format_preset: humanize\n    description: Sum of new tested cases\n  - name: cumulative_confirmed_max\n    display_name: Cumulative Confirmed\n    expression: MAX(cumulative_confirmed)\n    format_preset: humanize\n    description: Maximum cumulative confirmed cases\n  - name: cumulative_deceased_max\n    display_name: Cumulative Deceased\n    expression: MAX(cumulative_deceased)\n    format_preset: humanize\n    description: Maximum cumulative deceased cases\n  - name: cumulative_recovered_max\n    display_name: Cumulative Recovered\n    expression: MAX(cumulative_recovered)\n    format_preset: humanize\n    description: Maximum cumulative recovered cases\n  - name: cumulative_tested_max\n    display_name: Cumulative Tested\n    expression: MAX(cumulative_tested)\n    format_preset: humanize\n    description: Maximum cumulative tested cases\ntimeseries: date\n","path":"/metrics/covid.yaml"}'
- index: 20
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. Non-materialized models become views, which re-execute on every query.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: system
      content_type: text
      content: "# Instructions for developing a metrics view in Rill\n\n## Introduction\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database. They implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\n\nMetrics views are lightweight resources that only perform validation when reconciled. They are typically found downstream of connectors and models in the project's DAG. They power many user-facing features:\n\n- **Explore dashboards**: Interactive drill-down interfaces for data exploration\n- **Canvas dashboards**: Custom chart and table components\n- **Alerts**: Notifications when data meets certain criteria\n- **Reports**: Scheduled data exports and summaries\n- **Custom APIs**: Programmatic access to metrics\n\n## Core Concepts\n\n### Table source\n\nThe `model:` property specifies the underlying table that powers the metrics view. It can reference:\n\n1. **A model in the project**: Just use the model name (e.g., `model: events`)\n2. **An external table**: Specify the table name as it exists in the OLAP connector\n\n```yaml\n# Referencing a model in the project\nmodel: events\n\n# Referencing an external table (connector defaults to project's default OLAP)\nconnector: clickhouse  # Optional: specify if different from default\nmodel: my_external_table\n```\n\n**Note**: The `table:` property is a legacy alias for referencing external tables. Always prefer `model:` in new metrics views.\n\n### Timeseries\n\nThe `timeseries:` property identifies the timestamp column used for time-based filtering and line charts. This column must be a time/timestamp type in the underlying table.\n\n```yaml\ntimeseries: event_time\n```\n\nIf the timeseries column is not listed in `dimensions:`, Rill automatically adds it as a time dimension. You can optionally configure additional time-related settings:\n\n```yaml\ntimeseries: event_time\nsmallest_time_grain: hour      # Minimum granularity users can select\nfirst_day_of_week: 7           # Sunday (1=Monday, 7=Sunday)\nfirst_month_of_year: 4         # April (fiscal year starting in April)\n```\n\n### Dimensions\n\nDimensions are attributes you can group by or filter on. They are typically categorical (strings, enums) or temporal (dates, timestamps). Rill infers the dimension type from the underlying SQL data type:\n\n- **Categorical**: String, enum, boolean columns\n- **Time**: Timestamp, date, datetime columns\n- **Geospatial**: Geometry or geography columns\n\nDefine dimensions using either a direct column reference or a SQL expression:\n\n```yaml\ndimensions:\n  # Simple column reference\n  - name: country\n    column: country\n\n  # Computed expression\n  - name: device_category\n    expression: CASE WHEN device_type IN ('phone', 'tablet') THEN 'Mobile' ELSE 'Desktop' END\n\n  # With display name and description\n  - name: campaign_name\n    display_name: Campaign\n    description: Marketing campaign that drove the traffic\n    column: campaign_name\n```\n\n**Naming**: Each dimension needs a `name` (stable identifier used in APIs and references), which defaults to `column:` if provided. The `display_name:` is optional, and defaults to a humanized version of `name` if not specified.\n\n### Measures\n\nMeasures are aggregation expressions that compute numeric values when grouped by dimensions. They must use aggregate functions like `SUM()`, `COUNT()`, `AVG()`, `MIN()`, `MAX()`.\n\n```yaml\nmeasures:\n  - name: total_revenue\n    display_name: Total Revenue\n    expression: SUM(revenue)\n    description: Sum of all revenue in USD\n    format_preset: currency_usd\n\n  - name: unique_users\n    display_name: Unique Users\n    expression: COUNT(DISTINCT user_id)\n    format_preset: humanize\n\n  - name: conversion_rate\n    display_name: Conversion Rate\n    expression: SUM(conversions) / NULLIF(SUM(visits), 0)\n    format_preset: percentage\n    valid_percent_of_total: false  # Disable % of total for ratios\n```\n\n**Format presets**: Control how values are displayed:\n- `none`: Raw number\n- `humanize`: Round to K, M, B (e.g., 1.2M)\n- `currency_usd`: Dollar format with 2 decimals ($1,234.56)\n- `currency_eur`: Euro format\n- `percentage`: Multiply by 100 and add % sign\n- `interval_ms`: Convert milliseconds to human-readable duration\n\nFor custom formatting, use `format_d3` with a [d3-format](https://d3js.org/d3-format) string:\n\n```yaml\nformat_d3: \"$,.2f\"  # $1,234.56\nformat_d3: \".1%\"    # 12.3%\nformat_d3: \",.0f\"   # 1,235 (rounded, with thousands separator)\n```\n\n### Best practices for dimensions and measures\n\n**Naming conventions:**\n- Use `snake_case` for the `name` field (e.g., `total_revenue`, `unique_users`)\n- Only add `display_name` and `description` if they provide meaningful context beyond what `name` conveys (display names auto-humanize from the name by default)\n- Ensure measure names don't collide with column names in the underlying table\n\n**Getting started with measures:**\n- Start with a `COUNT(*)` measure as a baseline (e.g., `total_records` or `total_events`)\n- Add `SUM()` measures for numeric columns that represent quantities or values\n- Use `humanize` as the default format preset unless the data has a specific format requirement\n- Keep initial measures simple using only `COUNT`, `SUM`, `AVG`, `MIN`, `MAX` aggregations\n- Add more complex expressions (ratios, conditional aggregations) only when needed\n\n**Dimension selection:**\n- Include all categorical columns (strings, enums, booleans) that users might want to filter or group by\n- Include date/timestamp columns as dimensions for time-based analysis beyond the main timeseries\n- Start with 5-10 dimensions; add more based on user needs\n\n### Auto-generated explore\n\nWhen you create a metrics view, Rill automatically generates an explore dashboard with the same name, exposing all dimensions and measures. To customize the explore (you usually should not need to), add an `explore:` block:\n\n```yaml\nexplore:\n  display_name: Sales Dashboard\n  defaults:\n    time_range: P7D\n    measures:\n      - total_revenue\n      - order_count\n```\n\n**Legacy behavior**: Files with `version: 1` do NOT auto-generate an explore. Omit `version:` in new metrics views to get the auto-generated explore.\n\n## Full Example\n\nHere is a complete, annotated metrics view:\n\n```yaml\n# metrics/orders.yaml\ntype: metrics_view\n\n# Display metadata\ndisplay_name: Orders Analytics\ndescription: Analyze order performance by various dimensions\n\n# Data source - references the 'orders' model in the project\nmodel: orders\n\n# Time column for time-series charts and filtering\ntimeseries: order_date\nsmallest_time_grain: day\n\n# Dimensions for grouping and filtering\ndimensions:\n  - name: order_date\n    display_name: Order Date\n    column: order_date\n\n  - name: country\n    display_name: Country\n    column: shipping_country\n\n  - name: product_category\n    display_name: Product Category\n    column: category\n    description: High-level product grouping\n\n  - name: customer_segment\n    display_name: Customer Segment\n    expression: | \n      CASE\n        WHEN lifetime_value > 1000 THEN 'High Value'\n        WHEN lifetime_value > 100 THEN 'Medium Value'\n        ELSE 'Low Value'\n      END\n\n  - name: is_repeat_customer\n    display_name: Repeat Customer\n    expression: CASE WHEN order_number > 1 THEN 'Yes' ELSE 'No' END\n\n# Measures for aggregation\nmeasures:\n  - name: total_orders\n    display_name: Total Orders\n    expression: COUNT(*)\n    format_preset: humanize\n\n  - name: total_revenue\n    display_name: Total Revenue\n    expression: SUM(order_total)\n    format_preset: currency_usd\n    description: Gross revenue before refunds\n\n  - name: average_order_value\n    display_name: Avg Order Value\n    expression: SUM(order_total) / NULLIF(COUNT(*), 0)\n    format_preset: currency_usd\n    valid_percent_of_total: false\n\n  - name: unique_customers\n    display_name: Unique Customers\n    expression: COUNT(DISTINCT customer_id)\n    format_preset: humanize\n\n  - name: items_per_order\n    display_name: Items per Order\n    expression: SUM(item_count) / NULLIF(COUNT(*), 0)\n    format_d3: \",.1f\"\n    valid_percent_of_total: false\n```\n\n## Security Policies\n\nSecurity policies control who can access a metrics view and what data they can see. This is a powerful feature for multi-tenant dashboards and role-based access control.\n\n### Basic access control\n\nThe `access:` property controls whether users can view the metrics view at all:\n\n```yaml\nsecurity:\n  # Allow access for everyone\n  access: true\n\n  # Deny access for everyone (useful for draft dashboards)\n  access: false\n\n  # Conditional access based on user attributes\n  access: \"'{{ .user.admin }}' = 'true'\"\n```\n\nThe expression syntax should be a DuckDB expression, which will be evaluated in a sandbox without access to any tables.\n\n### Row-level security\n\nThe `row_filter:` property restricts which rows a user can see. It's a SQL expression that references user attributes via templating:\n\n```yaml\nsecurity:\n  access: true\n  row_filter: domain = '{{ .user.domain }}'\n```\n\nCommon user attributes:\n- `{{ .user.email }}`: User's email address\n- `{{ .user.domain }}`: Email domain (e.g., \"acme.com\")\n- `{{ .user.admin }}`: Boolean admin flag\n- Custom attributes configured in Rill Cloud\n\nThe row filter should use the SQL syntax of the metrics view's model, and can reference other tables in the model's connector.\n\n### Complex row filters\n\nUse logical operators for sophisticated access patterns:\n\n```yaml\nsecurity:\n  access: true\n  row_filter: >\n    {{ .user.admin }}\n    OR '{{ .user.domain }}' = 'acme.com'\n    {{ if hasKey .user \"tenant_id\" }}\n    OR tenant_id = '{{ .user.tenant_id }}'\n    {{ end }}\n```\n\n### Hiding dimensions and measures\n\nThe `exclude:` property conditionally hides specific dimensions or measures from certain users:\n\n```yaml\nsecurity:\n  access: true\n  exclude:\n    - if: \"NOT {{ .user.admin }}\"\n      names:\n        - cost_per_acquisition  # Hide sensitive cost data from non-admins\n        - internal_notes\n```\n\n## Advanced Features\n\n### Annotations\n\nAnnotations overlay contextual information (like events or milestones) on time-series charts:\n\n```yaml\nannotations:\n  - name: product_launches\n    model: product_launches  # Must have 'time' and 'description' columns\n    measures:\n      - total_revenue        # Only show on these measures\n\n  # Optional columns in annotation model:\n  # - time_end: For range annotations\n  # - grain: Show only at specific time grains (day, week, etc.)\n```\n\n### Unnest for array dimensions\n\nWhen a column contains arrays, use `unnest: true` to flatten it at query time:\n\n```yaml\ndimensions:\n  - name: tags\n    display_name: Tags\n    column: tags\n    unnest: true  # Allows filtering by individual array elements\n```\n\n### Cache configuration\n\nConfigure caching for slow metrics views that use external tables:\n\n```yaml\ncache:\n  enabled: true\n  key_ttl: 5m\n  key_sql: SELECT MAX(updated_at) FROM orders\n```\n\nYou should not add a `cache:` config when the metrics view references a model inside the project since Rill does automatic cache management in that case.\n\n## Dialect-Specific Notes\n\nSQL expressions in dimensions and measures use the underlying OLAP database's dialect.\n\n### DuckDB\n\nDuckDB is the default OLAP engine for local development.\n\n**Conditional aggregation with FILTER**:\n```yaml\n# DuckDB supports FILTER clause for conditional aggregation\nexpression: COUNT(*) FILTER (WHERE status = 'completed')\n```\n\n### ClickHouse\n\nClickHouse is recommended for production workloads with large datasets.\n\n**Conditional aggregation**:\n```yaml\n# ClickHouse uses IF or CASE inside aggregations\nexpression: countIf(status = 'completed')\nexpression: sumIf(revenue, status = 'completed')\n```\n\n**Date functions**:\n```yaml\nexpression: toYear(order_date)\nexpression: toStartOfMonth(order_date)\nexpression: toYYYYMMDD(order_date)\n```\n\n**Array functions**:\n```yaml\nexpression: arrayJoin(tags)  # Unnest arrays\n```\n\n### Druid\n\n**Approximate distinct counts**:\n```yaml\nexpression: APPROX_COUNT_DISTINCT_DS_HLL(user_id)\n```\n\n## JSON Schema\n\n```\nallOf:\n    - properties:\n        ai_instructions:\n            description: Extra instructions for [AI agents](/explore/mcp). Used to guide natural language question answering and routing.\n            type: string\n        annotations:\n            description: Used to define annotations that can be displayed on charts\n            items:\n                properties:\n                    connector:\n                        description: Refers to the connector to use for the annotation\n                        type: string\n                    database:\n                        description: Refers to the database to use in the OLAP engine (to be used in conjunction with table). Otherwise, will use the default database or schema if not specified\n                        type: string\n                    database_schema:\n                        description: Refers to the schema to use in the OLAP engine (to be used in conjunction with table). Otherwise, will use the default database or schema if not specified\n                        type: string\n                    measures:\n                        anyOf:\n                            - description: Simple field name as a string.\n                              type: string\n                            - description: List of field selectors, each can be a string or an object with detailed configuration.\n                              items:\n                                anyOf:\n                                    - description: Shorthand field selector, interpreted as the name.\n                                      type: string\n                                    - additionalProperties: false\n                                      description: Detailed field selector configuration with name and optional time grain.\n                                      properties:\n                                        name:\n                                            description: Name of the field to select.\n                                            type: string\n                                        time_grain:\n                                            description: Time grain for time-based dimensions.\n                                            enum:\n                                                - \"\"\n                                                - ms\n                                                - millisecond\n                                                - s\n                                                - second\n                                                - min\n                                                - minute\n                                                - hadditionalProperties: fal\n                                                - hour\n                                                - d\n                                                - day\n                                                - w\n                                                - week\n                                                - month\n                                                - q\n                                                - quarter\n                                                - \"y\"\n                                                - year\n                                            type: string\n                                      required:\n                                        - name\n                                      type: object\n                              type: array\n                        description: Specifies which measures to apply the annotation to. Applies to all measures if not specified\n                    model:\n                        description: Refers to the model powering the annotation (either table or model is required). The model must have 'time' and 'description' columns. Optional columns include 'time_end' for range annotations and 'grain' to specify when the annotation should appear based on dashboard grain level.\n                        type: string\n                    name:\n                        description: A stable identifier for the annotation. Defaults to model or table names when not specified\n                        type: string\n                    table:\n                        description: Refers to the table powering the annotation, should be used instead of model for annotations from external OLAP tables (either table or model is required)\n                        type: string\n                type: object\n            type: array\n        connector:\n            description: Refers to the connector type for the metrics view, see [OLAP engines](/developers/build/connectors/olap) for more information\n            type: string\n        database:\n            description: Refers to the database to use in the OLAP engine (to be used in conjunction with table). Otherwise, will use the default database or schema if not specified\n            type: string\n        database_schema:\n            description: Refers to the schema to use in the OLAP engine (to be used in conjunction with table). Otherwise, will use the default database or schema if not specified\n            type: string\n        description:\n            description: Refers to the description for the metrics view\n            type: string\n        dimensions:\n            description: Relates to exploring segments or dimensions of your data and filtering the dashboard\n            items:\n                anyOf:\n                    - required:\n                        - column\n                    - required:\n                        - expression\n                properties:\n                    column:\n                        description: a categorical column\n                        type: string\n                    description:\n                        description: a freeform text description of the dimension\n                        type: string\n                    display_name:\n                        description: a display name for your dimension\n                        type: string\n                    expression:\n                        description: a non-aggregate expression such as string_split(domain, '.'). One of column and expression is required but cannot have both at the same time\n                        type: string\n                    name:\n                        description: a stable identifier for the dimension\n                        type: string\n                    tags:\n                        description: optional list of tags for categorizing the dimension (defaults to empty)\n                        items:\n                            type: string\n                        type: array\n                    type:\n                        description: 'Dimension type: \"geo\" for geospatial dimensions, \"time\" for time dimensions or \"categorical\" for categorial dimensions. Default is undefined and the type will be inferred instead'\n                        type: string\n                    unnest:\n                        description: if true, allows multi-valued dimension to be unnested (such as lists) and filters will automatically switch to \"contains\" instead of exact match\n                        type: boolean\n                    uri:\n                        description: enable if your dimension is a clickable URL to enable single click navigation (boolean or valid SQL expression)\n                        type:\n                            - string\n                            - boolean\n                type: object\n            type: array\n        display_name:\n            description: Refers to the display name for the metrics view\n            type: string\n        explore:\n            $ref: '#/definitions/explore_properties'\n            description: Defines an optional inline explore view for the metrics view. If not specified a default explore will be emitted unless `skip` is set to true.\n            required:\n                - type\n        first_day_of_week:\n            description: Refers to the first day of the week for time grain aggregation (for example, Sunday instead of Monday). The valid values are 1 through 7 where Monday=1 and Sunday=7\n            type: integer\n        first_month_of_year:\n            description: Refers to the first month of the year for time grain aggregation. The valid values are 1 through 12 where January=1 and December=12\n            type: integer\n        measures:\n            description: Used to define the numeric aggregates of columns from your data model\n            items:\n                properties:\n                    description:\n                        description: a freeform text description of the measure\n                        type: string\n                    display_name:\n                        description: the display name of your measure.\n                        type: string\n                    expression:\n                        description: a combination of operators and functions for aggregations\n                        type: string\n                    format_d3:\n                        description: 'Controls the formatting of this measure using a [d3-format](https://d3js.org/d3-format) string. If an invalid format string is supplied, the measure will fall back to `format_preset: humanize`. A measure cannot have both `format_preset` and `format_d3`. If neither is provided, the humanize preset is used by default. Example: `format_d3: \".2f\"` formats using fixed-point notation with two decimal places. Example: `format_d3: \",.2r\"` formats using grouped thousands with two significant digits. (optional)'\n                        type: string\n                    format_d3_locale:\n                        description: |\n                            locale configuration passed through to D3, enabling changing the currency symbol among other things. For details, see the docs for D3's formatLocale.\n                              ```yaml\n                              format_d3: \"$,\"\n                              format_d3_locale:\n                                grouping: [3, 2]\n                                currency: [\"\", \"\"]\n                              ```\n                        properties:\n                            currency:\n                                description: the currency symbol\n                                type: array\n                            grouping:\n                                description: the grouping of the currency symbol\n                                type: array\n                        type: object\n                    format_preset:\n                        description: |\n                            Controls the formatting of this measure using a predefined preset. Measures cannot have both `format_preset` and `format_d3`. If neither is supplied, the measure will be formatted using the `humanize` preset by default.\n\n                              Available options:\n                              - `humanize`: Round numbers into thousands (K), millions(M), billions (B), etc.\n                              - `none`: Raw output.\n                              - `currency_usd`: Round to 2 decimal points with a dollar sign ($).\n                              - `currency_eur`: Round to 2 decimal points with a euro sign ().\n                              - `percentage`: Convert a rate into a percentage with a % sign.\n                              - `interval_ms`: Convert milliseconds into human-readable durations like hours (h), days (d), years (y), etc. (optional)\n                        type: string\n                    label:\n                        description: a label for your measure, deprecated use display_name\n                        type: string\n                    name:\n                        description: a stable identifier for the measure\n                        type: string\n                    per:\n                        $ref: '#/definitions/field_selectors_properties'\n                        description: for per dimensions\n                    requires:\n                        $ref: '#/definitions/field_selectors_properties'\n                        description: using an available measure or dimension in your metrics view to set a required parameter, cannot be used with simple measures. See [referencing measures](/developers/build/metrics-view/measures/referencing) for more information.\n                    tags:\n                        description: optional list of tags for categorizing the measure (defaults to empty)\n                        items:\n                            type: string\n                        type: array\n                    treat_nulls_as:\n                        description: used to configure what value to fill in for missing time buckets. This also works generally as COALESCING over non empty time buckets.\n                        type: string\n                    type:\n                        description: 'Measure calculation type: \"simple\" for basic aggregations, \"derived\" for calculations using other measures, or \"time_comparison\" for period-over-period analysis. Defaults to \"simple\" unless dependencies exist.'\n                        type: string\n                    valid_percent_of_total:\n                        description: a boolean indicating whether percent-of-total values should be rendered for this measure\n                        type: boolean\n                    window:\n                        anyOf:\n                            - description: 'Shorthand: `time` or `true` means time-partitioned, `all` means non-partitioned.'\n                              enum:\n                                - time\n                                - \"true\"\n                                - all\n                              type: string\n                            - additionalProperties: false\n                              description: Detailed window configuration for measure calculations, allowing control over partitioning, ordering, and frame definition.\n                              properties:\n                                frame:\n                                    description: Defines the window frame boundaries for calculations, specifying which rows are included in the window relative to the current row.\n                                    type: string\n                                order:\n                                    $ref: '#/definitions/field_selectors_properties'\n                                    description: Specifies the fields to order the window by, determining the sequence of rows within each partition.\n                                    type: string\n                                partition:\n                                    description: Controls whether the window is partitioned. When true, calculations are performed within each partition separately.\n                                    type: boolean\n                              type: object\n                        description: A measure window can be defined as a keyword string (e.g. 'time' or 'all') or an object with detailed window configuration. For more information, see the [window functions](/developers/build/metrics-view/measures/windows) documentation.\n                required:\n                    - name\n                    - display_name\n                    - expression\n                type: object\n            type: array\n        model:\n            description: Refers to the model powering the dashboard (either model or table is required)\n            type: string\n        parent:\n            description: Refers to the parent metrics view from which this metrics view is derived. If specified, this will inherit properties from the parent metrics view\n            type: string\n        parent_dimensions:\n            $ref: '#/definitions/field_selector_properties'\n            description: Optional field selectors for dimensions to inherit from the parent metrics view.\n        parent_measures:\n            $ref: '#/definitions/field_selector_properties'\n            description: Optional field selectors for measures to inherit from the parent metrics view.\n        security:\n            $ref: '#/definitions/security_policy_properties'\n            description: Defines a security policy for the dashboard\n        smallest_time_grain:\n            description: 'Refers to the smallest time granularity the user is allowed to view. The valid values are: millisecond, second, minute, hour, day, week, month, quarter, year'\n            type: string\n        table:\n            description: Refers to the table powering the dashboard, should be used instead of model for dashboards create from external OLAP tables (either table or model is required)\n            type: string\n        timeseries:\n            description: Refers to the timestamp column from your model that will underlie x-axis data in the line charts. If not specified, the line charts will not appear\n            type: string\n        type:\n            const: metrics_view\n            description: Refers to the resource type and must be `metrics_view`\n            type: string\n        version:\n            description: The version of the metrics view schema\n            type: string\n        watermark:\n            description: A SQL expression that tells us the max timestamp that the measures are considered valid for. Usually does not need to be overwritten\n            type: string\n      title: Properties\n      type: object\n    - $ref: '#/definitions/common_properties'\ndescription: In your Rill project directory, create a metrics view, `<metrics_view>.yaml`, file in the `metrics` directory. Rill will ingest the metric view definition next time you run `rill start`.\nid: metrics-views\ntitle: Metrics View YAML\ntype: object\n```"
    - role: user
      content_type: text
      content: |-
        You should develop a Rill project file based on the following task description: Update the metrics view for the covid table to resolve the validation error: rename each measure so it does not match the underlying column name. Use the following naming convention: for sum aggregations, use the suffix _sum (e.g., new_confirmed_sum); for max aggregations, use the suffix _max (e.g., cumulative_confirmed_max). Ensure all other configuration remains unchanged.

        Here is some important context:
        - You should develop the file at path: /metrics/covid.yaml
        - The file should be of type: metrics_view
        - You are running as a sub-agent of a larger developer agent. Stay aligned on your specific task, avoid extra discovery, and report back your success or failure to the parent agent.

        Here is some additional context that may or may not be relevant to your task:
        - The project's default OLAP connector is named clickhouse (driver: clickhouse).
    - role: assistant
      content_type: tool_call
      id: f8898f24b1814422
      tool_name: read_file
      content: '{"path":"/metrics/covid.yaml"}'
    - role: tool
      content_type: tool_response
      id: f8898f24b1814422
      content: '{"Contents":"type: metrics_view\nmodel: covid\nconnector: clickhouse\ndatabase: default\ndimensions:\n  - name: date\n    column: date\n    display_name: Date\n    description: Date of the record\n  - name: location_key\n    column: location_key\n    display_name: Location Key\n    description: Location identifier\nmeasures:\n  - name: new_confirmed\n    display_name: New Confirmed\n    expression: SUM(new_confirmed)\n    format_preset: humanize\n    description: Sum of new confirmed cases\n  - name: new_deceased\n    display_name: New Deceased\n    expression: SUM(new_deceased)\n    format_preset: humanize\n    description: Sum of new deceased cases\n  - name: new_recovered\n    display_name: New Recovered\n    expression: SUM(new_recovered)\n    format_preset: humanize\n    description: Sum of new recovered cases\n  - name: new_tested\n    display_name: New Tested\n    expression: SUM(new_tested)\n    format_preset: humanize\n    description: Sum of new tested cases\n  - name: cumulative_confirmed\n    display_name: Cumulative Confirmed\n    expression: MAX(cumulative_confirmed)\n    format_preset: humanize\n    description: Maximum cumulative confirmed cases\n  - name: cumulative_deceased\n    display_name: Cumulative Deceased\n    expression: MAX(cumulative_deceased)\n    format_preset: humanize\n    description: Maximum cumulative deceased cases\n  - name: cumulative_recovered\n    display_name: Cumulative Recovered\n    expression: MAX(cumulative_recovered)\n    format_preset: humanize\n    description: Maximum cumulative recovered cases\n  - name: cumulative_tested\n    display_name: Cumulative Tested\n    expression: MAX(cumulative_tested)\n    format_preset: humanize\n    description: Maximum cumulative tested cases\ntimeseries: date\n"}'
    - role: assistant
      content_type: tool_call
      id: ec9c74f481e24733
      tool_name: write_file
      content: '{"contents":"type: metrics_view\nmodel: covid\nconnector: clickhouse\ndatabase: default\ndimensions:\n  - name: date\n    column: date\n    display_name: Date\n    description: Date of the record\n  - name: location_key\n    column: location_key\n    display_name: Location Key\n    description: Location identifier\nmeasures:\n  - name: new_confirmed_sum\n    display_name: New Confirmed\n    expression: SUM(new_confirmed)\n    format_preset: humanize\n    description: Sum of new confirmed cases\n  - name: new_deceased_sum\n    display_name: New Deceased\n    expression: SUM(new_deceased)\n    format_preset: humanize\n    description: Sum of new deceased cases\n  - name: new_recovered_sum\n    display_name: New Recovered\n    expression: SUM(new_recovered)\n    format_preset: humanize\n    description: Sum of new recovered cases\n  - name: new_tested_sum\n    display_name: New Tested\n    expression: SUM(new_tested)\n    format_preset: humanize\n    description: Sum of new tested cases\n  - name: cumulative_confirmed_max\n    display_name: Cumulative Confirmed\n    expression: MAX(cumulative_confirmed)\n    format_preset: humanize\n    description: Maximum cumulative confirmed cases\n  - name: cumulative_deceased_max\n    display_name: Cumulative Deceased\n    expression: MAX(cumulative_deceased)\n    format_preset: humanize\n    description: Maximum cumulative deceased cases\n  - name: cumulative_recovered_max\n    display_name: Cumulative Recovered\n    expression: MAX(cumulative_recovered)\n    format_preset: humanize\n    description: Maximum cumulative recovered cases\n  - name: cumulative_tested_max\n    display_name: Cumulative Tested\n    expression: MAX(cumulative_tested)\n    format_preset: humanize\n    description: Maximum cumulative tested cases\ntimeseries: date\n","path":"/metrics/covid.yaml"}'
    - role: tool
      content_type: tool_response
      id: ec9c74f481e24733
      content: '{"diff":"--- /metrics/covid.yaml\n+++ /metrics/covid.yaml\n@@ -12,42 +12,42 @@\n     display_name: Location Key\n     description: Location identifier\n measures:\n-  - name: new_confirmed\n+  - name: new_confirmed_sum\n     display_name: New Confirmed\n     expression: SUM(new_confirmed)\n     format_preset: humanize\n     description: Sum of new confirmed cases\n-  - name: new_deceased\n+  - name: new_deceased_sum\n     display_name: New Deceased\n     expression: SUM(new_deceased)\n     format_preset: humanize\n     description: Sum of new deceased cases\n-  - name: new_recovered\n+  - name: new_recovered_sum\n     display_name: New Recovered\n     expression: SUM(new_recovered)\n     format_preset: humanize\n     description: Sum of new recovered cases\n-  - name: new_tested\n+  - name: new_tested_sum\n     display_name: New Tested\n     expression: SUM(new_tested)\n     format_preset: humanize\n     description: Sum of new tested cases\n-  - name: cumulative_confirmed\n+  - name: cumulative_confirmed_max\n     display_name: Cumulative Confirmed\n     expression: MAX(cumulative_confirmed)\n     format_preset: humanize\n     description: Maximum cumulative confirmed cases\n-  - name: cumulative_deceased\n+  - name: cumulative_deceased_max\n     display_name: Cumulative Deceased\n     expression: MAX(cumulative_deceased)\n     format_preset: humanize\n     description: Maximum cumulative deceased cases\n-  - name: cumulative_recovered\n+  - name: cumulative_recovered_max\n     display_name: Cumulative Recovered\n     expression: MAX(cumulative_recovered)\n     format_preset: humanize\n     description: Maximum cumulative recovered cases\n-  - name: cumulative_tested\n+  - name: cumulative_tested_max\n     display_name: Cumulative Tested\n     expression: MAX(cumulative_tested)\n     format_preset: humanize\n","resources":[{"kind":"rill.runtime.v1.MetricsView","name":"covid","reconcile_error":"","reconcile_status":"RECONCILE_STATUS_IDLE"},{"kind":"rill.runtime.v1.Explore","name":"covid","reconcile_error":"","reconcile_status":"RECONCILE_STATUS_IDLE"}]}'
  response:
    - role: assistant
      content_type: text
      content: |-
        The metrics view at /metrics/covid.yaml has been updated to resolve the validation error by renaming each measure so it does not match the underlying column name. The following naming convention was applied:

        - For SUM aggregations: suffix _sum (e.g., new_confirmed_sum)
        - For MAX aggregations: suffix _max (e.g., cumulative_confirmed_max)

        All other configuration remains unchanged. The file reconciled successfully with no errors. Task complete.
- index: 21
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. Non-materialized models become views, which re-execute on every query.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: user
      content_type: text
      content: |-
        This may not relate to the user's task, but for context, the project's default OLAP connector is named duckdb (driver: duckdb).
         The default OLAP is in readwrite mode, so you can use it in models if you want.


        Task: Clickhouse has a public playground database on play.clickhouse.com:9440 (username `play` and no password). Can you set it up as the OLAP connector and develop a dashboard based on one of its tables? Please proceed without asking clarifying questions.
    - role: assistant
      content_type: tool_call
      id: 5f915383ab074f81
      tool_name: develop_file
      content: '{"path":"connectors/clickhouse.yaml","prompt":"Create a connector for the public ClickHouse playground database at play.clickhouse.com:9440. Use the ''clickhouse'' driver. Set the username to ''play'' and leave the password blank. This connector will be used as an OLAP connector for dashboards. Set ''mode'' to ''read'' to prevent writes. Name the connector ''clickhouse''.","type":"connector"}'
    - role: tool
      content_type: tool_response
      id: 5f915383ab074f81
      content: '{"response":"The connector file /connectors/clickhouse.yaml was created as specified:\n\n- type: connector\n- driver: clickhouse\n- host: play.clickhouse.com\n- port: 9440\n- username: play\n- password: \"\" (blank)\n- mode: read\n\nThe file was written successfully. However, there is a reconcile error indicating a handshake failure, likely due to an SSL/TLS mismatch. If SSL is required for this endpoint, you may need to add ssl: true to the configuration. Please advise if you want to update the file to include SSL."}'
    - role: assistant
      content_type: tool_call
      id: 3925a34a97214026
      tool_name: develop_file
      content: '{"path":"connectors/clickhouse.yaml","prompt":"Update the ClickHouse connector configuration to include SSL support, as the public playground likely requires a secure connection. Add ''ssl: true'' to the YAML file. Ensure all other settings remain the same: driver clickhouse, host play.clickhouse.com, port 9440, username play, blank password, mode read.","type":"connector"}'
    - role: tool
      content_type: tool_response
      id: 3925a34a97214026
      content: '{"response":"Success: The ClickHouse connector configuration at /connectors/clickhouse.yaml has been updated to include ssl: true, as required for secure connections to the public playground. All other settings (driver, host, port, username, blank password, and mode) remain unchanged. No errors were encountered during the update."}'
    - role: assistant
      content_type: tool_call
      id: 83c850aa1fea4507
      tool_name: develop_file
      content: '{"path":"rill.yaml","prompt":"Update the project configuration to set the default OLAP connector to ''clickhouse'' instead of ''duckdb''. Ensure that the ''olap_connector'' property is set to ''clickhouse''. Retain any other existing configuration in rill.yaml.","type":"rill.yaml"}'
    - role: tool
      content_type: tool_response
      id: 83c850aa1fea4507
      content: '{"response":"The rill.yaml file has been updated to set olap_connector: clickhouse as the default OLAP connector, as requested. There was no other existing configuration to retain. Task complete."}'
    - role: assistant
      content_type: tool_call
      id: a902bdef4d944e31
      tool_name: list_tables
      content: '{"connector":"clickhouse","page_size":20}'
    - role: tool
      content_type: tool_response
      id: a902bdef4d944e31
      content: '{"tables":[{"database_schema":"blogs","name":"countries","is_view":false},{"database_schema":"blogs","name":"country_codes","is_view":false},{"database_schema":"blogs","name":"forex","is_view":false},{"database_schema":"blogs","name":"forex_2020s","is_view":false},{"database_schema":"blogs","name":"forex_usd","is_view":false},{"database_schema":"blogs","name":"noaa","is_view":false},{"database_schema":"blogs","name":"noaa_v2","is_view":false},{"database_schema":"blogs","name":"stop_words","is_view":false},{"database_schema":"blogs","name":"uk_codes","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"actors","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"benchmark_results","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"benchmark_runs","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"cell_towers","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"checks","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"checks_coverage_inverted","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"checks_v2","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"cisco_umbrella","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"covid","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"dish","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"dns","is_view":false}],"next_page_token":"ImRlZmF1bHQiCiJkbnMiCg"}'
    - role: assistant
      content_type: tool_call
      id: 46401ed1773746ab
      tool_name: show_table
      content: '{"connector":"clickhouse","database_schema":"default","table":"covid"}'
    - role: tool
      content_type: tool_response
      id: 46401ed1773746ab
      content: '{"name":"covid","is_view":false,"columns":[{"name":"date","type":"CODE_DATE"},{"name":"location_key","type":"CODE_STRING"},{"name":"new_confirmed","type":"CODE_INT32"},{"name":"new_deceased","type":"CODE_INT32"},{"name":"new_recovered","type":"CODE_INT32"},{"name":"new_tested","type":"CODE_INT32"},{"name":"cumulative_confirmed","type":"CODE_INT32"},{"name":"cumulative_deceased","type":"CODE_INT32"},{"name":"cumulative_recovered","type":"CODE_INT32"},{"name":"cumulative_tested","type":"CODE_INT32"}],"physical_size_bytes":-1}'
    - role: assistant
      content_type: tool_call
      id: 850ac6b8ec0b4f1b
      tool_name: develop_file
      content: '{"path":"metrics/covid.yaml","prompt":"Create a metrics view for the covid table in the ClickHouse playground (schema: default, table: covid). Use the clickhouse connector. Define the following dimensions: date (date), location_key (string). Define the following measures: new_confirmed (sum), new_deceased (sum), new_recovered (sum), new_tested (sum), cumulative_confirmed (max), cumulative_deceased (max), cumulative_recovered (max), cumulative_tested (max). Name the metrics view ''covid''.","type":"metrics_view"}'
    - role: tool
      content_type: tool_response
      id: 850ac6b8ec0b4f1b
      content: '{"response":"The metrics view file /metrics/covid.yaml was created as specified, targeting the covid table in the ClickHouse playground (schema: default, table: covid) using the clickhouse connector. It defines the requested dimensions (date, location_key) and measures (new_confirmed, new_deceased, new_recovered, new_tested, cumulative_confirmed, cumulative_deceased, cumulative_recovered, cumulative_tested).\n\nHowever, ClickHouse-backed metrics views in Rill do not allow measures to have the same name as columns in the underlying table. All the requested measure names match column names, resulting in a validation error for each measure.\n\nTo resolve this, you must rename each measure to something different from the corresponding column name (for example, prefix with total_ or use a suffix like _sum or _max). Please confirm if you want to proceed with renaming, and specify your preferred naming convention if any."}'
    - role: assistant
      content_type: tool_call
      id: 7be75a411bee47e9
      tool_name: develop_file
      content: '{"path":"metrics/covid.yaml","prompt":"Update the metrics view for the covid table to resolve the validation error: rename each measure so it does not match the underlying column name. Use the following naming convention: for sum aggregations, use the suffix _sum (e.g., new_confirmed_sum); for max aggregations, use the suffix _max (e.g., cumulative_confirmed_max). Ensure all other configuration remains unchanged.","type":"metrics_view"}'
    - role: tool
      content_type: tool_response
      id: 7be75a411bee47e9
      content: '{"response":"The metrics view at /metrics/covid.yaml has been updated to resolve the validation error by renaming each measure so it does not match the underlying column name. The following naming convention was applied:\n\n- For SUM aggregations: suffix _sum (e.g., new_confirmed_sum)\n- For MAX aggregations: suffix _max (e.g., cumulative_confirmed_max)\n\nAll other configuration remains unchanged. The file reconciled successfully with no errors. Task complete."}'
  response:
    - role: assistant
      content_type: tool_call
      id: call_7xjX4hdsvhePnRcSAFyCeCaZ
      tool_name: develop_file
      content: '{"path":"dashboards/covid.yaml","prompt":"Create an explore dashboard for the covid metrics view. Reference the metrics view named ''covid''. Use default settings for all other properties. This will provide a drill-down dashboard for covid data from the ClickHouse playground.","type":"explore"}'
- index: 22
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. Non-materialized models become views, which re-execute on every query.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: system
      content_type: text
      content: |-
        # Instructions for developing an explore dashboard in Rill

        ## Introduction

        Explore dashboards are resources that configure an interactive, drill-down dashboard for a metrics view. They are Rill's default dashboard type, designed for explorative slice-and-dice analysis of a single metrics view.

        Explore dashboards are lightweight resources that sit downstream of a metrics view in the project DAG. Their reconcile logic is fast (validation only), so they can be created and modified freely without performance concerns.

        ### When to use explores vs canvases

        - **Explore dashboards:** Best for explorative analysis, drill-down investigations, and letting users freely slice data by any dimension.
        - **Canvas dashboards:** Best for fixed reports, executive summaries, or combining multiple metrics views into a single view.

        ## Development approach

        Explore dashboards require minimal configuration. In most cases, you only need to:

        1. Reference the metrics view
        2. Select which dimensions and measures to expose (usually all, indicated by `'*'`)
        3. Optionally configure defaults and time ranges

        **Best practice:** Keep explore configurations simple. Only add advanced features (security policies, custom themes, restricted dimensions) when there is a clear requirement. The metrics view already defines the business logic; the explore just controls presentation and access.

        ## Inline explores in metrics views

        Metrics views create an explore resource by default with the same name as the metrics view. For legacy reasons, this does not happen for metrics views containing `version: 1`. You can customize a metrics view's explore with the `explore:` property inside the metrics view file:

        ```yaml
        # metrics/sales.yaml
        type: metrics_view
        display_name: Sales Analytics

        model: sales_model
        timeseries: order_date

        dimensions:
          - column: region
          - column: product_category

        measures:
          - name: total_revenue
            expression: SUM(revenue)

        # Inline explore configuration (optional)
        explore:
          time_ranges:
            - P7D
            - P30D
            - P90D
          defaults:
            time_range: P30D
        ```

        Use inline explores for simple cases where you want to keep the metrics view and its dashboard configuration together. Use separate explore files when you need multiple explores for the same metrics view or more complex configurations.

        ## Example with annotations

        Note that most explore dashboards work great without any of the optional properties shown here.

        ```yaml
        # dashboards/sales_explore.yaml

        # Required: resource type
        type: explore

        # Required: the metrics view this dashboard renders
        metrics_view: sales_metrics

        # Optional: display name shown in the navigation and header
        display_name: "Sales Performance"

        # Optional: informational banner at the top of the dashboard
        banner: "Data refreshes daily at 6 AM UTC"

        # Optional: which dimensions to expose (use '*' for all)
        dimensions: '*'

        # Optional: which measures to expose (use '*' for all)
        measures: '*'

        # Optional: customize the time range dropdown
        time_ranges:
          - P7D
          - P30D
          - P90D
          - P12M
          - rill-MTD
          - rill-YTD

        # Optional: default dashboard state on first load
        defaults:
          time_range: P30D
          comparison_mode: time

        # Optional: pin specific time zones to the top of the selector
        time_zones:
          - America/Los_Angeles
          - America/New_York

        # Optional: custom theme
        theme: my_theme

        # Optional: restrict access to specific users.
        # Note: usually you should do this in the metrics view, not the explore resource.
        security:
          access: "{{ .user.admin }} OR '{{ .user.email }}' LIKE '%@example.com'"
        ```

        ## Minimal example

        For most use cases, a minimal explore is sufficient:

        ```yaml
        type: explore
        metrics_view: sales_metrics
        display_name: "Sales Dashboard"
        dimensions: '*'
        measures: '*'
        ```

        ## JSON Schema

        ```
        allOf:
            - properties:
                allow_custom_time_range:
                    description: Defaults to true, when set to false it will hide the ability to set a custom time range for the user.
                    type: boolean
                banner:
                    description: Refers to the custom banner displayed at the header of an explore dashboard
                    type: string
                defaults:
                    additionalProperties: false
                    description: |
                        defines the defaults YAML struct
                          ```yaml
                          defaults: #define all the defaults within here
                            dimensions:
                              - dim_1
                              - dim_2
                            measures:
                              - measure_1
                              - measure_2
                            time_range: P1M
                            comparison_mode: dimension #time, none
                            comparison_dimension: filename
                          ```
                    properties:
                        comparison_dimension:
                            description: for dimension mode, specify the comparison dimension by name
                            type: string
                        comparison_mode:
                            description: 'Controls how to compare current data with historical or categorical baselines. Options: `none` (no comparison), `time` (compares with past based on default_time_range), `dimension` (compares based on comparison_dimension values)'
                            enum:
                                - none
                                - time
                                - dimension
                            type: string
                        dimensions:
                            $ref: '#/definitions/field_selector_properties'
                            description: Provides the default dimensions to load on viewing the dashboard
                        measures:
                            $ref: '#/definitions/field_selector_properties'
                            description: Provides the default measures to load on viewing the dashboard
                        time_range:
                            description: Refers to the default time range shown when a user initially loads the dashboard. The value must be either a valid [ISO 8601 duration](https://en.wikipedia.org/wiki/ISO_8601#Durations) (for example, PT12H for 12 hours, P1M for 1 month, or P26W for 26 weeks) or one of the [Rill ISO 8601 extensions](https://docs.rilldata.com/reference/rill-iso-extensions#extensions)
                            type: string
                    type: object
                description:
                    description: Refers to the description of the explore dashboard
                    type: string
                dimensions:
                    $ref: '#/definitions/field_selector_properties'
                    description: List of dimension names. Use '*' to select all dimensions (default)
                    examples:
                        - dimensions:
                            - country
                        - dimensions:
                            exclude:
                                - country
                        - dimensions:
                            expr: ^public_.*$
                display_name:
                    description: Refers to the display name for the explore dashboard
                    type: string
                embeds:
                    additionalProperties: false
                    description: Configuration options for embedded dashboard views
                    properties:
                        hide_pivot:
                            description: When true, hides the pivot table view in embedded mode
                            type: boolean
                    type: object
                lock_time_zone:
                    description: When true, the dashboard will be locked to the first time provided in the time_zones list. When no time_zones are provided, the dashboard will be locked to UTC
                    type: boolean
                measures:
                    $ref: '#/definitions/field_selector_properties'
                    description: List of measure names. Use '*' to select all measures (default)
                    examples:
                        - measures:
                            - sum_of_total
                        - measures:
                            exclude:
                                - sum_of_total
                        - measures:
                            expr: ^public_.*$
                metrics_view:
                    description: Refers to the metrics view resource
                    type: string
                security:
                    $ref: '#/definitions/dashboard_security_policy_properties'
                    description: Security rules to apply for access to the explore dashboard
                theme:
                    description: Name of the theme to use. Only one of theme and embedded_theme can be set.
                    oneOf:
                        - description: Name of an existing theme to apply to the dashboard
                          type: string
                        - $ref: '#/definitions/theme_properties'
                          description: Inline theme configuration.
                time_ranges:
                    description: |
                        Overrides the list of default time range selections available in the dropdown. It can be string or an object with a 'range' and optional 'comparison_offsets'
                          ```yaml
                          time_ranges:
                            - PT15M // Simplified syntax to specify only the range
                            - PT1H
                            - PT6H
                            - P7D
                            - range: P5D // Advanced syntax to specify comparison_offsets as well
                            - P4W
                            - rill-TD // Today
                            - rill-WTD // Week-To-date
                          ```
                    items:
                        $ref: '#/definitions/explore_time_range_properties'
                    type: array
                time_zones:
                    description: Refers to the time zones that should be pinned to the top of the time zone selector. It should be a list of [IANA time zone identifiers](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones)
                    items:
                        type: string
                    type: array
                type:
                    const: explore
                    description: Refers to the resource type and must be `explore`
                    type: string
              required:
                - type
                - display_name
                - metrics_view
              title: Properties
              type: object
            - $ref: '#/definitions/common_properties'
        description: Explore dashboards provide an interactive way to explore data with predefined measures and dimensions.
        id: explore-dashboards
        title: Explore Dashboard YAML
        type: object
        ```
    - role: user
      content_type: text
      content: |-
        You should develop a Rill project file based on the following task description: Create an explore dashboard for the covid metrics view. Reference the metrics view named 'covid'. Use default settings for all other properties. This will provide a drill-down dashboard for covid data from the ClickHouse playground.

        Here is some important context:
        - You should develop the file at path: /dashboards/covid.yaml
        - The file should be of type: explore
        - You are running as a sub-agent of a larger developer agent. Stay aligned on your specific task, avoid extra discovery, and report back your success or failure to the parent agent.

        Here is some additional context that may or may not be relevant to your task:
        - The project's default OLAP connector is named clickhouse (driver: clickhouse).
  response:
    - role: assistant
      content_type: tool_call
      id: call_lrBrIZUtdQtb4twDx2PpESq4
      tool_name: write_file
      content: '{"contents":"type: explore\nmetrics_view: covid\n","path":"/dashboards/covid.yaml"}'
- index: 23
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. Non-materialized models become views, which re-execute on every query.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: system
      content_type: text
      content: |-
        # Instructions for developing an explore dashboard in Rill

        ## Introduction

        Explore dashboards are resources that configure an interactive, drill-down dashboard for a metrics view. They are Rill's default dashboard type, designed for explorative slice-and-dice analysis of a single metrics view.

        Explore dashboards are lightweight resources that sit downstream of a metrics view in the project DAG. Their reconcile logic is fast (validation only), so they can be created and modified freely without performance concerns.

        ### When to use explores vs canvases

        - **Explore dashboards:** Best for explorative analysis, drill-down investigations, and letting users freely slice data by any dimension.
        - **Canvas dashboards:** Best for fixed reports, executive summaries, or combining multiple metrics views into a single view.

        ## Development approach

        Explore dashboards require minimal configuration. In most cases, you only need to:

        1. Reference the metrics view
        2. Select which dimensions and measures to expose (usually all, indicated by `'*'`)
        3. Optionally configure defaults and time ranges

        **Best practice:** Keep explore configurations simple. Only add advanced features (security policies, custom themes, restricted dimensions) when there is a clear requirement. The metrics view already defines the business logic; the explore just controls presentation and access.

        ## Inline explores in metrics views

        Metrics views create an explore resource by default with the same name as the metrics view. For legacy reasons, this does not happen for metrics views containing `version: 1`. You can customize a metrics view's explore with the `explore:` property inside the metrics view file:

        ```yaml
        # metrics/sales.yaml
        type: metrics_view
        display_name: Sales Analytics

        model: sales_model
        timeseries: order_date

        dimensions:
          - column: region
          - column: product_category

        measures:
          - name: total_revenue
            expression: SUM(revenue)

        # Inline explore configuration (optional)
        explore:
          time_ranges:
            - P7D
            - P30D
            - P90D
          defaults:
            time_range: P30D
        ```

        Use inline explores for simple cases where you want to keep the metrics view and its dashboard configuration together. Use separate explore files when you need multiple explores for the same metrics view or more complex configurations.

        ## Example with annotations

        Note that most explore dashboards work great without any of the optional properties shown here.

        ```yaml
        # dashboards/sales_explore.yaml

        # Required: resource type
        type: explore

        # Required: the metrics view this dashboard renders
        metrics_view: sales_metrics

        # Optional: display name shown in the navigation and header
        display_name: "Sales Performance"

        # Optional: informational banner at the top of the dashboard
        banner: "Data refreshes daily at 6 AM UTC"

        # Optional: which dimensions to expose (use '*' for all)
        dimensions: '*'

        # Optional: which measures to expose (use '*' for all)
        measures: '*'

        # Optional: customize the time range dropdown
        time_ranges:
          - P7D
          - P30D
          - P90D
          - P12M
          - rill-MTD
          - rill-YTD

        # Optional: default dashboard state on first load
        defaults:
          time_range: P30D
          comparison_mode: time

        # Optional: pin specific time zones to the top of the selector
        time_zones:
          - America/Los_Angeles
          - America/New_York

        # Optional: custom theme
        theme: my_theme

        # Optional: restrict access to specific users.
        # Note: usually you should do this in the metrics view, not the explore resource.
        security:
          access: "{{ .user.admin }} OR '{{ .user.email }}' LIKE '%@example.com'"
        ```

        ## Minimal example

        For most use cases, a minimal explore is sufficient:

        ```yaml
        type: explore
        metrics_view: sales_metrics
        display_name: "Sales Dashboard"
        dimensions: '*'
        measures: '*'
        ```

        ## JSON Schema

        ```
        allOf:
            - properties:
                allow_custom_time_range:
                    description: Defaults to true, when set to false it will hide the ability to set a custom time range for the user.
                    type: boolean
                banner:
                    description: Refers to the custom banner displayed at the header of an explore dashboard
                    type: string
                defaults:
                    additionalProperties: false
                    description: |
                        defines the defaults YAML struct
                          ```yaml
                          defaults: #define all the defaults within here
                            dimensions:
                              - dim_1
                              - dim_2
                            measures:
                              - measure_1
                              - measure_2
                            time_range: P1M
                            comparison_mode: dimension #time, none
                            comparison_dimension: filename
                          ```
                    properties:
                        comparison_dimension:
                            description: for dimension mode, specify the comparison dimension by name
                            type: string
                        comparison_mode:
                            description: 'Controls how to compare current data with historical or categorical baselines. Options: `none` (no comparison), `time` (compares with past based on default_time_range), `dimension` (compares based on comparison_dimension values)'
                            enum:
                                - none
                                - time
                                - dimension
                            type: string
                        dimensions:
                            $ref: '#/definitions/field_selector_properties'
                            description: Provides the default dimensions to load on viewing the dashboard
                        measures:
                            $ref: '#/definitions/field_selector_properties'
                            description: Provides the default measures to load on viewing the dashboard
                        time_range:
                            description: Refers to the default time range shown when a user initially loads the dashboard. The value must be either a valid [ISO 8601 duration](https://en.wikipedia.org/wiki/ISO_8601#Durations) (for example, PT12H for 12 hours, P1M for 1 month, or P26W for 26 weeks) or one of the [Rill ISO 8601 extensions](https://docs.rilldata.com/reference/rill-iso-extensions#extensions)
                            type: string
                    type: object
                description:
                    description: Refers to the description of the explore dashboard
                    type: string
                dimensions:
                    $ref: '#/definitions/field_selector_properties'
                    description: List of dimension names. Use '*' to select all dimensions (default)
                    examples:
                        - dimensions:
                            - country
                        - dimensions:
                            exclude:
                                - country
                        - dimensions:
                            expr: ^public_.*$
                display_name:
                    description: Refers to the display name for the explore dashboard
                    type: string
                embeds:
                    additionalProperties: false
                    description: Configuration options for embedded dashboard views
                    properties:
                        hide_pivot:
                            description: When true, hides the pivot table view in embedded mode
                            type: boolean
                    type: object
                lock_time_zone:
                    description: When true, the dashboard will be locked to the first time provided in the time_zones list. When no time_zones are provided, the dashboard will be locked to UTC
                    type: boolean
                measures:
                    $ref: '#/definitions/field_selector_properties'
                    description: List of measure names. Use '*' to select all measures (default)
                    examples:
                        - measures:
                            - sum_of_total
                        - measures:
                            exclude:
                                - sum_of_total
                        - measures:
                            expr: ^public_.*$
                metrics_view:
                    description: Refers to the metrics view resource
                    type: string
                security:
                    $ref: '#/definitions/dashboard_security_policy_properties'
                    description: Security rules to apply for access to the explore dashboard
                theme:
                    description: Name of the theme to use. Only one of theme and embedded_theme can be set.
                    oneOf:
                        - description: Name of an existing theme to apply to the dashboard
                          type: string
                        - $ref: '#/definitions/theme_properties'
                          description: Inline theme configuration.
                time_ranges:
                    description: |
                        Overrides the list of default time range selections available in the dropdown. It can be string or an object with a 'range' and optional 'comparison_offsets'
                          ```yaml
                          time_ranges:
                            - PT15M // Simplified syntax to specify only the range
                            - PT1H
                            - PT6H
                            - P7D
                            - range: P5D // Advanced syntax to specify comparison_offsets as well
                            - P4W
                            - rill-TD // Today
                            - rill-WTD // Week-To-date
                          ```
                    items:
                        $ref: '#/definitions/explore_time_range_properties'
                    type: array
                time_zones:
                    description: Refers to the time zones that should be pinned to the top of the time zone selector. It should be a list of [IANA time zone identifiers](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones)
                    items:
                        type: string
                    type: array
                type:
                    const: explore
                    description: Refers to the resource type and must be `explore`
                    type: string
              required:
                - type
                - display_name
                - metrics_view
              title: Properties
              type: object
            - $ref: '#/definitions/common_properties'
        description: Explore dashboards provide an interactive way to explore data with predefined measures and dimensions.
        id: explore-dashboards
        title: Explore Dashboard YAML
        type: object
        ```
    - role: user
      content_type: text
      content: |-
        You should develop a Rill project file based on the following task description: Create an explore dashboard for the covid metrics view. Reference the metrics view named 'covid'. Use default settings for all other properties. This will provide a drill-down dashboard for covid data from the ClickHouse playground.

        Here is some important context:
        - You should develop the file at path: /dashboards/covid.yaml
        - The file should be of type: explore
        - You are running as a sub-agent of a larger developer agent. Stay aligned on your specific task, avoid extra discovery, and report back your success or failure to the parent agent.

        Here is some additional context that may or may not be relevant to your task:
        - The project's default OLAP connector is named clickhouse (driver: clickhouse).
    - role: assistant
      content_type: tool_call
      id: 2a88b54334494f46
      tool_name: write_file
      content: '{"contents":"type: explore\nmetrics_view: covid\n","path":"/dashboards/covid.yaml"}'
    - role: tool
      content_type: tool_response
      id: 2a88b54334494f46
      content: '{"diff":"--- /dashboards/covid.yaml\n+++ /dashboards/covid.yaml\n@@ -1 +1,3 @@\n+type: explore\n+metrics_view: covid\n \n","is_new_file":true,"resources":[{"kind":"rill.runtime.v1.Explore","name":"covid","reconcile_error":"","reconcile_status":"RECONCILE_STATUS_IDLE"}]}'
  response:
    - role: assistant
      content_type: text
      content: The explore dashboard file for the covid metrics view has been created at /dashboards/covid.yaml with default settings, as requested. The file references the metrics view named 'covid' and uses the minimal configuration for an explore dashboard. Task complete and ready for parent agent review.
- index: 24
  input:
    - role: system
      content_type: text
      content: "# Instructions for developing a Rill project\n\nThis document is intended for data engineering agents specialized in developing projects in the Rill business intelligence platform.\n\n## Introduction to Rill\n\nRill is a business intelligence platform built around the following principles:\n- Code-first: configure projects using versioned and reproducible source code in the form of YAML and SQL files.\n- Full stack: go from raw data sources to user-friendly dashboards powered by clean data with a single tool.\n- Declarative: describe your business logic and Rill automatically runs the infrastructure, migrations and services necessary to make it real.\n- OLAP databases: you can easily provision a fast analytical database and load data into it to build dashboards that stay interactive at scale.\n\n## Project structure\n\nA Rill project consists of resources that are defined using YAML and SQL files in the project's file directory.\nRill supports different resource types, such as connectors, models, metrics views, explore dashboards, and more.\n\nHere is an example listing of files for a small Rill project:\n```\n.env\nconnectors/duckdb.yaml\nconnectors/s3.yaml\nmodels/events_raw.yaml\nmodels/events.sql\nmetrics/events.yaml\ndashboards/events.yaml\nrill.yaml\n```\n\nLet's start with the project-wide files at the root of the directory:\n- `rill.yaml` is a required file that contains project-wide configuration. It can be compared to `package.json` in Node.js or `dbt_project.yml` in dbt.\n- `.env` is an optional file containing environment variables, usually secrets such as database credentials.\n\nThe other YAML and SQL files define individual resources in the project. They follow a few rules:\n- The YAML files must contain a `type:` property that identifies the resource type. The other properties in the file are specific to the selected resource type.\n- SQL files are a convenient way of creating model resources. They are equivalent to a YAML file with `type: model` and a `sql:` property.\n- Each file declares one main resource, but may in some cases also emit some dependent resources with internally generated names.\n- The main resource declared by a file gets a unique name derived from the filename by removing the directory name and extension. For example, `connectors/duckdb.yaml` defines a connector called `duckdb`.\n- Directories are ignored by the parser and can be used to organize the project as you see fit. Small projects often have one directory per resource type.\n- Resources can reference other resources, which forms a dependency graph (DAG) that informs the sequence they are executed.\n- Resource names are unique within a resource type. For example, only one model can be named `events` (regardless of directory), but it is possible for both a model and a metrics view to be called `events`.\n- Clear resource names are important as they are widely used as unique identifiers throughout the platform (e.g. in CLI commands, URL slugs, API calls). They are usually lowercase and snake case, but that is not enforced.\n\n## Project execution\n\nRill automatically watches project files and processes changes. There are two key phases:\n- **Parsing**: Files are converted into resources and organized into a DAG. Malformed files produce *parse errors*.\n- **Reconciliation**: Resources are executed to achieve their desired state. Failures produce *reconcile errors*.\n\nSome resources are cheap to reconcile (validation, non-materialized models), others are expensive (materialized models, managed connectors). Be cautious with expensive operations; see resource-specific instructions for details.\n\nResources can also have scheduled reconciliation via cron expressions (e.g. daily model refresh).\n\n## Rill's environments\n\nRill has a local CLI (`rill`) for development and a cloud service for production. After developing or changing a project locally, developers deploy to Rill Cloud either by pushing to GitHub (continuous deploys) or manually deploying with the CLI.\n\n## OLAP databases\n\nRill places high emphasis on \"operational intelligence\", meaning low-latency, high-performance, drill-down dashboards with support for alerts and scheduled reports.\nRill supports these features using OLAP databases and has drivers that are heavily optimized to leverage database-specific features to get high performance.\n\nOLAP databases are configured as any other connector in Rill.\nPeople can either connect an external OLAP database with existing tables, or can ask Rill to provision an empty OLAP database for them, which they can load data into using Rill's `model` resource type.\n\nOLAP connectors are currently the only connectors that can directly power the metrics views resources that in turn power dashboards. So data must be in an OLAP database to power a dashboard.\n\nSince OLAP databases have a special role in Rill, every project must have a _default_ OLAP connector that you configure using the `olap_connector:` property in `rill.yaml`. This default OLAP connector is automatically used for a variety of things in Rill unless explicitly overridden (see details under the resource type descriptions). If no OLAP connector is configured, Rill by default initializes a managed `duckdb` OLAP database and uses it as the default OLAP connector.\n\n## Resource types\n\nThe sections below contain descriptions of the different resource types that Rill supports and when to use them.\nThe descriptions are high-level; you can find detailed descriptions and examples in the separate resource-specific instruction files.\n\n### Connectors\n\nConnectors are resources containing credentials and settings for connecting to an external system.\nThey are usually lightweight as their reconcile logic usually only validates the connection.\nThey are normally found at the root of the DAG, powering other downstream resource types.\n\nThere are a variety of built-in connector _drivers_, which each implements one or more capabilities:\n- **OLAP database:** can power dashboards (e.g. `duckdb`, `clickhouse`)\n- **SQL database:** can run SQL queries and models (e.g. `postgres`, `bigquery`, `snowflake`)\n- **Information schema:** can list tables and their schemas (e.g. `duckdb`, `bigquery`, `postgres`)\n- **Object store:** can list, read and write flat files (e.g. `s3`)\n- **Notifier:** can send notifications (e.g. `slack`)\n\nHere are some useful things to know when developing connectors:\n- Actual secrets like database passwords should go in `.env` and be referenced from the connector's YAML file\n- Connectors are usually called the same as their driver, unless there are multiple connectors that use the same driver.\n- OLAP connectors with the property `managed: true` will automatically be provisioned by Rill, so you don't need to handle the infrastructure or credentials directly. This is only supported for the `duckdb` and `clickhouse` drivers. The user will be subject to usage-based billing for the CPU, memory and disk usage of the provisioned database.\n- User-configured OLAP connectors with externally managed tables should have `mode: read` to protect from unintended writes from Rill models.\n- The primary OLAP connector used in a project should be configured in `rill.yaml` using the `olap_connector:` property.\n\n### Models\n\nModels are resources that specify ETL or transformation logic that outputs a tabular dataset in one of the project's connectors.\nThey are usually expensive resources that are found near the root of the DAG, referencing only connectors and other models.\n\nModels usually (and by default) output data as a table with the same name as the model in the project's default OLAP connector.\nThey usually center around a `SELECT` SQL statement that Rill will run as a `CREATE TABLE <name> AS <SELECT statement>`.\nThis means models in Rill are similar to models in dbt, but they support some additional advanced features, namely:\n- Different input and output connectors (making it easy to e.g. run a query in BigQuery and output it to the default OLAP connector)\n- Stateful incremental ingestion with support for explicit partitions (e.g. for loading Hive partitioned files from S3)\n- Scheduled refresh using a cron expression in the model itself\n\nWhen reasoning about a model, it can be helpful to think in terms of the following attributes:\n- **Source model:** references external data, usually reading data from a SQL or object store connector and writing it into an OLAP connector\n- **Derived model:** references other models, usually doing joins or formatting columns to prepare a denormalized table suitable for use in metrics views and dashboards \n- **Materialized model:** outputs a physical table (i.e. not just a SQL view)\n- **Incremental model:** has logic for incrementally loading data\n- **Partitioned model:** capable of loading data in well-defined increments, such as daily partitions, enabling scalability and idempotent incremental runs\n\nModels are usually expensive resources that can take a long time to run, and should be created or edited with caution.\nThe only exception is non-materialized models that have the same input and output connector, which get created as cheap SQL views.\nIn development, you can avoid expensive operations by adding a \"dev partition\", which limits data processed to a subset. See the instructions for model development for details.\n\n### Metrics views\n\nMetrics views are resources that define queryable business metrics on top of a table in an OLAP database.\nThey implement what other business intelligence tools call a \"semantic layer\" or \"metrics layer\".\nThey are lightweight resources found downstream of connectors and models in the DAG.\nThey power many user-facing features, such as dashboards, alerts, and scheduled reports.\n\nMetrics views consist of:\n- **Model:** a table in an OLAP database; can either be a pre-existing table in an external OLAP database or a table produced by a model in the Rill project\n- **Dimensions:** SQL expressions that can be grouped by (e.g. time, string or geospatial types)\n- **Measures:** SQL expressions that define aggregations (usually numeric types)\n- **Security policies:** access rules and row filters that reference attributes of the querying user\n\n### Explores\n\nExplore resources define an \"explore dashboard\", an opinionated dashboard type that comes baked into Rill.\nThese dashboards are specifically designed as an explorative, drill-down, slice-and-dice interface for a single metrics view.\nThey are Rill's default dashboard type, and usually configured for every metrics view in a project.\nThey are lightweight resources that are always found downstream of a metrics view in the DAG.\n\nExplore resources can either be configured as stand-alone files or as part of a metrics view definition (see metrics view instructions for details).\nThe only required configuration is a metrics view to render, but you can optionally also configure things like a theme, default dimension and measures to show, time range presets, and more.\n\n### Canvases\n\nCanvas resources configure a \"canvas dashboard\", which is a free-form dashboard type consisting of custom chart and table components laid out in a grid.\nThey enable users to build overview/report style dashboards with limited drill-down options, similar to those found in traditional business intelligence tools.\n\nCanvas dashboards support a long list of component types, including line charts, bar charts, pie charts, markdown text, tables, and more.\nAll components are defined in the canvas file, but each component is emitted as a separate resource of type `component`, which gets placed upstream of the canvas in the project DAG.\nEach canvas component fetches data individually, almost always from a metrics view resource; so you often find metrics view resources upstream of components in the DAG.\n\n### Themes\n\nThemes are resources that define a custom color palette for a Rill project.\nThey are referenced from `rill.yaml` or directly from an explore or canvas dashboards.\n\n### Custom APIs\n\nCustom APIs are resources that define a query that serves data from the Rill project on a custom endpoint.\nThey are advanced resources that enable easy programmatic integration with a Rill project.\nThey are lightweight resources that are usually found downstream of metrics views in the DAG (but sometimes directly downstream of a connector or model).\n\nCustom APIs are mounted as `GET` and `POST` REST APIs on `<project URL>/api/<resource name>`.\nThe queries can use templating to inject request parameters or user attributes.\n\nRill supports a number of different \"data resolver\" types, which execute queries and return data.\nThe most common ones are:\n- `metrics_sql`: queries a metrics view using a generic SQL syntax (recommended)\n- `metrics`: queries a metrics view using a structured query object\n- `sql`: queries an OLAP connector using a raw SQL query in its native SQL dialect\n\n### Alerts\n\nAlerts are resources that enable sending alerts when certain criteria matches data in the Rill project.\nThey consists of a refresh schedule, a query to execute, and notification settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define alerts directly as files; instead, users can define alerts using a UI in Rill Cloud.\n\n### Reports\n\nReports are resources that enable sending scheduled reports of data in the project.\nThey consists of a delivery schedule, a query to execute, and delivery settings.\nSince they repeatedly run a query, they are slightly expensive resources.\nThey are usually found downstream of a metrics view in the DAG.\nMost projects don't define reports directly as files; instead, users can define reports using a UI in Rill Cloud.\n\n### `rill.yaml`\n\n`rill.yaml` is a required file for project-wide config found at the root directory of a Rill project.\nIt is mainly used for:\n- Setting shared properties for all resources of a given type (e.g. giving all dashboards the same theme)\n- Setting default values for non-sensitive environment variables\n- Customizing feature flags\n- Configuring mock users for testing security policies locally\n\n### `.env`\n\n`.env` is an optional file containing environment variables, which Rill loads when running the project.\nOther resources can reference these environment variables using a templating syntax.\nBy convention, environment variables in Rill use snake-case, lowercase names (this differs from shell environment variables).\n\n## Development process\n\nThis section describes the recommended workflow for developing resources in a Rill project.\n\n### Understanding the task\n\nBefore making changes, determine what kind of task you are performing:\n- **Querying**: If you need to answer a question about data in the project, use query tools but do not modify files.\n- **Surgical edit**: If you need to create or update a single resource, focus on that resource and its immediate dependencies.\n- **Full pipeline**: If you need to go from raw data to dashboard, expect your changes to cover a sequential pipeline through connector(s), source model(s), derived model(s), metrics view(s), and an explore or canvas dashboard.\n\n### Checking project capabilities\n\nBefore proceeding, verify what the project supports:\n- **Write access**: Do you have access to modify files in the project? If not, you are limited to explaining the project or guiding the developer.\n- **Data access**: Does the project have a connector for the relevant data source? If not, you need to create a connector and add the required credentials to `.env`, then ask the user to populate those values before continuing.\n- **OLAP mode**: Is the default OLAP connector read-only or readwrite? If read-only, you cannot create models; instead, create metrics views and dashboards directly on existing tables in the OLAP database.\n\n### Recommended workflow\n\nYour workflow will depend on the kind of task you are undertaking. Here follows an idealized workflow for a full data source to dashboard journey:\n\n1. **Survey existing resources**: Check what resources already exist in the project using the project status and file tools. You may be able to reuse or extend existing models, metrics views, or dashboards rather than creating new ones.\n2. **Explore available data**: Use connector introspection tools to discover what tables or files are available. For SQL databases, query the information schema. For object stores, list buckets and files.\n3. **Handle missing data**: If the project lacks access to the data you need, ask the user whether to generate mock data or help them configure a connector to their data source.\n4. **Create or update models** (managed or readwrite OLAP only): Build models that ingest and transform data into denormalized tables suitable for dashboard queries. Materialize models that involve expensive joins or aggregations. Use dev partitions to limit data during development.\n5. **Profile the data**: Before creating a metrics view, look at the schema of the underlying model/table to understand its shape. This informs which dimensions and measures you create. Consider using the SQL query tool to do a couple well-chosen queries to the table to get row counts, cardinality of important columns, example column values, date ranges, or similar. Be very careful not to run too many queries or expensive queries.\n6. **Create or update the metrics view**: Define dimensions and measures using columns in the underlying model/table. Start small with up to 10 dimensions and up to 5 measures, and add more later if relevant.\n7. **Ensure there are dashboards**: Create an explore dashboard for drill-down analysis of the metrics view if one doesn't already exist. If the user wants an overview or report-style view, also create a canvas dashboard with components from one or more metrics views.\n\n### Available tools\n\nThe following tools are typically available for project development:\n\n- `file_list`, `file_search` and `file_read` for accessing existing files in the project\n- `develop_file` for delegating file development to a sub-agent, which handles writing and iterating on errors\n- `file_write` for directly creating, updating or deleting a file (available to sub-agents; waits for parse/reconcile and returns resource status)\n\n- `project_status` for checking resource names and their current status (idle, running, error)\n- `query_sql` for running SQL against a connector; use `SELECT` statements with `LIMIT` clauses and low timeouts, and be mindful of performance or making too many queries\n- `query_metrics_view` for querying a metrics view; useful for answering data questions and validating dashboard behavior\n- `list_tables` and `get_table` for accessing the information schema of a database connector\n- `list_buckets` and `list_bucket_files` for exploring object stores; load files into models using SQL before querying them\n\n\n\n### Common pitfalls\n\nAvoid these mistakes when developing a project:\n- **Duplicating ETL logic**: Ingest data once, then derive from it within the project. Do not create multiple models that pull the same data from an external source.\n- **Forgetting to materialize**: Always materialize models that reference external data or perform expensive operations. Non-materialized models become views, which re-execute on every query.\n- **Processing too much data in development**: Use dev partitions to limit data to a small subset (e.g., one day) during development. This speeds up iteration and avoids unnecessary costs.\n\n- **Doing too much introspection/profiling:** Reading files, introspecting connectors, profiling models/tables can be time consuming and easily load too much context. Stay disciplined and don't do too much open-ended exploration or unnecessarily look into other levels of the DAG, especially if your task is a small/surgical edit.\n- **Not using the `develop_file` tool:** You should plan the changes you want to make first, then delegate each change separately to the `develop_file` tool. When calling `develop_file`, pass in any relevant context from your investigation/planning/profiling phase.\n- **Doing too much at a time:** Consider the minimal amount of work to accomplish your current task. It's better to make changes incrementally and let the user guide your work.\n"
    - role: user
      content_type: text
      content: |-
        This may not relate to the user's task, but for context, the project's default OLAP connector is named duckdb (driver: duckdb).
         The default OLAP is in readwrite mode, so you can use it in models if you want.


        Task: Clickhouse has a public playground database on play.clickhouse.com:9440 (username `play` and no password). Can you set it up as the OLAP connector and develop a dashboard based on one of its tables? Please proceed without asking clarifying questions.
    - role: assistant
      content_type: tool_call
      id: 5f915383ab074f81
      tool_name: develop_file
      content: '{"path":"connectors/clickhouse.yaml","prompt":"Create a connector for the public ClickHouse playground database at play.clickhouse.com:9440. Use the ''clickhouse'' driver. Set the username to ''play'' and leave the password blank. This connector will be used as an OLAP connector for dashboards. Set ''mode'' to ''read'' to prevent writes. Name the connector ''clickhouse''.","type":"connector"}'
    - role: tool
      content_type: tool_response
      id: 5f915383ab074f81
      content: '{"response":"The connector file /connectors/clickhouse.yaml was created as specified:\n\n- type: connector\n- driver: clickhouse\n- host: play.clickhouse.com\n- port: 9440\n- username: play\n- password: \"\" (blank)\n- mode: read\n\nThe file was written successfully. However, there is a reconcile error indicating a handshake failure, likely due to an SSL/TLS mismatch. If SSL is required for this endpoint, you may need to add ssl: true to the configuration. Please advise if you want to update the file to include SSL."}'
    - role: assistant
      content_type: tool_call
      id: 3925a34a97214026
      tool_name: develop_file
      content: '{"path":"connectors/clickhouse.yaml","prompt":"Update the ClickHouse connector configuration to include SSL support, as the public playground likely requires a secure connection. Add ''ssl: true'' to the YAML file. Ensure all other settings remain the same: driver clickhouse, host play.clickhouse.com, port 9440, username play, blank password, mode read.","type":"connector"}'
    - role: tool
      content_type: tool_response
      id: 3925a34a97214026
      content: '{"response":"Success: The ClickHouse connector configuration at /connectors/clickhouse.yaml has been updated to include ssl: true, as required for secure connections to the public playground. All other settings (driver, host, port, username, blank password, and mode) remain unchanged. No errors were encountered during the update."}'
    - role: assistant
      content_type: tool_call
      id: 83c850aa1fea4507
      tool_name: develop_file
      content: '{"path":"rill.yaml","prompt":"Update the project configuration to set the default OLAP connector to ''clickhouse'' instead of ''duckdb''. Ensure that the ''olap_connector'' property is set to ''clickhouse''. Retain any other existing configuration in rill.yaml.","type":"rill.yaml"}'
    - role: tool
      content_type: tool_response
      id: 83c850aa1fea4507
      content: '{"response":"The rill.yaml file has been updated to set olap_connector: clickhouse as the default OLAP connector, as requested. There was no other existing configuration to retain. Task complete."}'
    - role: assistant
      content_type: tool_call
      id: a902bdef4d944e31
      tool_name: list_tables
      content: '{"connector":"clickhouse","page_size":20}'
    - role: tool
      content_type: tool_response
      id: a902bdef4d944e31
      content: '{"tables":[{"database_schema":"blogs","name":"countries","is_view":false},{"database_schema":"blogs","name":"country_codes","is_view":false},{"database_schema":"blogs","name":"forex","is_view":false},{"database_schema":"blogs","name":"forex_2020s","is_view":false},{"database_schema":"blogs","name":"forex_usd","is_view":false},{"database_schema":"blogs","name":"noaa","is_view":false},{"database_schema":"blogs","name":"noaa_v2","is_view":false},{"database_schema":"blogs","name":"stop_words","is_view":false},{"database_schema":"blogs","name":"uk_codes","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"actors","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"benchmark_results","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"benchmark_runs","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"cell_towers","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"checks","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"checks_coverage_inverted","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"checks_v2","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"cisco_umbrella","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"covid","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"dish","is_view":false},{"database_schema":"default","is_default_database_schema":true,"name":"dns","is_view":false}],"next_page_token":"ImRlZmF1bHQiCiJkbnMiCg"}'
    - role: assistant
      content_type: tool_call
      id: 46401ed1773746ab
      tool_name: show_table
      content: '{"connector":"clickhouse","database_schema":"default","table":"covid"}'
    - role: tool
      content_type: tool_response
      id: 46401ed1773746ab
      content: '{"name":"covid","is_view":false,"columns":[{"name":"date","type":"CODE_DATE"},{"name":"location_key","type":"CODE_STRING"},{"name":"new_confirmed","type":"CODE_INT32"},{"name":"new_deceased","type":"CODE_INT32"},{"name":"new_recovered","type":"CODE_INT32"},{"name":"new_tested","type":"CODE_INT32"},{"name":"cumulative_confirmed","type":"CODE_INT32"},{"name":"cumulative_deceased","type":"CODE_INT32"},{"name":"cumulative_recovered","type":"CODE_INT32"},{"name":"cumulative_tested","type":"CODE_INT32"}],"physical_size_bytes":-1}'
    - role: assistant
      content_type: tool_call
      id: 850ac6b8ec0b4f1b
      tool_name: develop_file
      content: '{"path":"metrics/covid.yaml","prompt":"Create a metrics view for the covid table in the ClickHouse playground (schema: default, table: covid). Use the clickhouse connector. Define the following dimensions: date (date), location_key (string). Define the following measures: new_confirmed (sum), new_deceased (sum), new_recovered (sum), new_tested (sum), cumulative_confirmed (max), cumulative_deceased (max), cumulative_recovered (max), cumulative_tested (max). Name the metrics view ''covid''.","type":"metrics_view"}'
    - role: tool
      content_type: tool_response
      id: 850ac6b8ec0b4f1b
      content: '{"response":"The metrics view file /metrics/covid.yaml was created as specified, targeting the covid table in the ClickHouse playground (schema: default, table: covid) using the clickhouse connector. It defines the requested dimensions (date, location_key) and measures (new_confirmed, new_deceased, new_recovered, new_tested, cumulative_confirmed, cumulative_deceased, cumulative_recovered, cumulative_tested).\n\nHowever, ClickHouse-backed metrics views in Rill do not allow measures to have the same name as columns in the underlying table. All the requested measure names match column names, resulting in a validation error for each measure.\n\nTo resolve this, you must rename each measure to something different from the corresponding column name (for example, prefix with total_ or use a suffix like _sum or _max). Please confirm if you want to proceed with renaming, and specify your preferred naming convention if any."}'
    - role: assistant
      content_type: tool_call
      id: 7be75a411bee47e9
      tool_name: develop_file
      content: '{"path":"metrics/covid.yaml","prompt":"Update the metrics view for the covid table to resolve the validation error: rename each measure so it does not match the underlying column name. Use the following naming convention: for sum aggregations, use the suffix _sum (e.g., new_confirmed_sum); for max aggregations, use the suffix _max (e.g., cumulative_confirmed_max). Ensure all other configuration remains unchanged.","type":"metrics_view"}'
    - role: tool
      content_type: tool_response
      id: 7be75a411bee47e9
      content: '{"response":"The metrics view at /metrics/covid.yaml has been updated to resolve the validation error by renaming each measure so it does not match the underlying column name. The following naming convention was applied:\n\n- For SUM aggregations: suffix _sum (e.g., new_confirmed_sum)\n- For MAX aggregations: suffix _max (e.g., cumulative_confirmed_max)\n\nAll other configuration remains unchanged. The file reconciled successfully with no errors. Task complete."}'
    - role: assistant
      content_type: tool_call
      id: f0f39f38538e47e0
      tool_name: develop_file
      content: '{"path":"dashboards/covid.yaml","prompt":"Create an explore dashboard for the covid metrics view. Reference the metrics view named ''covid''. Use default settings for all other properties. This will provide a drill-down dashboard for covid data from the ClickHouse playground.","type":"explore"}'
    - role: tool
      content_type: tool_response
      id: f0f39f38538e47e0
      content: '{"response":"The explore dashboard file for the covid metrics view has been created at /dashboards/covid.yaml with default settings, as requested. The file references the metrics view named ''covid'' and uses the minimal configuration for an explore dashboard. Task complete and ready for parent agent review."}'
  response:
    - role: assistant
      content_type: text
      content: "Heres what Ive accomplished for your task:\n\n1. **Set up the ClickHouse Playground as an OLAP connector**:\n   - Created `/connectors/clickhouse.yaml` using the public endpoint (`play.clickhouse.com:9440`), username `play`, no password, `ssl: true`, and `mode: read`.\n   - Updated `rill.yaml` to set `clickhouse` as the default OLAP connector.\n\n2. **Developed a dashboard based on a ClickHouse table**:\n   - Chose the `covid` table from the `default` schema.\n   - Created a metrics view (`/metrics/covid.yaml`) with:\n     - Dimensions: `date`, `location_key`\n     - Measures (with suffixes to avoid name conflicts):  \n       - Sums: `new_confirmed_sum`, `new_deceased_sum`, `new_recovered_sum`, `new_tested_sum`\n       - Maxes: `cumulative_confirmed_max`, `cumulative_deceased_max`, `cumulative_recovered_max`, `cumulative_tested_max`\n   - Created an explore dashboard (`/dashboards/covid.yaml`) for the `covid` metrics view.\n\n**You now have a working dashboard on COVID-19 data from the ClickHouse public playground, fully integrated into your Rill project.**"
